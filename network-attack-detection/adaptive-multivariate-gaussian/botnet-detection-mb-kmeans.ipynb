{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import datetime as dt\n",
    "import seaborn as sns\n",
    "import matplotlib.gridspec as gridspec\n",
    "import ipaddress\n",
    "import random as rnd\n",
    "import plotly.graph_objs as go\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "import itertools\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "from functools import reduce\n",
    "from numpy import genfromtxt\n",
    "from scipy import linalg\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn import preprocessing, mixture\n",
    "from sklearn.metrics import classification_report, average_precision_score, f1_score, recall_score, precision_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "from sklearn.cluster import MiniBatchKMeans, KMeans\n",
    "from sklearn.metrics.pairwise import pairwise_distances_argmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleasing(df):\n",
    "    # data cleasing, feature engineering and save clean data into pickles\n",
    "    \n",
    "    print('### Data Cleasing and Feature Engineering')\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    \n",
    "    # [Protocol] - Discard ipv6-icmp and categorize\n",
    "    df = df[df.Proto != 'ipv6-icmp']\n",
    "    df['Proto'] = df['Proto'].fillna('-')\n",
    "    df['Proto'] = le.fit_transform(df['Proto'])\n",
    "    le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "\n",
    "    # [Label] - Categorize \n",
    "    anomalies = df.Label.str.contains('Botnet')\n",
    "    normal = np.invert(anomalies);\n",
    "    df.loc[anomalies, 'Label'] = np.uint8(1)\n",
    "    df.loc[normal, 'Label'] = np.uint8(0)\n",
    "    df['Label'] = pd.to_numeric(df['Label'])\n",
    "\n",
    "    # [Dport] - replace NaN with 0 port number\n",
    "    df['Dport'] = df['Dport'].fillna('0')\n",
    "    df['Dport'] = df['Dport'].apply(lambda x: int(x,0))\n",
    "\n",
    "    # [sport] - replace NaN with 0 port number\n",
    "    try:\n",
    "        df['Sport'] = df['Sport'].fillna('0')\n",
    "        df['Sport'] = df['Sport'].str.replace('.*x+.*', '0')\n",
    "        df['Sport'] = df['Sport'].apply(lambda x: int(x,0))\n",
    "    except:                        \n",
    "        print(\"Unexpected error:\", sys.exc_info()[0])\n",
    "\n",
    "    # [sTos] - replace NaN with \"10\" and convert to int\n",
    "    df['sTos'] = df['sTos'].fillna('10')\n",
    "    df['sTos'] = df['sTos'].astype(int)\n",
    "\n",
    "    # [dTos] - replace NaN with \"10\" and convert to int\n",
    "    df['dTos'] = df['dTos'].fillna('10')\n",
    "    df['dTos'] = df['dTos'].astype(int)\n",
    "\n",
    "    # [State] - replace NaN with \"-\" and categorize\n",
    "    df['State'] = df['State'].fillna('-')\n",
    "    df['State'] = le.fit_transform(df['State'])\n",
    "    le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "\n",
    "    # [Dir] - replace NaN with \"-\" and categorize \n",
    "    df['Dir'] = df['Dir'].fillna('-')\n",
    "    df['Dir'] = le.fit_transform(df['Dir'])\n",
    "    le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "\n",
    "    # [SrcAddr] Extract subnet features and categorize\n",
    "    df['SrcAddr'] = df['SrcAddr'].fillna('0.0.0.0')\n",
    "    \n",
    "    # [DstAddr] Extract subnet features\n",
    "    df['DstAddr'] = df['DstAddr'].fillna('0.0.0.0')\n",
    "    \n",
    "    # [StartTime] - Parse to datatime, reindex based on StartTime, but first drop the ns off the time stamps\n",
    "    df['StartTime'] = df['StartTime'].apply(lambda x: x[:19])\n",
    "    df['StartTime'] = pd.to_datetime(df['StartTime'])\n",
    "    \n",
    "    print('### Sorting...')\n",
    "    df = df.sort_values('StartTime')\n",
    "    gc.collect()\n",
    "    print('### Sorted!')\n",
    "    df = df.set_index('StartTime')\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def classify_ip(ip):\n",
    "    '''\n",
    "    str ip - ip address string to attempt to classify. treat ipv6 addresses as N/A\n",
    "    '''\n",
    "    try: \n",
    "        ip_addr = ipaddress.ip_address(ip)\n",
    "        if isinstance(ip_addr, ipaddress.IPv6Address):\n",
    "            return 'ipv6'\n",
    "        elif isinstance(ip_addr, ipaddress.IPv4Address):\n",
    "            # split on .\n",
    "            octs = ip_addr.exploded.split('.')\n",
    "            if 0 < int(octs[0]) < 127: return 'A'\n",
    "            elif 127 < int(octs[0]) < 192: return 'B'\n",
    "            elif 191 < int(octs[0]) < 224: return 'C'\n",
    "            else: return 'N/A'\n",
    "    except ValueError:\n",
    "        return 'N/A'\n",
    "\n",
    "    \n",
    "def avg_duration(x):\n",
    "    return np.average(x)\n",
    "    \n",
    "    \n",
    "def n_dports_gt1024(x):\n",
    "    if x.size == 0: return 0\n",
    "    return reduce((lambda a,b: a+b if b>1024 else a),x)\n",
    "n_dports_gt1024.__name__ = 'n_dports>1024'\n",
    "\n",
    "\n",
    "def n_dports_lt1024(x):\n",
    "    if x.size == 0: return 0\n",
    "    return reduce((lambda a,b: a+b if b<1024 else a),x)\n",
    "n_dports_lt1024.__name__ = 'n_dports<1024'\n",
    "\n",
    "\n",
    "def n_sports_gt1024(x):\n",
    "    if x.size == 0: return 0\n",
    "    return reduce((lambda a,b: a+b if b>1024 else a),x)\n",
    "n_sports_gt1024.__name__ = 'n_sports>1024'\n",
    "\n",
    "\n",
    "def n_sports_lt1024(x):\n",
    "    if x.size == 0: return 0\n",
    "    return reduce((lambda a,b: a+b if b<1024 else a),x)\n",
    "n_sports_lt1024.__name__ = 'n_sports<1024'\n",
    "\n",
    "\n",
    "def label_atk_v_norm(x):\n",
    "    for l in x:\n",
    "        if l == 1: return 1\n",
    "    return 0\n",
    "label_atk_v_norm.__name__ = 'label'\n",
    "\n",
    "\n",
    "def background_flow_count(x):\n",
    "    count = 0\n",
    "    for l in x:\n",
    "        if l == 0: count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "def normal_flow_count(x):\n",
    "    if x.size == 0: return 0\n",
    "    count = 0\n",
    "    for l in x:\n",
    "        if l == 0: count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "def n_conn(x):\n",
    "    return x.size\n",
    "\n",
    "\n",
    "def n_tcp(x):\n",
    "    count = 0\n",
    "    for p in x: \n",
    "        if p == 10: count += 1 # tcp == 10\n",
    "    return count\n",
    "       \n",
    "    \n",
    "def n_udp(x):\n",
    "    count = 0\n",
    "    for p in x: \n",
    "        if p == 11: count += 1 # udp == 11\n",
    "    return count\n",
    "       \n",
    "    \n",
    "def n_icmp(x):\n",
    "    count = 0\n",
    "    for p in x: \n",
    "        if p == 1: count += 1 # icmp == 1\n",
    "    return count\n",
    "\n",
    "\n",
    "def n_s_a_p_address(x):\n",
    "    count = 0\n",
    "    for i in x: \n",
    "        if classify_ip(i) == 'A': count += 1\n",
    "    return count\n",
    "     \n",
    "    \n",
    "def n_d_a_p_address(x):\n",
    "    count = 0\n",
    "    for i in x: \n",
    "        if classify_ip(i) == 'A': count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "def n_s_b_p_address(x):\n",
    "    count = 0\n",
    "    for i in x: \n",
    "        if classify_ip(i) == 'B': count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "def n_d_b_p_address(x):\n",
    "    count = 0\n",
    "    for i in x: \n",
    "        if classify_ip(i) == 'A': count += 1\n",
    "    return count\n",
    "             \n",
    "    \n",
    "def n_s_c_p_address(x):\n",
    "    count = 0\n",
    "    for i in x: \n",
    "        if classify_ip(i) == 'C': count += 1\n",
    "    return count\n",
    "           \n",
    "    \n",
    "def n_d_c_p_address(x):\n",
    "    count = 0\n",
    "    for i in x: \n",
    "        if classify_ip(i) == 'C': count += 1\n",
    "    return count\n",
    "            \n",
    "    \n",
    "def n_s_na_p_address(x):\n",
    "    count = 0\n",
    "    for i in x: \n",
    "        if classify_ip(i) == 'N/A': count += 1\n",
    "    return count\n",
    "           \n",
    "    \n",
    "def n_d_na_p_address(x):\n",
    "    count = 0\n",
    "    for i in x: \n",
    "        if classify_ip(i) == 'N/A': count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "def n_ipv6(x):\n",
    "    count = 0\n",
    "    for i in x:\n",
    "        if classify_ip(i) == 'ipv6': count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "def estimateGaussian(dataset):\n",
    "    mu = np.mean(dataset, axis=0)\n",
    "    sigma = np.cov(dataset.T)\n",
    "    return mu, sigma\n",
    "\n",
    "\n",
    "def multivariateGaussian(dataset, mu, sigma):\n",
    "    p = multivariate_normal(mean=mu, cov=sigma, allow_singular=True)\n",
    "    return p.pdf(dataset)\n",
    "\n",
    "\n",
    "def selectThresholdByCV(probs, labels):\n",
    "    # select best epsilon (threshold)\n",
    "    \n",
    "    # initialize\n",
    "    best_epsilon = 0\n",
    "    best_f1 = 0\n",
    "    best_precision = 0\n",
    "    best_recall = 0\n",
    "    \n",
    "    min_prob = min(probs);\n",
    "    max_prob = max(probs);\n",
    "    stepsize = (max(probs) - min(probs)) / 1000;\n",
    "    epsilons = np.arange(min(probs), max(probs), stepsize)\n",
    "    \n",
    "    for epsilon in epsilons:\n",
    "        predictions = (probs < epsilon)\n",
    "        \n",
    "        f1 = f1_score(labels, predictions, average = \"binary\")\n",
    "        Recall = recall_score(labels, predictions, average = \"binary\")\n",
    "        Precision = precision_score(labels, predictions, average = \"binary\")            \n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_epsilon = epsilon\n",
    "            best_f1 = f1\n",
    "            best_precision = Precision\n",
    "            best_recall = Recall\n",
    "\n",
    "    return best_f1, best_epsilon\n",
    "\n",
    "\n",
    "def print_classification_report(y_test, y_predic):\n",
    "    f = f1_score(y_test, y_predic, average = \"binary\")\n",
    "    Recall = recall_score(y_test, y_predic, average = \"binary\")\n",
    "    Precision = precision_score(y_test, y_predic, average = \"binary\")\n",
    "    print('\\tF1 Score: ',f1,', Recall: ',Recall,', Precision: ,',Precision)\n",
    "\n",
    "    \n",
    "def data_splitting(df, drop_feature):\n",
    "    \n",
    "    # drop non discriminant features\n",
    "    df.drop(drop_feature, axis =1, inplace = True)\n",
    "\n",
    "    # split into normal and anomaly\n",
    "    df_l1 = df[df[\"Label\"] == 1]\n",
    "    df_l0 = df[df[\"Label\"] == 0]\n",
    "\n",
    "    # Length and indexes\n",
    "    anom_len = len(df_l1)                               # total number of anomalous flows\n",
    "    anom_train_end = anom_len // 2                      # 50% of anomalous for training\n",
    "    anom_cv_start = anom_train_end + 1                  # 50% of anomalous for testing\n",
    "    norm_len = len(df_l0)                               # total number of normal flows\n",
    "    norm_train_end = (norm_len * 60) // 100             # 60% of normal for training\n",
    "    norm_cv_start = norm_train_end + 1                  # 20% of normal for cross validation\n",
    "    norm_cv_end = (norm_len * 80) // 100                # 20% of normal for cross validation\n",
    "    norm_test_start = norm_cv_end + 1                   # 20% of normal for testing\n",
    "\n",
    "    # anomalies split data\n",
    "    anom_cv_df  = df_l1[:anom_train_end]                # 50% of anomalies59452 \n",
    "    anom_test_df = df_l1[anom_cv_start:anom_len]        # 50% of anomalies\n",
    "\n",
    "    # normal split data\n",
    "    norm_train_df = df_l0[:norm_train_end]              # 60% of normal\n",
    "    norm_cv_df = df_l0[norm_cv_start:norm_cv_end]       # 20% of normal\n",
    "    norm_test_df = df_l0 [norm_test_start:norm_len]     # 20% of normal\n",
    "\n",
    "    # CV and test data. train data is norm_train_df\n",
    "    cv_df = pd.concat([norm_cv_df, anom_cv_df], axis=0)\n",
    "    test_df = pd.concat([norm_test_df, anom_test_df], axis=0)\n",
    "\n",
    "    # labels\n",
    "    cv_label_df = cv_df[\"Label\"]\n",
    "    test_label_df = test_df[\"Label\"]\n",
    "\n",
    "    # drop label\n",
    "    norm_train_df = norm_train_df.drop(labels = [\"Label\"], axis = 1)\n",
    "    cv_df = cv_df.drop(labels = [\"Label\"], axis = 1)\n",
    "    test_df = test_df.drop(labels = [\"Label\"], axis = 1)\n",
    "    \n",
    "    return norm_train_df, cv_df, test_df, cv_label_df, test_label_df\n",
    "\n",
    "def getBestByCV(X_train, X_cv, labels):\n",
    "    # select the best epsilon (threshold) and number of clusters\n",
    "    \n",
    "    # initialize\n",
    "    best_epsilon = 0\n",
    "    best_cluster_size = 0\n",
    "    best_batch_size = 0\n",
    "    best_f1 = 0\n",
    "    best_precision = 0\n",
    "    best_recall = 0\n",
    "    \n",
    "    for m_clusters in np.arange(1, 10, 2):\n",
    "        \n",
    "        for m_batch_size in range(10, 100, 10): \n",
    "\n",
    "            mbkmeans = MiniBatchKMeans(init='k-means++', n_clusters=m_clusters, batch_size=m_batch_size, n_init=10, max_no_improvement=10).fit(X_train)\n",
    "            \n",
    "            X_cv_clusters = mbkmeans.predict(X_cv)\n",
    "            X_cv_clusters_centers = mbkmeans.cluster_centers_\n",
    "\n",
    "            dist = [np.linalg.norm(x-y) for x,y in zip(X_cv.as_matrix(), X_cv_clusters_centers[X_cv_clusters])]\n",
    "\n",
    "            y_pred = np.array(dist)        \n",
    "\n",
    "            for m_epsilon in np.arange(70, 95, 2):\n",
    "                y_pred[dist >= np.percentile(dist,m_epsilon)] = 1\n",
    "                y_pred[dist < np.percentile(dist,m_epsilon)] = 0\n",
    "            \n",
    "                f1 = f1_score(labels, y_pred, average = \"binary\")\n",
    "                Recall = recall_score(labels, y_pred, average = \"binary\")\n",
    "                Precision = precision_score(labels, y_pred, average = \"binary\") \n",
    "\n",
    "                if f1 > best_f1:\n",
    "                    best_cluster_size = m_clusters\n",
    "                    best_batch_size = m_batch_size\n",
    "                    best_epsilon = m_epsilon\n",
    "                    best_f1 = f1\n",
    "                    best_precision = Precision\n",
    "                    best_recall = Recall\n",
    "\n",
    "    return best_cluster_size, best_batch_size, best_epsilon, best_f1, best_precision, best_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-87cea6892f23>\u001b[0m in \u001b[0;36mgetBestByCV\u001b[0;34m(X_train, X_cv, labels)\u001b[0m\n\u001b[1;32m    341\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mm_epsilon\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m70\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m95\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m                 \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdist\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpercentile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm_epsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m                 \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdist\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpercentile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm_epsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m                 \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "column_types = {\n",
    "            'StartTime': 'str',\n",
    "            'Dur': 'float32',\n",
    "            'Proto': 'str',\n",
    "            'SrcAddr': 'str',\n",
    "            'Sport': 'str',\n",
    "            'Dir': 'str',\n",
    "            'DstAddr': 'str',\n",
    "            'Dport': 'str',\n",
    "            'State': 'str',\n",
    "            'sTos': 'float16',\n",
    "            'dTos': 'float16',\n",
    "            'TotPkts': 'uint32',\n",
    "            'TotBytes': 'uint32',\n",
    "            'SrcBytes': 'uint32',\n",
    "            'Label': 'str'}\n",
    "\n",
    "# feature selection\n",
    "drop_features = {\n",
    "    'drop_features01':['SrcAddr','DstAddr','sTos','Sport','Proto','TotBytes','SrcBytes'],\n",
    "    'drop_features02':['SrcAddr','DstAddr','sTos','Sport','TotBytes','SrcBytes'],\n",
    "    'drop_features03':['SrcAddr','DstAddr','sTos','Sport','Proto','SrcBytes'],\n",
    "    'drop_features04':['SrcAddr','DstAddr','sTos','Proto']\n",
    "}\n",
    "\n",
    "raw_path = os.path.join('/media/thiago/ubuntu/datasets/network/stratosphere-botnet-2011/ctu-13/raw/')\n",
    "raw_directory = os.fsencode(raw_path)\n",
    "raw_files = os.listdir(raw_directory)\n",
    "\n",
    "pkl_path = os.path.join('/media/thiago/ubuntu/datasets/network/stratosphere-botnet-2011/ctu-13/pkl/')\n",
    "pkl_directory = os.fsencode(pkl_path)\n",
    "\n",
    "# for each feature set\n",
    "for features_key, value in drop_features.items():\n",
    "\n",
    "    # Initialize labels\n",
    "    mbkmeans_test_label = []\n",
    "    mbkmeans_pred_test_label = []\n",
    "\n",
    "    for sample_file in raw_files:        \n",
    "    \n",
    "        raw_file_path = os.path.join(raw_directory, sample_file).decode('utf-8')    \n",
    "        pkl_file_path = os.path.join(pkl_directory, sample_file).decode('utf-8')\n",
    "    \n",
    "        # read pickle file with pandas or...\n",
    "        if os.path.isfile(pkl_file_path):\n",
    "            df = pd.read_pickle(pkl_file_path)\n",
    "        else:# load raw file and save clean data into pickles\n",
    "            raw_df = pd.read_csv(raw_file_path, header = 0, dtype=column_types)\n",
    "            df = data_cleasing(raw_df)\n",
    "            df.to_pickle(pkl_file_path)\n",
    "        gc.collect()\n",
    "\n",
    "        # data splitting\n",
    "        norm_train_df, cv_df, test_df, cv_label_df, test_label_df = data_splitting(df, drop_features[features_key])\n",
    "        norm_train_df.loc[:, 'Label'] = int(0)\n",
    "        cv_df.loc[:, 'Label'] = cv_label_df\n",
    "\n",
    "        # Cross-Validation\n",
    "        best_cluster_size, best_batch_size, best_epsilon, best_f1, best_precision, best_recall = getBestByCV(norm_train_df, cv_df, cv_label_df)\n",
    "        print('###[MB-KMeans][',features_key,'] Cross-Validation (cluster_size, batch_size, epsilon, f1, precision, recall): ', best_cluster_size, best_batch_size, best_epsilon, best_f1, best_precision, best_recall)\n",
    "\n",
    "        # Training - estimate clusters (anomalous or normal) for training    \n",
    "        mbkmeans = MiniBatchKMeans(init='k-means++', n_clusters=best_cluster_size, batch_size=best_batch_size, n_init=10, max_no_improvement=10).fit(train_df)\n",
    "\n",
    "        # Test prediction\n",
    "        test_clusters = mbkmeans.predict(test_df)\n",
    "        test_clusters_centers = kmeans.cluster_centers_\n",
    "        dist = [np.linalg.norm(x-y) for x,y in zip(test_df.as_matrix(), test_clusters_centers[test_clusters])]\n",
    "        pred_test_label = np.array(dist)\n",
    "        pred_test_label[dist >= np.percentile(dist, best_epsilon)] = 1\n",
    "        pred_test_label[dist < np.percentile(dist, best_epsilon)] = 0\n",
    "\n",
    "        # print results\n",
    "        print('###[MB-KMeans][',features_key,'] Test')\n",
    "        print_classification_report(test_label_df.astype(int).values, pred_test_label)\n",
    "        \n",
    "        # save results for total evaluation later\n",
    "        mbkmeans_test_label.extend(test_label)\n",
    "        mbkmeans_pred_test_label.extend(pred_test_label)\n",
    "        \n",
    "    print('###[KMeans][',features_key,'] Test Full dataset')\n",
    "    print_classification_report(mbkmeans_test_label, mbkmeans_pred_test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

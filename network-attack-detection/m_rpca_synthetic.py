# -*- coding: utf-8 -*-
"""m-rpca

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uhAspwVSAdPrI5uwh7j3LSpAikuZylp-
"""

from __future__ import division
from __future__ import print_function

import os
import sys
import warnings
warnings.filterwarnings("ignore")
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.font_manager
from scipy.stats import weibull_min
from numpy import percentile

# Import all models
from pyod.models.abod import ABOD
from pyod.models.cblof import CBLOF
from pyod.models.feature_bagging import FeatureBagging
from pyod.models.hbos import HBOS
from pyod.models.iforest import IForest
from pyod.models.knn import KNN
from pyod.models.lof import LOF
from pyod.models.loci import LOCI
from pyod.models.mcd import MCD
from pyod.models.ocsvm import OCSVM
from pyod.models.pca import PCA
from pyod.models.sos import SOS
from pyod.models.lscp import LSCP

import pandas as pd
from scipy import linalg
from scipy.stats import skew, kurtosis
from sklearn.metrics import pairwise_distances, f1_score

from tensorly.decomposition.robust_decomposition import robust_pca

from scipy.stats import skewnorm

# temporary solution for relative imports in case pyod is not installed
# if pyod is installed, no need to use the following line
sys.path.append(
    os.path.abspath(os.path.join(os.path.dirname("__file__"), '..')))

def fit_m_rpca(data, m_reg_J=1):
    """
    Robust PCA based estimation of mean, covariance, skewness and kurtosis.
    
    :param data: MxN matrix with M observations and N features, where M>N 
    :param m_reg_J: regularization. Default value is 1
    :return: 
        L: array-like, shape (m_obserations, n_features,)
            Robust data
        
        rob_mean:
        rob_cov:
        rob_dist:
        rob_precision:
        rob_skew:
        rob_skew_dist:
        rob_kurt:
        rob_kurt_dist:
    """

    L, S = robust_pca(data, reg_J=m_reg_J)

    rob_mean = L.mean(axis=0)
    rob_cov = pd.DataFrame(L).cov()
    rob_precision = linalg.pinvh(rob_cov)
    rob_dist = (np.dot(L, rob_precision) * L).sum(axis=1)

    rob_skew = skew(L, axis=0, bias=True)
    rob_skew_dist = (np.dot(L - rob_skew, rob_precision) * (L - rob_skew)).sum(axis=1)

    rob_kurt = kurtosis(L, axis=0, fisher=True, bias=True)
    rob_kurt_dist = (np.dot(L - rob_kurt, rob_precision) * (L - rob_kurt)).sum(axis=1)

    return L, rob_mean, rob_cov, rob_dist, rob_precision, rob_skew, rob_skew_dist, rob_kurt, rob_kurt_dist


def cv_location_contamination(cv_df, cv_labels, location, precision):
    """

    :param cv_df: cross-validation data frame
    :param cv_labels: labels to evaluate prediction performance by contamination
    :param location: mean vector
    :param precision: inverse of covariance matrix
    :return: For all tested contamination rates, returns the rate in which the best F1-score were achieved.
    """

    contamination = round(0.00, 2)
    contamination_prediction_list = []
    labels = np.array(cv_labels)

    for i in range(40):
        contamination += 0.01
        contamination = round(contamination, 2)
        pred_label = predict_by_location_contamination(cv_df, location, precision, contamination)
        contamination_prediction_list.append((contamination, f1_score(labels, pred_label)))

    contamination_prediction_list.sort(key=lambda tup: tup[1], reverse=True)
    contamination_best_f1 = contamination_prediction_list[0][0]

    return contamination_best_f1


def cv_location_threshold(cv_df, cv_labels, location, precision, dist):
    """

    :param cv_df: cross-validation data frame
    :param cv_labels: labels to evaluate prediction performance by contamination
    :param location:
    :param precision:
    :return: For all tested contamination rates, returns the rate in which the best F1-score were achieved.
    """

    threshold_prediction_list = []
    labels = np.array(cv_labels)
    min_dist = min(dist)
    max_dist = max(dist)

    for m_threshold in np.linspace(min_dist, max_dist, 40):
        pred_label = predict_by_location_threshold(cv_df, location, precision, m_threshold)
        threshold_prediction_list.append((m_threshold, f1_score(labels, pred_label)))

    threshold_prediction_list.sort(key=lambda tup: tup[1], reverse=True)
    best_threshold = threshold_prediction_list[0][0]

    return best_threshold


def cv_skewness_contamination(cv_df, cv_labels, skewness, precision):
    """

    :param cv_df: cross-validation data frame
    :param cv_labels: labels to evaluate prediction performance by contamination
    :param skewness:
    :param precision:
    :return: For all tested contamination rates, returns the rate in which the best F1-score were achieved.
    """

    contamination = round(0.00, 2)
    contamination_prediction_list = []
    actual_anomalies = np.array(cv_labels)
    for i in range(40):
        contamination += 0.01
        contamination = round(contamination, 2)
        pred_label = predict_by_skewness_contamination(cv_df, precision, skewness, contamination)
        contamination_prediction_list.append((contamination, f1_score(actual_anomalies, pred_label)))

    contamination_prediction_list.sort(key=lambda tup: tup[1], reverse=True)
    best_contamination = contamination_prediction_list[0][0]

    return best_contamination


def cv_skewness_threshold(cv_df, cv_labels, skewness, precision, skew_dist):
    """

    :param cv_df:
    :param cv_labels: labels to evaluate prediction performance by contamination
    :param skewness:
    :param precision:
    :param skew_dist:
    :return:
    """

    threshold_prediction_list = []
    actual_anomalies = np.array(cv_labels)
    min_dist = min(skew_dist)
    max_dist = max(skew_dist)

    for m_threshold in np.linspace(min_dist, max_dist, 40):
        pred_label = predict_by_skewness_threshold(cv_df, precision, skewness, m_threshold)
        threshold_prediction_list.append((m_threshold, f1_score(actual_anomalies, pred_label)))

    threshold_prediction_list.sort(key=lambda tup: tup[1], reverse=True)
    best_threshold = threshold_prediction_list[0][0]

    return best_threshold


def cv_kurtosis_contamination(cv_df, cv_labels, m_kurtosis, precision):
    """

    :param df: cross-validation data frame
    :param location:
    :param precision:
    :return: For all tested contamination rates, returns the rate in which the best F1-score were achieved.
    """

    contamination = round(0.00, 2)
    contamination_prediction_list = []
    actual_anomalies = np.array(cv_labels)
    for i in range(40):
        contamination += 0.01
        contamination = round(contamination, 2)
        pred_label = predict_by_kurtosis_contamination(cv_df, precision, m_kurtosis, contamination)
        contamination_prediction_list.append((contamination, f1_score(actual_anomalies, pred_label)))

    contamination_prediction_list.sort(key=lambda tup: tup[1], reverse=True)
    best_contamination = contamination_prediction_list[0][0]

    return best_contamination


def cv_kurtosis_threshold(cv_df, cv_labels, kurtosis, precision, kurt_dist):
    """

    :param cv_df:
    :param cv_labels: labels to evaluate prediction performance by contamination
    :param kurtosis:
    :param precision:
    :param kurt_dist:
    :return:
    """

    threshold_prediction_list = []
    actual_anomalies = np.array(cv_labels)
    min_dist = min(kurt_dist)
    max_dist = max(kurt_dist)

    for m_threshold in np.linspace(min_dist, max_dist, 40):
        pred_label = predict_by_kurtosis_threshold(cv_df, precision, kurtosis, m_threshold)
        threshold_prediction_list.append((m_threshold, f1_score(actual_anomalies, pred_label)))

    threshold_prediction_list.sort(key=lambda tup: tup[1], reverse=True)
    best_threshold = threshold_prediction_list[0][0]

    return best_threshold


def md_rpca_prediction(test_df, location, precision, contamination):
    """

    :param test_df:
    :param location:
    :param precision:
    :param contamination:
    :return:
    """

    pred_label = np.full(test_df.shape[0], 0, dtype=int)
    if contamination is not None:
        # malhalanobis distance
        mahal_dist = pairwise_distances(test_df, location[np.newaxis, :], metric='mahalanobis', VI=precision)
        mahal_dist = np.reshape(mahal_dist, (len(test_df),)) ** 2  #MD squared
        # detect outliers
        contamination_threshold = np.percentile(mahal_dist,  100. * (1. - contamination))
        pred_label[mahal_dist > contamination_threshold] = 1
    else:
        raise NotImplementedError("You must provide a contamination rate.")

    return pred_label


def predict_by_location_centered_contamination(X, location, precision, contamination):
    """

    :param X:
    :param location:
    :param precision:
    :param contamination:
    :return:
    """

    pred_label = np.full(X.shape[0], 0, dtype=int)
    if contamination is not None:
        # malhalanobis distance
        X = X - location
        mahal_dist = pairwise_distances(X, location[np.newaxis, :], metric='mahalanobis', VI=precision)
        mahal_dist = np.reshape(mahal_dist, (len(X),)) ** 2  #MD squared
        # detect outliers
        contamination_threshold = np.percentile(mahal_dist,  100. * (1. - contamination))
        pred_label[mahal_dist > contamination_threshold] = 1
    else:
        raise NotImplementedError("You must provide a contamination rate.")

    return pred_label


def predict_by_location_threshold(X, location, precision, threshold):
    """

    :param X:
    :param location:
    :param precision:
    :param threshold:
    :return:
    """

    pred_label = np.full(X.shape[0], 0, dtype=int)

    # malhalanobis distance
    mahal_dist = pairwise_distances(X, location[np.newaxis, :], metric='mahalanobis', VI=precision)
    mahal_dist = np.reshape(mahal_dist, (len(X),)) ** 2  #MD squared
    # detect outliers
    pred_label[mahal_dist > threshold] = 1

    return pred_label


def sd_rpca_prediction(X, skewness, precision, contamination):
    """

    :param X:
    :param precision:
    :param skewness:
    :param contamination:
    :return:
    """

    pred_label = np.full(X.shape[0], 0, dtype=int)

    # malhalanobis distance
    mahal_dist = pairwise_distances(X, skewness[np.newaxis, :], metric='mahalanobis', VI=precision)
    mahal_dist = np.reshape(mahal_dist, (len(X),)) ** 2  # MD squared
    pred_skew_dist = -mahal_dist

    # detect outliers
    contamination_threshold = np.percentile(pred_skew_dist, 100. * contamination)
    pred_label[pred_skew_dist <= contamination_threshold] = 1

    return pred_label


def predict_by_skewness_centered_contamination(X, precision, skewness, contamination):
    """

    :param X:
    :param precision:
    :param skewness:
    :param contamination:
    :return:
    """

    pred_label = np.full(X.shape[0], 0, dtype=int)

    # skewness of the data
    X_skew = X - skew(X, axis=0, bias=True)

    # malhalanobis distance
    mahal_dist = pairwise_distances(X_skew, skewness[np.newaxis, :], metric='mahalanobis', VI=precision)
    mahal_dist = np.reshape(mahal_dist, (len(X_skew),)) ** 2 #MD squared
    pred_skew_dist = -mahal_dist

    # detect outliers
    contamination_threshold = np.percentile(pred_skew_dist, 100. * contamination)
    pred_label[pred_skew_dist <= contamination_threshold] = 1

    return pred_label


def predict_by_skewness_threshold(X, precision, skewness, threshold):
    """

    :param X:
    :param precision:
    :param skewness:
    :param threshold:
    :return:
    """

    pred_label = np.full(X.shape[0], 0, dtype=int)

    # malhalanobis distance
    mahal_dist = pairwise_distances(X, skewness[np.newaxis, :], metric='mahalanobis', VI=precision)
    mahal_dist = np.reshape(mahal_dist, (len(X),)) ** 2  # MD squared
    pred_skew_dist = -mahal_dist

    # detect outliers
    pred_label[pred_skew_dist <= threshold] = 1

    return pred_label


def predict_by_skewness_centered_threshold(X, precision, skewness, threshold):
    """

    :param X:
    :param precision:
    :param skewness:
    :param threshold:
    :return:
    """

    pred_label = np.full(X.shape[0], 0, dtype=int)

    # skewness of the data
    X_skew = X - skew(X, axis=0, bias=True)

    # malhalanobis distance
    mahal_dist = pairwise_distances(X_skew, skewness[np.newaxis, :], metric='mahalanobis', VI=precision)
    mahal_dist = np.reshape(mahal_dist, (len(X_skew),)) ** 2  # MD squared
    pred_skew_dist = -mahal_dist

    # detect outliers
    pred_label[pred_skew_dist <= threshold] = 1

    return pred_label


def kd_rpca_prediction(X, m_kurtosis, precision, contamination):
    """

    :param X:
    :param precision:
    :param m_kurtosis	:
    :param contamination:
    :return:
    """

    pred_label = np.full(X.shape[0], 0, dtype=int)

    # malhalanobis distance
    mahal_dist = pairwise_distances(X, m_kurtosis[np.newaxis, :], metric='mahalanobis', VI=precision)
    mahal_dist = np.reshape(mahal_dist, (len(X),)) ** 2  # MD squared
    pred_kurt_dist = -mahal_dist

    # detect outliers
    contamination_threshold = np.percentile(pred_kurt_dist, 100. * contamination)
    pred_label[pred_kurt_dist <= contamination_threshold] = 1

    return pred_label


def predict_by_kurtosis_centered_contamination(X, precision, m_kurtosis, contamination):
    """

    :param X:
    :param precision:
    :param m_kurtosis	:
    :param contamination:
    :return:
    """

    pred_label = np.full(X.shape[0], 0, dtype=int)

    # m_kurtosis	 of the data
    X_kurt = X - kurtosis(X, axis=0, bias=True)

    # malhalanobis distance
    mahal_dist = pairwise_distances(X_kurt, m_kurtosis[np.newaxis, :], metric='mahalanobis', VI=precision)
    mahal_dist = np.reshape(mahal_dist, (len(X_kurt),)) ** 2 #MD squared
    pred_kurt_dist = -mahal_dist

    # detect outliers
    contamination_threshold = np.percentile(pred_kurt_dist, 100. * contamination)
    pred_label[pred_kurt_dist <= contamination_threshold] = 1

    return pred_label


def predict_by_kurtosis_threshold(X, precision, m_kurtosis, threshold):
    """

    :param X:
    :param precision:
    :param m_kurtosis	:
    :param threshold:
    :return:
    """

    pred_label = np.full(X.shape[0], 0, dtype=int)

    # malhalanobis distance
    mahal_dist = pairwise_distances(X, m_kurtosis[np.newaxis, :], metric='mahalanobis', VI=precision)
    mahal_dist = np.reshape(mahal_dist, (len(X),)) ** 2  # MD squared
    pred_kurt_dist = -mahal_dist

    # detect outliers
    pred_label[pred_kurt_dist <= threshold] = 1

    return pred_label


def predict_by_kurtosis_centered_threshold(X, precision, m_kurtosis, threshold):
    """

    :param X:
    :param precision:
    :param m_kurtosis	:
    :param threshold:
    :return:
    """

    pred_label = np.full(X.shape[0], 0, dtype=int)

    # m_kurtosis	 of the data
    X_kurt = X - kurtosis(X, axis=0, bias=True)

    # malhalanobis distance
    mahal_dist = pairwise_distances(X_kurt, m_kurtosis[np.newaxis, :], metric='mahalanobis', VI=precision)
    mahal_dist = np.reshape(mahal_dist, (len(X_kurt),)) ** 2  # MD squared
    pred_kurt_dist = -mahal_dist

    # detect outliers
    pred_label[pred_kurt_dist <= threshold] = 1

    return pred_label

# Define the number of inliers and outliers
n_samples = 2400
outliers_fraction = 0.33

result_path = 'output/synthetic/'

# Initialize the data
n_inliers = int((1. - outliers_fraction) * n_samples)
n_outliers = int(n_samples - n_inliers)
ground_truth = np.zeros(n_samples, dtype=int)
ground_truth[-n_outliers:] = 1 #put outliers into the end

print('n_samples:', n_samples)
print('n_inliers:', n_inliers)
print('n_outliers:', n_outliers)
print('ground_truth:', ground_truth.shape)

# Data generation
np.random.seed(11)
X1 = 0.3 * np.random.randn(n_inliers, 2)
X2 = 0.3 * np.random.randn(n_inliers, 2) 
Xgaussian = np.r_[X1, X2]
Xgaussian_t = Xgaussian[:n_inliers]
Xgaussian = Xgaussian[n_inliers:]
print('Xgaussian:', Xgaussian.shape)
sns.distplot(Xgaussian[:, 0], color="black", label="Feature 1")
# sns.distplot(Xgaussian[:, 1], color="red", label="Feature 2")
plt.legend()
plt.savefig("%sXgaussian.png" % result_path)
# plt.show()
plt.close()
print('Xgaussian_t:', Xgaussian_t.shape)
sns.distplot(Xgaussian_t[:, 0], color="black", label="Feature 1")
# sns.distplot(Xgaussian_t[:, 1], color="red", label="Feature 2")
plt.legend()
plt.savefig("%sXgaussian_t.png" % result_path)
# plt.show()
plt.close()

# Append outliers
Cuniform = np.random.uniform(low=-6, high=6, size=(n_outliers, 2))
print('Cuniform:', Cuniform.shape)
sns.distplot(Cuniform[:, 0], color="pink", label="Feature 1")
# sns.distplot(Cuniform[:, 1], color="red", label="Feature 2")
plt.legend()
plt.savefig("%sCuniform.png" % result_path)
# plt.show()
plt.close()
Xgu = np.r_[Xgaussian,Cuniform]
print('Xgu:', Xgu.shape)
sns.distplot(Xgu[:, 0], color="skyblue", label="Feature 1")
# sns.distplot(Xgu[:, 1], color="blue", label="Feature 2")
plt.legend()
plt.savefig("%sXgu.png" % result_path)
# plt.show()
plt.close()

print('### Train: Gaussian')
print('### Test: Gaussian contaminated by Uniform')
# initialize a set of detectors for LSCP
detector_list = [LOF(n_neighbors=5), LOF(n_neighbors=10), LOF(n_neighbors=15),
                 LOF(n_neighbors=20), LOF(n_neighbors=25), LOF(n_neighbors=30),
                 LOF(n_neighbors=35), LOF(n_neighbors=40), LOF(n_neighbors=45),
                 LOF(n_neighbors=50)]

random_state = np.random.RandomState(42)

# Define nine outlier detection tools to be compared
classifiers = {
    'Angle-based Outlier Detector (ABOD)':
        ABOD(contamination=outliers_fraction),
    'Cluster-based Local Outlier Factor (CBLOF)':
        CBLOF(contamination=outliers_fraction,
              check_estimator=False, random_state=random_state),
    'Feature Bagging':
        FeatureBagging(LOF(n_neighbors=35),
                       contamination=outliers_fraction,
                       random_state=random_state),
    'Histogram-base Outlier Detection (HBOS)': HBOS(
        contamination=outliers_fraction),
    'Isolation Forest': IForest(contamination=outliers_fraction,
                                random_state=random_state),
    'K Nearest Neighbors (KNN)': KNN(
        contamination=outliers_fraction),
    'Average KNN': KNN(method='mean',
                       contamination=outliers_fraction),
    'Local Outlier Factor (LOF)':
        LOF(n_neighbors=35, contamination=outliers_fraction),
    'Minimum Covariance Determinant (MCD)': MCD(
        contamination=outliers_fraction, random_state=random_state),
    'One-class SVM (OCSVM)': OCSVM(contamination=outliers_fraction),
    'Principal Component Analysis (PCA)': PCA(
        contamination=outliers_fraction, random_state=random_state),
   # 'Locally Selective Combination (LSCP)': LSCP(
   #     detector_list, contamination=outliers_fraction,
   #     random_state=random_state)
}


for i, (clf_name, clf) in enumerate(classifiers.items()):    
    # fit the data and tag outliers
    clf.fit(Xgu)
    scores_pred = clf.decision_function(Xgu) * -1
    y_pred = clf.predict(Xgu)
    threshold = percentile(scores_pred, 100 * outliers_fraction)    
    f1 = f1_score(y_pred, ground_truth, average='macro')
    print(i + 1, 'fitting', clf_name, f1)
    
# Train
r_L, r_mu, r_cov, r_dist, r_precision, r_skew, _, r_kurt, _ = fit_m_rpca(Xgaussian_t)
# Testing md-rpca
md_pred_label = md_rpca_prediction(Xgu, r_mu, r_precision, outliers_fraction)
md_f1 = f1_score(ground_truth, md_pred_label)
print('md_rpca - F1: %f' % (md_f1))
# Testing sd-rpca
sd_pred_label = sd_rpca_prediction(Xgu, r_skew, r_precision, outliers_fraction)
sd_f1 = f1_score(ground_truth, sd_pred_label)
print('sd_rpca - F1: %f' % (sd_f1))
# Testing kd-rpca
kd_pred_label = kd_rpca_prediction(Xgu, r_kurt, r_precision, outliers_fraction)
kd_f1 = f1_score(ground_truth, kd_pred_label)
print('kd_rpca - F1: %f' % (kd_f1))

# Train
r_L, r_mu, r_cov, r_dist, r_precision, r_skew, _, r_kurt, _ = fit_m_rpca(Xgu)
# Testing md-rpca
md_pred_label = md_rpca_prediction(Xgu, r_mu, r_precision, outliers_fraction)
md_f1 = f1_score(ground_truth, md_pred_label)
print('md_rpca - F1: %f' % (md_f1))
# Testing sd-rpca
sd_pred_label = sd_rpca_prediction(Xgu, r_skew, r_precision, outliers_fraction)
sd_f1 = f1_score(ground_truth, sd_pred_label)
print('sd_rpca - F1: %f' % (sd_f1))
# Testing kd-rpca
kd_pred_label = kd_rpca_prediction(Xgu, r_kurt, r_precision, outliers_fraction)
kd_f1 = f1_score(ground_truth, kd_pred_label)
print('kd_rpca - F1: %f' % (kd_f1))

# Pareto distribution
np.random.seed(42)
a = 3.  # shape
m = 1.  # mode
p1 = (np.random.pareto(a, n_inliers) + 1) * m
p2 = (np.random.pareto(a, n_inliers) + 1) * m
Xpareto = np.vstack((p1,p2)).transpose()

p1 = (np.random.pareto(a, n_inliers) + 1) * m
p2 = (np.random.pareto(a, n_inliers) + 1) * m
Xpareto_t = np.vstack((p1,p2)).transpose()

# Add outliers
Cgaussian = 0.1 * np.random.randn(n_outliers, 2)
Xpg = np.r_[Xpareto, Cgaussian]

print('Xpareto:', Xpareto.shape)
print('Xpareto_t:', Xpareto_t.shape)
print('Cgaussian:', Cgaussian.shape)
print('Xpg:', Xpg.shape)

count, bins, _ = plt.hist(Xpareto[:,0], 100, density=True)
fit = a*m**a / bins**(a+1)
plt.plot(bins, max(count)*fit/max(fit), linewidth=2, color='r', label="Pareto")
plt.legend()
plt.savefig("%sXhist_pareto.png" % result_path)
# plt.show()
plt.close()

count, bins, _ = plt.hist(Xpg[:,0], 100, density=True)
fit = a*m**a / bins**(a+1)
plt.plot(bins, linewidth=2, color='r', label="Pareto")
plt.legend()
plt.savefig("%sXhist_pareto_c.png" % result_path)
# plt.show()
plt.close()

print('### Train: Pareto')
print('### Test: Pareto contaminated by Gaussian')
# Compare given detectors under given settings
# Initialize the data
xx, yy = np.meshgrid(np.linspace(-7, 7, 100), np.linspace(-7, 7, 100))
ground_truth = np.zeros(n_samples, dtype=int)
ground_truth[-n_outliers:] = 1

# initialize a set of detectors for LSCP
detector_list = [LOF(n_neighbors=5), LOF(n_neighbors=10), LOF(n_neighbors=15),
                 LOF(n_neighbors=20), LOF(n_neighbors=25), LOF(n_neighbors=30),
                 LOF(n_neighbors=35), LOF(n_neighbors=40), LOF(n_neighbors=45),
                 LOF(n_neighbors=50)]

# Show the statics of the data
print('Number of inliers: %i' % n_inliers)
print('Number of outliers: %i' % n_outliers)
print(
    'Ground truth shape is {shape}. Outlier are 1 and inliers are 0.\n'.format(
        shape=ground_truth.shape))
print(ground_truth, '\n')

random_state = np.random.RandomState(42)
# Define nine outlier detection tools to be compared
classifiers = {
    'Angle-based Outlier Detector (ABOD)':
        ABOD(contamination=outliers_fraction),
    'Cluster-based Local Outlier Factor (CBLOF)':
        CBLOF(contamination=outliers_fraction,
              check_estimator=False, random_state=random_state),
    'Feature Bagging':
        FeatureBagging(LOF(n_neighbors=35),
                       contamination=outliers_fraction,
                       random_state=random_state),
    'Histogram-base Outlier Detection (HBOS)': HBOS(
        contamination=outliers_fraction),
    'Isolation Forest': IForest(contamination=outliers_fraction,
                                random_state=random_state),
    'K Nearest Neighbors (KNN)': KNN(
        contamination=outliers_fraction),
    'Average KNN': KNN(method='mean',
                       contamination=outliers_fraction),
    # 'Median KNN': KNN(method='median',
    #                   contamination=outliers_fraction),
    'Local Outlier Factor (LOF)':
        LOF(n_neighbors=35, contamination=outliers_fraction),
    # 'Local Correlation Integral (LOCI)':
    #     LOCI(contamination=outliers_fraction),
    'Minimum Covariance Determinant (MCD)': MCD(
        contamination=outliers_fraction, random_state=random_state),
    'One-class SVM (OCSVM)': OCSVM(contamination=outliers_fraction),
    'Principal Component Analysis (PCA)': PCA(
        contamination=outliers_fraction, random_state=random_state),
    # 'Stochastic Outlier Selection (SOS)': SOS(
    #     contamination=outliers_fraction),
    #'Locally Selective Combination (LSCP)': LSCP(
    #    detector_list, contamination=outliers_fraction,
    #    random_state=random_state)
}

clusters_separation = [0]

# Fit the models with the generated data and
# compare model performances
for i, offset in enumerate(clusters_separation):
    
    # Fit the model
    plt.figure(figsize=(15, 12))
    for i, (clf_name, clf) in enumerate(classifiers.items()):        
        # fit the data and tag outliers
        clf.fit(Xpg)
        scores_pred = clf.decision_function(Xpg) * -1
        y_pred = clf.predict(Xpg)
        threshold = percentile(scores_pred, 100 * outliers_fraction)
        #n_errors = (y_pred != ground_truth).sum()
        f1 = f1_score(y_pred, ground_truth, average='macro')
        print(i + 1, 'fitting', clf_name, f1)  
        
#        # plot the levels lines and the points
#        Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()]) * -1
#        Z = Z.reshape(xx.shape)
#        subplot = plt.subplot(3, 4, i + 1)
#        subplot.contourf(xx, yy, Z, levels=np.linspace(Z.min(), threshold, 7),
#                         cmap=plt.cm.Blues_r)
#        a = subplot.contour(xx, yy, Z, levels=[threshold],
#                            linewidths=2, colors='red')
#        subplot.contourf(xx, yy, Z, levels=[threshold, Z.max()],
#                         colors='orange')
#        b = subplot.scatter(X[:-n_outliers, 0], X[:-n_outliers, 1], c='white',
#                            s=20, edgecolor='k')
#        c = subplot.scatter(X[-n_outliers:, 0], X[-n_outliers:, 1], c='black',
#                            s=20, edgecolor='k')
#        subplot.axis('tight')
#        subplot.legend(
#            [a.collections[0], b, c],
#            ['learned decision function', 'true inliers', 'true outliers'],
#            prop=matplotlib.font_manager.FontProperties(size=10),
#            loc='lower right')
#        subplot.set_xlabel("%d. %s (errors: %d)" % (i + 1, clf_name, n_errors))
#       subplot.set_xlim((-7, 7))
#        subplot.set_ylim((-7, 7))
#    plt.subplots_adjust(0.04, 0.1, 0.96, 0.94, 0.1, 0.26)
#    plt.suptitle("Outlier detection")
#plt.savefig('ALL.png', dpi=300)
#plt.show()


# Train
r_L, r_mu, r_cov, r_dist, r_precision, r_skew, _, r_kurt, _ = fit_m_rpca(Xpareto_t)
# Testing md-rpca
md_pred_label = md_rpca_prediction(Xpg, r_mu, r_precision, outliers_fraction)
md_f1 = f1_score(ground_truth, md_pred_label)
print('md_rpca - F1: %f' % (md_f1))
# Testing sd-rpca
sd_pred_label = sd_rpca_prediction(Xpg, r_skew, r_precision, outliers_fraction)
sd_f1 = f1_score(ground_truth, sd_pred_label)
print('sd_rpca - F1: %f' % (sd_f1))
# Testing kd-rpca
kd_pred_label = kd_rpca_prediction(Xpg, r_kurt, r_precision, outliers_fraction)
kd_f1 = f1_score(ground_truth, kd_pred_label)
print('kd_rpca - F1: %f' % (kd_f1))

# Train
r_L, r_mu, r_cov, r_dist, r_precision, r_skew, _, r_kurt, _ = fit_m_rpca(Xpg)
# Testing md-rpca
md_pred_label = md_rpca_prediction(Xpg, r_mu, r_precision, outliers_fraction)
md_f1 = f1_score(ground_truth, md_pred_label)
print('md_rpca - F1: %f' % (md_f1))
# Testing sd-rpca
sd_pred_label = sd_rpca_prediction(Xpg, r_skew, r_precision, outliers_fraction)
sd_f1 = f1_score(ground_truth, sd_pred_label)
print('sd_rpca - F1: %f' % (sd_f1))
# Testing kd-rpca
kd_pred_label = kd_rpca_prediction(Xpg, r_kurt, r_precision, outliers_fraction)
kd_f1 = f1_score(ground_truth, kd_pred_label)
print('kd_rpca - F1: %f' % (kd_f1))

print('Xpareto', Xpareto.shape)
sns.distplot(Xpareto[:, 0] , color="blue", label="Feature 1")
plt.legend()
plt.savefig("%sXpareto1.png" % result_path)
# plt.show()
plt.close()
sns.distplot(Xpareto[:, 1] , color="green", label="Feature 2")
plt.legend()
plt.savefig("%sXpareto2.png" % result_path)
# plt.show()
plt.close()
print('Cgaussian', Cgaussian.shape)
sns.distplot(Cgaussian[:, 0] , color="firebrick", label="Feature 1")
# sns.distplot(Cgaussian[:, 1] , color="salmon", label="Feature 2")
plt.legend()
plt.savefig("%sCgaussian.png" % result_path)
# plt.show()
plt.close()
print('Xpg', Xpg.shape)
sns.distplot(Xpg[:, 0] , color="black", label="Feature 1")
# sns.distplot(Xpg[:, 1] , color="gray", label="Feature 2")
plt.legend()
plt.savefig("%sXpg.png" % result_path)
# plt.show()
plt.close()

print('### Train: Pareto conaminated by Gaussian')
print('### Test: Pareto contaminated by Gaussian')
np.random.seed(42)
Ct = 0.1 * np.random.randn(n_outliers * 2, 2)
Xpareto_tc = np.r_[Xpareto_t, Ct]

# Compare given detectors under given settings
# Initialize the data
xx, yy = np.meshgrid(np.linspace(-7, 7, 100), np.linspace(-7, 7, 100))
ground_truth = np.zeros(n_samples, dtype=int)
ground_truth[-n_outliers:] = 1

# initialize a set of detectors for LSCP
detector_list = [LOF(n_neighbors=5), LOF(n_neighbors=10), LOF(n_neighbors=15),
                 LOF(n_neighbors=20), LOF(n_neighbors=25), LOF(n_neighbors=30),
                 LOF(n_neighbors=35), LOF(n_neighbors=40), LOF(n_neighbors=45),
                 LOF(n_neighbors=50)]

# Show the statics of the data
print('Number of inliers: %i' % n_inliers)
print('Number of outliers: %i' % n_outliers)
print(
    'Ground truth shape is {shape}. Outlier are 1 and inliers are 0.\n'.format(
        shape=ground_truth.shape))
print(ground_truth, '\n')

random_state = np.random.RandomState(42)
# Define nine outlier detection tools to be compared
classifiers = {
    'Angle-based Outlier Detector (ABOD)':
        ABOD(contamination=outliers_fraction),
    'Cluster-based Local Outlier Factor (CBLOF)':
        CBLOF(contamination=outliers_fraction,
              check_estimator=False, random_state=random_state),
    'Feature Bagging':
        FeatureBagging(LOF(n_neighbors=35),
                       contamination=outliers_fraction,
                       random_state=random_state),
    'Histogram-base Outlier Detection (HBOS)': HBOS(
        contamination=outliers_fraction),
    'Isolation Forest': IForest(contamination=outliers_fraction,
                                random_state=random_state),
    'K Nearest Neighbors (KNN)': KNN(
        contamination=outliers_fraction),
    'Average KNN': KNN(method='mean',
                       contamination=outliers_fraction),
    # 'Median KNN': KNN(method='median',
    #                   contamination=outliers_fraction),
    'Local Outlier Factor (LOF)':
        LOF(n_neighbors=35, contamination=outliers_fraction),
    # 'Local Correlation Integral (LOCI)':
    #     LOCI(contamination=outliers_fraction),
    'Minimum Covariance Determinant (MCD)': MCD(
        contamination=outliers_fraction, random_state=random_state),
    'One-class SVM (OCSVM)': OCSVM(contamination=outliers_fraction),
    'Principal Component Analysis (PCA)': PCA(
        contamination=outliers_fraction, random_state=random_state),
    # 'Stochastic Outlier Selection (SOS)': SOS(
    #     contamination=outliers_fraction),
    #'Locally Selective Combination (LSCP)': LSCP(
    #    detector_list, contamination=outliers_fraction,
    #    random_state=random_state)
}

clusters_separation = [0]

# Fit the models with the generated data and
# compare model performances
for i, offset in enumerate(clusters_separation):
    # Add outliers
    
    # Fit the model
    plt.figure(figsize=(15, 12))
    for i, (clf_name, clf) in enumerate(classifiers.items()):        
        # fit the data and tag outliers
        clf.fit(Xpg)
        scores_pred = clf.decision_function(Xpg) * -1
        y_pred = clf.predict(Xpg)
        threshold = percentile(scores_pred, 100 * outliers_fraction)
        #n_errors = (y_pred != ground_truth).sum()
        f1 = f1_score(y_pred, ground_truth, average='macro')
        print(i + 1, 'fitting', clf_name, f1)  
        
#        # plot the levels lines and the points
#        Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()]) * -1
#        Z = Z.reshape(xx.shape)
#        subplot = plt.subplot(3, 4, i + 1)
#        subplot.contourf(xx, yy, Z, levels=np.linspace(Z.min(), threshold, 7),
#                         cmap=plt.cm.Blues_r)
#        a = subplot.contour(xx, yy, Z, levels=[threshold],
#                            linewidths=2, colors='red')
#        subplot.contourf(xx, yy, Z, levels=[threshold, Z.max()],
#                         colors='orange')
#        b = subplot.scatter(X[:-n_outliers, 0], X[:-n_outliers, 1], c='white',
#                            s=20, edgecolor='k')
#        c = subplot.scatter(X[-n_outliers:, 0], X[-n_outliers:, 1], c='black',
#                            s=20, edgecolor='k')
#        subplot.axis('tight')
#        subplot.legend(
#            [a.collections[0], b, c],
#            ['learned decision function', 'true inliers', 'true outliers'],
#            prop=matplotlib.font_manager.FontProperties(size=10),
#            loc='lower right')
#        subplot.set_xlabel("%d. %s (errors: %d)" % (i + 1, clf_name, n_errors))
#       subplot.set_xlim((-7, 7))
#        subplot.set_ylim((-7, 7))
#    plt.subplots_adjust(0.04, 0.1, 0.96, 0.94, 0.1, 0.26)
#    plt.suptitle("Outlier detection")
#plt.savefig('ALL.png', dpi=300)
#plt.show()

# Train
r_L, r_mu, r_cov, r_dist, r_precision, r_skew, _, r_kurt, _ = fit_m_rpca(Xpareto_tc)
# Testing md-rpca
md_pred_label = md_rpca_prediction(Xpg, r_mu, r_precision, outliers_fraction)
md_f1 = f1_score(ground_truth, md_pred_label)
print('md_rpca - F1: %f' % (md_f1))
# Testing sd-rpca
sd_pred_label = sd_rpca_prediction(Xpg, r_skew, r_precision, outliers_fraction)
sd_f1 = f1_score(ground_truth, sd_pred_label)
print('sd_rpca - F1: %f' % (sd_f1))
# Testing kd-rpca
kd_pred_label = kd_rpca_prediction(Xpg, r_kurt, r_precision, outliers_fraction)
kd_f1 = f1_score(ground_truth, kd_pred_label)
print('kd_rpca - F1: %f' % (kd_f1))

# Train
r_L, r_mu, r_cov, r_dist, r_precision, r_skew, _, r_kurt, _ = fit_m_rpca(Xpg)
# Testing md-rpca
md_pred_label = md_rpca_prediction(Xpg, r_mu, r_precision, outliers_fraction)
md_f1 = f1_score(ground_truth, md_pred_label)
print('md_rpca - F1: %f' % (md_f1))
# Testing sd-rpca
sd_pred_label = sd_rpca_prediction(Xpg, r_skew, r_precision, outliers_fraction)
sd_f1 = f1_score(ground_truth, sd_pred_label)
print('sd_rpca - F1: %f' % (sd_f1))
# Testing kd-rpca
kd_pred_label = kd_rpca_prediction(Xpg, r_kurt, r_precision, outliers_fraction)
kd_f1 = f1_score(ground_truth, kd_pred_label)
print('kd_rpca - F1: %f' % (kd_f1))

print('Xpareto_tc', Xpareto_tc.shape)
sns.distplot(Xpareto_tc[:, 0] , color="yellow", label="Feature 1")
# sns.distplot(Xpareto_tc[:, 1] , color="green", label="Feature 2")
plt.legend()
plt.savefig("%sXpareto_tc.png" % result_path)
# plt.show()
plt.close()
print('Xpareto', Xpareto.shape)
sns.distplot(Xpareto[:, 0] , color="blue", label="Feature 1")
# sns.distplot(Xpareto[:, 1] , color="green", label="Feature 2")
plt.legend()
plt.savefig("%sXpareto.png" % result_path)
# plt.show()
plt.close()
print('Cgaussian', Cgaussian.shape)
sns.distplot(Cgaussian[:, 0] , color="firebrick", label="Feature 1")
# sns.distplot(Cgaussian[:, 1] , color="salmon", label="Feature 2")
plt.legend()
plt.savefig("%sCgaussian.png" % result_path)
# plt.show()
plt.close()
print('Xpg', Xpg.shape)
sns.distplot(Xpg[:, 0] , color="black", label="Feature 1")
# sns.distplot(Xpg[:, 1] , color="gray", label="Feature 2")
plt.legend()
plt.savefig("%sXpg.png" % result_path)
# plt.show()
plt.close()

mu = 0.    # mean 
sigma = 1. # standard deviation

ln1 = np.random.lognormal(mu, sigma, n_inliers)
count, bins, ignored = plt.hist(ln1, 100, density=True, align='mid')
x = np.linspace(min(bins), max(bins), 10000)
pdf = (np.exp(-(np.log(x) - mu)**2 / (2 * sigma**2)) / (x * sigma * np.sqrt(2 * np.pi)))
plt.plot(x, pdf, linewidth=2, color='r', label="Lognormal")
plt.axis('tight')
plt.legend()
plt.savefig("%slognormal1.png" % result_path)
# plt.show()
plt.close()

ln2 = np.random.lognormal(mu, sigma, n_inliers)
count, bins, ignored = plt.hist(ln2, 100, density=True, align='mid')
x = np.linspace(min(bins), max(bins), 10000)
pdf = (np.exp(-(np.log(x) - mu)**2 / (2 * sigma**2)) / (x * sigma * np.sqrt(2 * np.pi)))
plt.plot(x, pdf, linewidth=2, color='r', label="Lognormal")
plt.axis('tight')
plt.legend()
plt.savefig("%slognormal2.png" % result_path)
# plt.show()
plt.close()

Xlogn = np.vstack((ln1,ln2)).transpose()
print('Xlogn:', Xlogn.shape)

# Add gaussian outliers
Xlogng = np.r_[Xlogn, Cgaussian]

ln1 = np.random.lognormal(mu, sigma, n_inliers)
ln2 = np.random.lognormal(mu, sigma, n_inliers)
Xlogn_t = np.vstack((ln1,ln2)).transpose()

print('### Train: Log-normal')
print('### Test: Log-normal contaminated by Gaussian')

mu = 0.    # mean
sigma = 1. # standard deviation

ln1 = np.random.lognormal(mu, sigma, n_inliers)
count, bins, ignored = plt.hist(Xlogn[:,0], 100, density=True, align='mid')
x = np.linspace(min(bins), max(bins), 10000)
pdf = (np.exp(-(np.log(x) - mu)**2 / (2 * sigma**2)) / (x * sigma * np.sqrt(2 * np.pi)))
plt.plot(x, pdf, linewidth=2, color='r', label="Lognormal")
plt.axis('tight')
plt.legend()
plt.savefig("%slognormal3.png" % result_path)
# plt.show()
plt.close()

ln2 = np.random.lognormal(mu, sigma, n_inliers)
count, bins, ignored = plt.hist(Xlogng[:,0], 100, density=True, align='mid')
x = np.linspace(min(bins), max(bins), 10000)
pdf = (np.exp(-(np.log(x) - mu)**2 / (2 * sigma**2)) / (x * sigma * np.sqrt(2 * np.pi)))
plt.plot(x, pdf, linewidth=2, color='r', label="Lognormal")
plt.axis('tight')
plt.legend()
plt.savefig("%slognormal4.png" % result_path)
# plt.show()
plt.close()

Xlogn = np.vstack((ln1,ln2)).transpose()
print('Xlogn:', Xlogn.shape)

# Add gaussian outliers
Xlogng = np.r_[Xlogn, Cgaussian]

ln1 = np.random.lognormal(mu, sigma, n_inliers)
ln2 = np.random.lognormal(mu, sigma, n_inliers)
Xlogn_t = np.vstack((ln1,ln2)).transpose()

# Compare given detectors under given settings
# Initialize the data
xx, yy = np.meshgrid(np.linspace(-7, 7, 100), np.linspace(-7, 7, 100))
ground_truth = np.zeros(n_samples, dtype=int)
ground_truth[-n_outliers:] = 1

# initialize a set of detectors for LSCP
detector_list = [LOF(n_neighbors=5), LOF(n_neighbors=10), LOF(n_neighbors=15),
                 LOF(n_neighbors=20), LOF(n_neighbors=25), LOF(n_neighbors=30),
                 LOF(n_neighbors=35), LOF(n_neighbors=40), LOF(n_neighbors=45),
                 LOF(n_neighbors=50)]

# Show the statics of the data
print('Number of inliers: %i' % n_inliers)
print('Number of outliers: %i' % n_outliers)
print(
    'Ground truth shape is {shape}. Outlier are 1 and inliers are 0.\n'.format(
        shape=ground_truth.shape))
print(ground_truth, '\n')

random_state = np.random.RandomState(42)
# Define nine outlier detection tools to be compared
classifiers = {
    'Angle-based Outlier Detector (ABOD)':
        ABOD(contamination=outliers_fraction),
    'Cluster-based Local Outlier Factor (CBLOF)':
        CBLOF(contamination=outliers_fraction,
              check_estimator=False, random_state=random_state),
    'Feature Bagging':
        FeatureBagging(LOF(n_neighbors=35),
                       contamination=outliers_fraction,
                       random_state=random_state),
    'Histogram-base Outlier Detection (HBOS)': HBOS(
        contamination=outliers_fraction),
    'Isolation Forest': IForest(contamination=outliers_fraction,
                                random_state=random_state),
    'K Nearest Neighbors (KNN)': KNN(
        contamination=outliers_fraction),
    'Average KNN': KNN(method='mean',
                       contamination=outliers_fraction),
    # 'Median KNN': KNN(method='median',
    #                   contamination=outliers_fraction),
    'Local Outlier Factor (LOF)':
        LOF(n_neighbors=35, contamination=outliers_fraction),
    # 'Local Correlation Integral (LOCI)':
    #     LOCI(contamination=outliers_fraction),
    'Minimum Covariance Determinant (MCD)': MCD(
        contamination=outliers_fraction, random_state=random_state),
    'One-class SVM (OCSVM)': OCSVM(contamination=outliers_fraction),
    'Principal Component Analysis (PCA)': PCA(
        contamination=outliers_fraction, random_state=random_state),
    # 'Stochastic Outlier Selection (SOS)': SOS(
    #     contamination=outliers_fraction),
    #'Locally Selective Combination (LSCP)': LSCP(
    #    detector_list, contamination=outliers_fraction,
    #    random_state=random_state)
}

clusters_separation = [0]

# Fit the models with the generated data and
# compare model performances
for i, offset in enumerate(clusters_separation):
    
    # Fit the model
    plt.figure(figsize=(15, 12))
    for i, (clf_name, clf) in enumerate(classifiers.items()):        
        # fit the data and tag outliers
        clf.fit(Xlogng)
        scores_pred = clf.decision_function(Xlogng) * -1
        y_pred = clf.predict(Xlogng)
        threshold = percentile(scores_pred, 100 * outliers_fraction)
        #n_errors = (y_pred != ground_truth).sum()
        f1 = f1_score(y_pred, ground_truth, average='macro')
        print(i + 1, 'fitting', clf_name, f1)  
        
#        # plot the levels lines and the points
#        Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()]) * -1
#        Z = Z.reshape(xx.shape)
#        subplot = plt.subplot(3, 4, i + 1)
#        subplot.contourf(xx, yy, Z, levels=np.linspace(Z.min(), threshold, 7),
#                         cmap=plt.cm.Blues_r)
#        a = subplot.contour(xx, yy, Z, levels=[threshold],
#                            linewidths=2, colors='red')
#        subplot.contourf(xx, yy, Z, levels=[threshold, Z.max()],
#                         colors='orange')
#        b = subplot.scatter(X[:-n_outliers, 0], X[:-n_outliers, 1], c='white',
#                            s=20, edgecolor='k')
#        c = subplot.scatter(X[-n_outliers:, 0], X[-n_outliers:, 1], c='black',
#                            s=20, edgecolor='k')
#        subplot.axis('tight')
#        subplot.legend(
#            [a.collections[0], b, c],
#            ['learned decision function', 'true inliers', 'true outliers'],
#            prop=matplotlib.font_manager.FontProperties(size=10),
#            loc='lower right')
#        subplot.set_xlabel("%d. %s (errors: %d)" % (i + 1, clf_name, n_errors))
#       subplot.set_xlim((-7, 7))
#        subplot.set_ylim((-7, 7))
#    plt.subplots_adjust(0.04, 0.1, 0.96, 0.94, 0.1, 0.26)
#    plt.suptitle("Outlier detection")
#plt.savefig('ALL.png', dpi=300)
#plt.show()


# Train
r_L, r_mu, r_cov, r_dist, r_precision, r_skew, _, r_kurt, _ = fit_m_rpca(Xlogn_t)
# Testing md-rpca
md_pred_label = md_rpca_prediction(Xlogng, r_mu, r_precision, outliers_fraction)
md_f1 = f1_score(ground_truth, md_pred_label)
print('md_rpca_prediction - F1: %f' % (md_f1))
# Testing sd-rpca
sd_pred_label = sd_rpca_prediction(Xlogng, r_skew, r_precision, outliers_fraction)
sd_f1 = f1_score(ground_truth, sd_pred_label)
print('sd_rpca_prediction - F1: %f' % (sd_f1))
# Testing kd-rpca
kd_pred_label = kd_rpca_prediction(Xlogng, r_kurt, r_precision, outliers_fraction)
kd_f1 = f1_score(ground_truth, kd_pred_label)
print('kd_rpca_prediction - F1: %f' % (kd_f1))

# Train
r_L, r_mu, r_cov, r_dist, r_precision, r_skew, _, r_kurt, _ = fit_m_rpca(Xlogng)
# Testing md-rpca
md_pred_label = md_rpca_prediction(Xlogng, r_mu, r_precision, outliers_fraction)
md_f1 = f1_score(ground_truth, md_pred_label)
print('md_rpca_prediction - F1: %f' % (md_f1))
# Testing sd-rpca
sd_pred_label = sd_rpca_prediction(Xlogng, r_skew, r_precision, outliers_fraction)
sd_f1 = f1_score(ground_truth, sd_pred_label)
print('sd_rpca_prediction - F1: %f' % (sd_f1))
# Testing kd-rpca
kd_pred_label = kd_rpca_prediction(Xlogng, r_kurt, r_precision, outliers_fraction)
kd_f1 = f1_score(ground_truth, kd_pred_label)
print('kd_rpca_prediction - F1: %f' % (kd_f1))

print('Xlogn', Xlogn.shape)
sns.distplot(Xlogn[:, 0] , color="blue", label="Feature 1")
# sns.distplot(Xlogn[:, 1] , color="green", label="Feature 2")
plt.legend()
plt.savefig("%sXlogn.png" % result_path)
# plt.show()
plt.close()
print('Xlogn_t', Xlogn_t.shape)
sns.distplot(Xlogn_t[:, 0] , color="yellow", label="Feature 1")
# sns.distplot(Xlogn_t[:, 1] , color="green", label="Feature 2")
plt.legend()
plt.savefig("%sXlogn_t.png" % result_path)
# plt.show()
plt.close()
print('Cgaussian', Cgaussian.shape)
sns.distplot(Cgaussian[:, 0] , color="firebrick", label="Feature 1")
# sns.distplot(Cgaussian[:, 1] , color="salmon", label="Feature 2")
plt.legend()
plt.savefig("%sCgaussian.png" % result_path)
# plt.show()
plt.close()
print('Xlogng', Xlogng.shape)
sns.distplot(Xlogng[:, 0] , color="black", label="Feature 1")
# sns.distplot(Xlogng[:, 1] , color="gray", label="Feature 2")
plt.legend()
plt.savefig("%sXlogng.png" % result_path)
# plt.show()
plt.close()
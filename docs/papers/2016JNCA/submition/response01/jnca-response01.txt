the changes
 - replaced: probing for probe
 - replaced: DoS for flood
 - replaced: for "we consider network traffic datasets"
 - added: and in a well known public dataset
 - added: Additionally, anomaly detection techniques can be categorized in classification, statistical, Information theory and clustering based \cite{bhuyan2014network,ahmed2016survey,osanaiye2016distributed}.
 - replaced: In addition, we also evaluate the accuracy and performance of the proposed framework applied to a simulated scenario and to the DARPA 1998 Dataset \citep{osanaiye2016distributed}
 - replaced: This paper is organized as follows. In Section \ref{sec:relatedworks}, related works are discussed. Section \ref{sec:datamodel} presents the data model and the evaluated datasets. Section \ref{sec:prop_getv} describes the proposed framework for blind and automatic detection of flood and probe attacks. Section \ref{sec:experimentalresults} discusses the experimental validation and presents the results, and Section \ref{sec:Complexity} discuss the computational complexity of the proposed framework and evaluates the required processing time for simulated scenarios. Section \ref{sec:conclusionandfutureworks} draws the conclusions and the suggestions for future work. The \ref{sec:mos} presents mathematical concepts of examples of state-of-the-art MOS schemes.
 - added: Callegari \emph{et al} \cite{Zonglin2009} proposed a method for identifying the traffic flows responsible for an anomaly detected at the aggregate level and evaluated their proposal through a dataset with synthetic anomalies added in the data. However, Callegari \emph{et al} focused in flood attack detection, not addressing probe attack detection, and their approach relies on visual analysis. 
 - added: DoS attempts to block access to system or network resources, it is implemented by either forcing targets to be unavailable through the exploiting of system vulnerabilities, or consuming resources through large amount of network traffic, characterizing flood attacks. Probe attacks scan the networks to to collect information about host, such as IP addresses, ports and services.
 - added: Several approaches for network attack detections uses the KDD 99 \cite{ji2016multi,ahmed2016survey,osanaiye2016distributed,bhuyan2014network} datasets for accuracy and performance evaluation, due to their availability and labeled attacks. Even though the KDD 99 dataset are criticized by for the generation procedure and the risk of over-estimations of anomaly detection due to data redundancy, it still represents one of the few publicly available labeled datasets currently in use today by researchers \cite{osanaiye2016distributed,bhuyan2014network}. NSL-KDD \cite{tavallaee2009detailed} dataset is the refined version of the KDD 99 dataset that redundant data records are removed, in order to avoid biased classifications. Additionally, some approaches uses simulated \cite{callegari2011novel} scenarios or non-public datasets their evaluations. Since the proposed approach relies on a packet level analysis and the KDD 99 and NSL-KDD datasets adopt a traffic aggregation by connections, we consider the use of a simulated scenario on a real network and the DARPA 1998 dataset, which is the source for the creation of the KDD 99 and NSL-KDD datasets. It is important to note that the proposed approach is not based on learning or classification techniques, which are more susceptible to biased results caused by the issues in the DARPA/KDD datasets.
 - replaced "legitimate traffic (S)" por "legitimate traffic (U)"
 - added: and the selected cases of the DARPA dataset, 
 - added: Subsection \ref{sec:DarpaDataset} discuss the use of the DARPA dataset for evaluation of the proposed approach.
 - added: The DARPA 1998 dataset\footnote{https://www.ll.mit.edu/ideval/data/} includes 7 weeks of sniffed traffic saved into raw TCPDUMP packet data, from inside and outside origins, with labeled attacks. The attacks in this dataset can be grouped into: denial-of-service (DoS); remote to local (R2L), which is characterized by unauthorized access from a remote machine; user to root (U2R), which is characterized by unauthorized access to local super-user privileges; and probe attack. Since the proposed approach focus on flood and probe attack, we analyze the attacks that present behaviors similar to flood or probe attack. We observe that the most cases of DoS focus on exploit system vulnerabilities instead of on flooding attack. One example is the occurrence of a neptune attack which sends 20 only SYN packets, what is a behavior that differs of the expected flood attack behavior. Therefore, we select the cases that simulates several network traffic or numerous connection requests, also known as flooding attack \cite{ahmed2016survey,osanaiye2016distributed}, and the cases that scan ports sending just a few packets. From the simulated probe attacks, we select the cases that rely on TCP or UDP connections. The data modeling follows the method described by the Subsection \ref{sec:ModelingData}, with time frames of 20 minutes, packet counting aggregation by minute and considering the traffic to the following ports: 20, 21, 22, 23, 25, 79, 80, 88, 107, 109, 110, 113, 115, 143, 161, 389, 443.
 - added: \subsection{DARPA Scenario}
\label{sec:DarpaEvaluation}
This subsection presents a summarized view of results obtained from the application of the proposed framework, focusing on the largest eigenvalue analysis, model order selection and the eigenvalue analysis, for flood and probe attack detection in the DARPA 1998 dataset. Since the proposed framework is detailed explained in previous sections, here we focus on the parameter selection, dataset evaluation and results for flood and probe attack identification.
The DARPA dataset includes 7 weeks of sniffed traffic saved into raw packet data. The traffic and the labeled attacks are grouped by week and day, with information of the number and types of attacks per day, but also providing the start time for each labeled attack. For this evaluation, we performed an evaluation per day, considering the network traffic of 24 hours split into $Q$ time frames of 60 minutes ($N = 60$) and aggregated by minute and by port number. For each time frame $q$, a traffic matrix $\boldsymbol{X}^{(q)} \in \mathbb{R}^{17 \times 20}$ was obtained, considering the ports 20, 21, 22, 23, 25, 79, 80, 88, 107, 109, 110, 113, 115, 143, 161, 389 and 443.
Since the proposed framework focus on flood and probe attack detection, we evaluated only the attacks with behavior similar to flood or probe attack. Initially we selected all DoS and probe attacks, but we observed that the most cases of DoS focus on exploit system vulnerabilities instead of flooding attack, and most of probe attacks focus on ICMP intead of port scanning. Therefore, for evaluation of the proposed approach for flood and probe attack detection, it is necessary to select cases that implements flood or port scan behaviors. We select the following week-day-attack cases: 
\begin{enumerate}
	\item week3-thursday-neptune;
	\item week4-friday-portsweep;
	\item week5-thursday-neptune;
	\item week5-thursday-portsweep;
	\item week5-friday-portsweep;
	\item week6-wednesday-neptune;
	\item week6-thursday-neptune;
	\item week7-wednesday-portsweep.
\end{enumerate}
The analysis based on sample covariation of zero mean variables was evaluated for flooding behavior of netpune attacks, obtaining rates of 100\% for true positive (TP) detection and 60\% for false positive (FP) detection from 30 time frames. The results also show 50\% of misclassification rate \cite{bhuyan2014network}, which attempts to estimate the probability of disagreement between the true and predicted cases by dividing the sum of FN and FP by the total number of pairs observed. 
The result for FP and misclassification analysis is poor due to the legitimate traffic of DARPA dataset presents high number of packets per time  from one source to one target, with no variation on IP source or target port. This observation corroborates with previous evaluations of the DARPA dataset that highlight issues regarding traffic redundancy. 
The analysis based on covariance of zero mean and unitary standard deviation variables was evaluated for port scan attacks, including probe attacks and DoS attacks that send few packets for several ports in order to exploit some vulnerability. The results show rates of 76.9\% for TP detection and 23\% for FP detection from 94 time frames. The observed misclassification rate for this scenario was 32.7\%. We observed that all FN cases are probe attacks with a time delay between scanning one port and start scanning the next port, what can be called as sparse probe attacks. Cases with a delay of one minute or more were not detected by the proposed approach.
The performance of detection rate of flooding attacks is compared with the method proposed by Callegari \emph{et al} \cite{Zonglin2009}. This work is a statistical method, based on PCA, without training or learning methods, even though it relies on visual analysis for principal components selection. The best detection rate of \cite{Zonglin2009} was 82\% for detection of synthetically added flood attacks, while our proposal obtains 100\% of detection rate for detection of flood attacks of DARPA dataset.
Due to the lack of statistical techniques without training or learning methods for detection of probe attacks, we compare our approach with Lu and Ghorbani's \cite{Lu2009} proposal, which is a network anomaly detection model based on signal processing techniques that uses DARPA dataset for evaluation. The results of \cite{Lu2009} show the best detection rate of 94.67\% in terms of general attack instance detection, but shows a case case with 50\% of attack instance detection for the portsweep attack. The proposed approach presents 76.9\% of detection rate measured specifically for probe attacks, without the requirement of learning or training methods, in contrast to Lu and Ghorbani's \cite{Lu2009} work.
 - replaced: This section discusses the computational complexity and the performance evaluation of the proposed framework, focusing on the main steps, which are the eigenvalues decomposition (EVD), largest eigenvalues analysis, application of MOS scheme and eigen similarity analysis, according to Figure \ref{fig:fig80} and equations presented in Section \ref{sec:prop_getv}.
 - added: For better understanding the scalability and impact of configurations of $N$, $M$ and $Q$, we evaluated the processing time requited for different scenarios of parameter configurations and dataset, measuring the processing time of: 

\begin{enumerate}
	\item Eigen analysis based on sample covariance of zero mean;
	\item Eigen analysis based on sample covariance of zero mean and unitary standard deviation;
	\item EDC MOS scheme;
\end{enumerate}

The performance evaluation focus on the main steps, which were previously discussed regarding the complexity analysis. The data modeling is also a time consuming step, however its processing can be optimized trough distributed processing techniques, such as MapReduce, achieving high throughput for packet counting or even for deep packet inspection \cite{Vieira2013}.

The experiments were performed in a desktop computer with a Intel Core i7-4510U 2.00GHz and 16 GB of RAM, considering: variations on the network traffic time; the frame size denoted as $N$; the number of network ports denoted as $M$; the mean processing time for eigen analysis based on sample covariance of zero mean, denoted as 1-time; the mean processing time for eigen analysis based on sample covariance of zero mean and unitary standard deviation, denoted as 2-time; and the mean processing time for EDC MOS scheme, denoted as 3-time. The mean time calculations was obtained from 200 measurement repetitions, in order to obtain reliable values.

Table \ref{tab:11} presents the measured results. The experiment considered traffic time of 16, 20 and 22 hours, according to the selected traffic time per day available by the DARPA dataset. It is possible to observe the processing time increases according to the increment in traffic time, around 2 or 3 times for 1-time and 2-time, but the worst measured processing time is 4.7250 milliseconds.

\begin{table*}[!t]
	\caption{Processing time of the main steps for anomaly detection}
	\label{tab:11}
	\centering
	\begin{tabular}{|r|r|r|r|r|r|r|}
		\hline \rowcolor{Gray} \begin{tabular}[x]{@{}l@{}}Traffic Time\\(hour)\end{tabular}	& \begin{tabular}[x]{@{}l@{}}Frame Size\\(min)\end{tabular}	 & \begin{tabular}[x]{@{}l@{}}Num. Ports\end{tabular}	& \begin{tabular}[x]{@{}l@{}}1-time\\(ms)\end{tabular}	& \begin{tabular}[x]{@{}l@{}}2-time\\(ms)\end{tabular}	& \begin{tabular}[x]{@{}l@{}}3-time\\(ms)\end{tabular}	\\ \hline
16	& 10	& 17	& 0.7900	& 0.8100	& 0.0650\\ \hline
16	& 20	& 17	& 0.5250	& 0.5950	& 0.0100\\ \hline
16	& 60	& 17	& 0.9700	& 1.1400	& 0.0250\\ \hline
16	& 120	& 17	& 0.6050	& 0.6100	& 0.0050\\ \hline
16	& 60	& 34	& 1.2750	& 1.2200	& 0.0050\\ \hline
16	& 120	& 34	& 1.1200	& 1.1700	& 0.0050\\ \hline
20	& 10	& 17	& 2.7950	& 2.8950	& 1.1000\\ \hline
20	& 20	& 17	& 2.0700	& 2.0200	& 0.3500\\ \hline
20	& 60	& 17	& 1.0250	& 1.0450	& 0.0650\\ \hline
20	& 120	& 17	& 1.0000	& 1.0700	& 0.0350\\ \hline
20	& 60	& 34	& 2.9650	& 3.2100	& 0.0400\\ \hline
20	& 120	& 34	& 2.9950	& 3.1150	& 0.0200\\ \hline
22	& 10	& 17	& 4.7250	& 4.0850	& 1.4600\\ \hline
22	& 20	& 17	& 2.3200	& 2.6800	& 0.2450\\ \hline
22	& 60	& 17	& 1.0700	& 1.1200	& 0.0300\\ \hline
22	& 120	& 17	& 0.9900	& 1.0500	& 0.0250\\ \hline
22	& 60	& 34	& 3.0850	& 3.1250	& 0.0650\\ \hline
22	& 120	& 34	& 2.8100	& 2.9600	& 0.0250\\ \hline
	\end{tabular}
\end{table*}

It is possible to observe that the processing time increases with the frame size $N$ decreasing, therefore it is possible to evaluate the frame size that produces better identification rates and acceptable processing time. The number of ports evaluated during the proposed scheme is also an important variable regarding processing time optimizations, since the significant increase of processing time observed between scenarios considering 17 or 34 ports, with growth between 7\% and 199\%.

 - added: , as well as presenting acceptable complexity and performance regarding the processing time

 - replaced: Future research is directed to improvements for obtaining better false positive rates, as well as for make the proposed framework able to identify sparse probe attacks. Additionally, future research can evaluate the application of the proposed approach to different domains, considering cases that are aware to behavioral analysis.















KDD-99
 - possui tráfego malicioso e legítimo
 - dos and probing
 - probing = connection records were sorted by destination host, and features were constructed using a window of 100 connections to the same host instead of a time window -> host-based traffic features.
 - o dado é agrupado em "connections", que seria semelhante a um "flow". Entretanto, nosso experimento faz uma análise de pacotes, desconsiderando a formação de flows, que é uma tarefa complicada
 - o dado não tem timestamp, enquanto nossa abordagem depende de timestamp dos pacotes, assim como outras técnicas de detecção de anomalias baseado em comportamento
 - o dado não tem ip (origem ou destino), mas nossa abordagem é agrupada por IP de destino
 - a porta de origem e destino não são fornecidas, mas as portas de destino podem ser estimadas de acordo com o serviço declarado (http -> 80, por exemplo)

 - kdd e ksl-kdd, que são os mais utilizados, trabalham com dados agregados em connections, ou fluxos, o que não é adequado para nossa análise, que é em packet level. desta forma, utilizamos o dataset que é utilizado como fonte para o kdd e ksl-kdd, que é o darpa 98.
 - falar que o nosso método é aplicado para flooding attacks and probe, mas o darpa dataset aplica DOS com pouca quantidade de pacotes, como casos em que um synflood é feito com 15 ou 20 pacotes
 - ademais, o tráfego legítimo tem comportamentos semelhantes a casos de ataques de flooding, que são several packets de uma mesma origem para um mesmo destino
 - outros ataques synflood fazem um envio de pacotes em todas as portas de uma determinada máquina, incrementando unitariamente o valor das portas. este comportamento difere do comportamento que avaliamos incialmente para synflood, mas o envio de pacotes para várias portas é semelhante a um portscan, sendo portanto efetivamente detectado pela nossa técnica de portscan.
 - algumas reclamações do kdd é a repetição de tráfego que pode influenciar algoritmos baseados em aprendizado, mas a nossa abordagem não sofre deste tipo de influência. entretanto, a repetição de tráfego em larga escala, com mesmas portas, ip de origem e destino, pode reproduzir comportamentos de flooding attacks que poderiam ser detectados por nossa técnica, o que torna nossa técnica sucetível a falso positivos
 - casos de probe attacks com baixa concentração de tempo, com intervalo entre portas superiores a 20 segundos, não são detectados por nossa técnica baseada em correlação, sendo um ponto a ser melhorado

citar:
	A survey of network anomaly detection techniques - jnca - 2016
	Denial of Service Attack Detection using Multivariate Correlation Analysis - 2016
	A multi-level intrusion detection method for abnormal network behaviors - jnca - 2016
	Distributed denial of service (DDoS) resilience in cloud: Review and conceptual cloud DDoS mitigation framework - jnca - 2016
	An empirical evaluation of information metrics for low-rate and high-rate DDoS attack detection - 2015
	Network Anomaly Detection: Methods, Systems and Tools - 2014
	Discriminating DDoS Attacks from Flash Crowds Using Flow Correlation Coefficient - transaction - 2012
	Monitoring the Application-Layer DDoS Attacks for Popular Websites - transaction - 2009

comparar:
	A novel PCA-based Network Anomaly Detection - icc - 2011

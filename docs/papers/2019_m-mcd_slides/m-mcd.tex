%	Name			:: 	sthlm Beamer Theme  HEAVILY based on the hsrmbeamer theme (Benjamin Weiss)
%	Author			:: 	Mark Hendry Olson (mark@hendryolson.com)
%	Created			::	2013-07-31
%	Updated	    	::	[[April]] 04, 2017 at 16:26:39
%	Version			:: 	2.0.2
%	Email			:: 	hendryolson@gmail.com
%	Website			:: 	http://markolson.se
%	Twitter			:: 	markolsonse
%	Instagram		:: 	markolsonse
%
%	License			:: 	This file may be distributed and/or modified under the
%					GNU Public License.
%
%	Description		::	This presentation is a demonstration of the sthlm beamer
%					theme, which is HEAVILY based on the HSRM beamer theme created by Benjamin Weiss
%					(benjamin.weiss@student.hs-rm.de), which can be found on GitHub
%					<https://github.com/hsrmbeamertheme/hsrmbeamertheme>.  It also borrows heavily
%					from the work of Matthias Vogelgesang, (https://bloerg.net) and his Metropolis Mtheme,
%					<https://github.com/matze/mtheme>.
%
%	Theme			::	newPxFont
%	Options			::	progressbar
%					::	sectionpages
%					::	numfooter
%					::	fullfooter
%					::	dovaligncolumns
%					::	protectframetitle
%					::	greybg
%					::	cblock
%					::	minimal
%-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
%
%        LOADING DOCUMENT
%
%-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=

\documentclass[newPxFont, numfooter, sectionpages]{beamer}
\usepackage[utf8]{inputenc}
\usetheme{sthlm}
\usepackage{pgfplots}
\pgfplotsset{compat=1.9}
\usepackage{cancel}
\usepackage{amsthm,amsmath}
\usepackage{mathtools}
\usepackage{booktabs}
\usepackage{supertabular}
\usepackage{tabularx}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{hhline}
\usepackage{color, colortbl}
\usepackage[backend=bibtex, style=numeric, defernumbers=true]{biblatex}
\usepackage[linesnumbered, ruled, vlined]{algorithm2e}
\addbibresource{references.bib}

\DeclarePairedDelimiter\abs{\lvert}{\rvert}%
\DeclarePairedDelimiter\norm{\lVert}{\rVert}%

\definecolor{lightgray}{gray}{0.8}
\definecolor{Gray}{gray}{0.9}

\title{Network Anomaly Detection Based on Robust Moments}
\subtitle{\small{Ph.D. Thesis Presentation}}
\author{\texttt{Thiago P. de B. Vieira \newline \textbf{Advisor:} João Paulo C. L. da Costa}}
\institute{
	\scriptsize{Universidade de Brasília}\\
	\scriptsize{Departamento de Engenharia Elétrica - ENE/FT}\\
	\scriptsize{Programa de Pós-Graduação em Engenharia Elétrica - PPGEE}
}


%document-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
\begin{document}

%title-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
\maketitle

%-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
%	FRAME: Theme Package Requirements
%-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=
\begingroup
\setbeamercolor{normal text}{fg=\cnDarkGrey,bg=white}


%outline-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
\section*{Outline}

\begin{frame}{Outline}
	\scriptsize
	\tableofcontents[hideallsubsections]% For longer presentations use hideallsubsections option
\end{frame}


%section-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
\section{Motivation}

\begin{frame}[c]{Anomaly Detection}	
	\begin{itemize}
		\item Anomaly detection is the identification of rare and suspicions observations by differing the normal or majority of the data.
		\item Anomalies can be related to network attack, fraud, defect or medical problems, for examples.
		\item Anomalies are also referred to as outliers, novelties, noise, deviations and exceptions.
		\item Anomalies can be hard to identify and separate from normal data.
		\item Considering that anomalies are rare in comparison to normal events, anomaly detection algorithms have to deal with problems imposed by imbalanced data.
	\end{itemize}
\end{frame}

\begin{frame}[c]{Anomaly Detection}
	\begin{figure}[h!]
	     \centering
	     \includegraphics[width=8cm]{figures/raw_dur_dport_pairplot.png}
	     \caption{Nomalies from original data.}
	     \label{fig:fig01}
	\end{figure}
\end{frame}

\begin{frame}[c]{Anomaly Detection}	
	\begin{itemize}
		\item Some techniques can make the data discriminative for better anomaly detection:
		\begin{enumerate}
			\item Data engineering (i.e. sampling \cite{??}, aggregation \cite{acarali2016survey}).
			\item Feature extraction (i.e. decomposition \cite{??}, robust estimates \cite{??}).
		\end{enumerate}
	\end{itemize}
\end{frame}

\begin{frame}[c]{Anomaly Detection}
	\begin{figure}[h!]
	     \centering
	     \includegraphics[width=8cm]{figures/sum_avg_duration_conn_pairplot.png}
	     \caption{Anomalies from summarized data.}
	     \label{fig:fig02}
	\end{figure}
\end{frame}

\begin{frame}[c]{Anomaly Detection based on Robust Estimates}	
	\begin{itemize}
		\item Robust statistics produce methods that are not unduly affected by outliers.
		\item Robust statistical methods have been developed for estimating location, scale, and regression parameters.
		\item Robust estimates from contaminated data can denoise, remove anomalies or even separate them.
		\item Anomaly detection based on robust estimates has been adopted for network anomaly detection, due to good results and its adaptability.
	\end{itemize}
\end{frame}

\begin{frame}[c]{Distance Based Anomaly Detection}	
	\begin{itemize}
		\item The distance between robust estimates and new observations can reveal anomalies.
		\item Mahalanobis Distance (MD) is a generalized distance which is useful for determining the similarity between an unknown sample and a collection of known samples by, considering the correlations between the variables and their mean values.
		\item \textbf{MD has been used for distance based anomaly detection with robust estimates} in many areas, and is commonly combined to Fast Minimum Covariance Determinant (Fast-MCD) algorithm \cite{rousseeuw1999fast} for robust estimates.
	\end{itemize}
\end{frame}

\begin{frame}[c]{Distance Based Anomaly Detection}
	\begin{figure}[h!]
	     \centering
	     \includegraphics[width=7.6cm]{figures/17_1s_distances.png}
	     \caption{Anomaly detection from robust estimates.}
	     \label{fig:fig03}
	\end{figure}
\end{frame}

\begin{frame}{Network Anomaly Detection Based on Robust Moments}
	\begin{itemize}
	    \item Network anomaly detection problems are usually characterized by skewed and imbalanced data \cite{Phua2004minority}.
	    \item Learning algorithms for imbalanced data has been a relevant research topic. 
	    \item The fundamental issue with the imbalanced learning problem is the ability of imbalanced data to significantly compromise the performance of most standard learning algorithms \cite{he2008learning}.
	    \item Skewness and the kurtosis tests are very sensitive outlier detectors. The properties of the kurtosis can be used to identify clusters or outliers \cite{pena2010eigenvectors}, while skewness has been used as adjust factor for outlier detection \cite{hubert2008outlier}.
    \end{itemize}
\end{frame}

\begin{frame}{Network Anomaly Detection Based on Robust Moments}
	\begin{itemize}
	    \item We propose a network anomaly detection approach based on robust moment estimates and distance analysis.
    	\begin{itemize}
    	    \item Considering that Fast-MCD is a widely adopted robust estimator of location and scatter for anomaly detection, we propose to extend Fast-MCD for computing robust skewness and kurtosis.
		    \item We also propose to use Mahalanobis Distance for detecting anomalies based on robust skewness and kurtosis, instead of the location-based usage of classical approaches.
		    \item Considering that it is hard to have labeled data in real problems \cite{osanaiye2016distributed}, such as for network attacks or frauds, also considering that known anomalies are constantly changing due to adversarial models or due to lack of regular patterns, we propose a Semi-Supervised approach based on robust moments for network anomaly detection.
	    \end{itemize} 
    \end{itemize}
\end{frame}


%section-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
\section{Literature Review}

\begin{frame}[c]{Literature Review}
	\cite{gu2007bothunter} (Bothunter: Detecting malware infection through ids-driven dialog correlation)
	\begin{enumerate}
		\item ...
	\end{enumerate}	
\end{frame}

\begin{frame}[c]{Literature Review}
	\cite{gu2008botminer} (Botminer: Clustering analysis of network traffic for protocol-and structure-independent botnet detection)
	\begin{enumerate}
		\item ...
	\end{enumerate}	
\end{frame}

\begin{frame}[c]{Literature Review}
	\cite{hubert2008outlier} (Outlier detection for skewed data)
	\begin{enumerate}
		\item ...
	\end{enumerate}	
\end{frame}

\begin{frame}[c]{Literature Review}
	\cite{garcia2014empirical} (An empirical comparison of botnet detection methods)
	\begin{enumerate}
		\item ...
	\end{enumerate}	
\end{frame}

\begin{frame}[c]{Literature Review}
	\cite{khattak2015botflex} (BotFlex: A community-driven tool for botnet detection)
	\begin{enumerate}
		\item ...
	\end{enumerate}	
\end{frame}

\begin{frame}[c]{Literature Review}
	\cite{wang2017botnet} (Botnet detection based on anomaly and community detection)
	\begin{enumerate}
		\item ...
	\end{enumerate}	
\end{frame}

\begin{frame}[c]{Literature Review}
	\cite{da2018online} (Online detection of Botnets on Network Flows using Stream Mining)
	\begin{enumerate}
		\item ...
	\end{enumerate}
\end{frame}

\begin{frame}[c]{Literature Review}
	\cite{moustafa2019holistic} (A holistic review of Network Anomaly Detection Systems: A comprehensive survey)
	\begin{enumerate}
		\item ...
		\item Network anomaly detection approaches are classified by \cite{moustafa2019holistic} in six categories: classification, clustering, deep learning, knowledge, combination and statistical-based.
	\end{enumerate}	
\end{frame}

\begin{frame}[c]{Literature Review}
	\cite{Phua2004minority} (Minority Report in Fraud Detection: Classification of Skewed Data)
	\begin{enumerate}
		\item ...
		\item \cite{acarali2016survey} (Survey of approaches and features for the identification of HTTP-based botnet traffic)
		\item \cite{lakhina2005mining} (Mining anomalies using traffic feature distributions)
		\item \cite{goldberg2015importance} (The importance of features for statistical anomaly detection)
	\end{enumerate}	
\end{frame}

\begin{frame}[c]{Literature Review}
	\cite{acarali2016survey} (Survey of approaches and features for the identification of HTTP-based botnet traffic)
	\begin{enumerate}
		\item ...
		\item \cite{lakhina2005mining} (Mining anomalies using traffic feature distributions)
		\item \cite{goldberg2015importance} (The importance of features for statistical anomaly detection)
	\end{enumerate}	
\end{frame}

\begin{frame}[c]{Literature Review}
	\cite{lakhina2005mining} (Mining anomalies using traffic feature distributions)
	\begin{enumerate}
		\item ...
		\item \cite{goldberg2015importance} (The importance of features for statistical anomaly detection)
	\end{enumerate}	
\end{frame}

\begin{frame}[c]{Literature Review}
	\cite{goldberg2015importance} (The importance of features for statistical anomaly detection)
	\begin{enumerate}
		\item ...
	\end{enumerate}	
\end{frame}

\begin{frame}[c]{Clustering Method for anomaly detection}
	\begin{enumerate}
		\item Clustering methods for anomaly detection rely on three key assumptions: legitimate data instances often fall into a cluster whereas attacks do not, legitimate data instances are usually located near the closest cluster centroid while anomaly ones are often far away from it, legitimate data instances fall into vast and dense clusters and anomalies into small or spare ones \cite{moustafa2019holistic}.
	\end{enumerate}	
\end{frame}

\begin{frame}[c]{Gaussian Mixture Model}
	\begin{enumerate}
		\item GMM is a statistical-based and parametric approach algorithm that estimates the distribution of the normal class from a training set and is typically based on a set of kernels \cite{moustafa2019holistic}.
	\end{enumerate}	
\end{frame}

\begin{frame}[c]{Mahalanobis Distance}
	\begin{enumerate}
		\item The Mahalanobis Distance is a measure of the distance between a vector $\boldsymbol{x}$ and a distribution $\boldsymbol{X}$, introduced by P. C. Mahalanobis in 1936 \cite{mahalanobis1936generalized}.
		\item It is a multi-dimensional generalization for measuring how many standard deviations away $\boldsymbol{x}$ is from the mean $\boldsymbol{\bar{x}}$ and covariance $\boldsymbol{\hat{S}}$ of $\boldsymbol{X}$
	\end{enumerate}	
\end{frame}

\begin{frame}[c]{Mahalanobis Distance}
	\begin{enumerate}
		\item Mahalanobis Distance is defined as		
		\begin{equation}\label{eq:eq01}
			d(\boldsymbol{x},\bar{\boldsymbol{x}}, \boldsymbol{\hat{S}}) = \sqrt{(\boldsymbol{x} - \bar{\boldsymbol{x}}) \boldsymbol{\hat{S}}^{-1}(\boldsymbol{x} - \bar{\boldsymbol{x}})^\prime}.
		\end{equation}
		\item $\boldsymbol{x}$ is a vector of a new observation, $\bar{\boldsymbol{x}}$ is the mean vector, also referred as location, of known observations and $\boldsymbol{\hat{S}}$ is the covariance matrix, also referred as scatter, of known observations.
	\end{enumerate}	
\end{frame}

\begin{frame}[c]{Mahalanobis Distance}
	\begin{enumerate}
		\item Classical estimates can be so strongly affected by contamination that diagnostic tools, such as the Mahalanobis Distances, become unable to detect the outliers.
		\item Therefore, we need reliable and robust estimators that can resist outliers contaminated data.
		\item Minimum Covariance Determinant (MCD) \cite{rousseeuw1984least} is a robust estimator commonly used for computing Mahalanobis Distance in order to detect anomalies.
	\end{enumerate}	
\end{frame}

\begin{frame}[c]{Mahalanobis Distance}
	\begin{figure}[h!]
	     \centering
	     \includegraphics[width=11cm]{figures/mahalanobis_robust.png}
	     \caption{Mahalanobis Distance for non-robust and robust estimates.}
	     \label{fig:fig04}
	\end{figure}
\end{frame}

\begin{frame}[c]{Mahalanobis Distance}
	\begin{figure}[h!]
	     \centering
	     \includegraphics[width=8cm]{figures/mahalanobis_distances01.png}
	     \caption{Mahalanobis Distance for non-robust and robust estimates.}
	     \label{fig:fig05}
	\end{figure}
\end{frame}

\begin{frame}[c]{MCD - Minimum Covariance Determinant}
	\begin{enumerate}
		\item The MCD estimator is one of the first affine equivariant and highly robust estimators of multivariate location and scatter.
		\begin{enumerate}
			\item Affine equivariant means that when the data are translated or subjected to a linear transformation, the resulting location and scatter will transform accordingly \cite{rousseeuw1984least,rousseeuw1999fast}.
		\end{enumerate}
		\item Its objective is to find $h$ observations (out of $n$) whose covariance matrix has the lowest determinant.
		\begin{enumerate}
			\item The lowest determinant means the lowest distance generalized variance and high distance between the largest eigenvalue and the remaining [ref?].
		\end{enumerate}
		\item MCD depends on random sampling, therefore it is not deterministic.
	\end{enumerate}
\end{frame}

\begin{frame}[c]{MCD - Minimum Covariance Determinant}
	\begin{enumerate}
		\item MCD has been applied in numerous fields (medicine, finance, image analysis, and chemistry) and been used to develop many robust multivariate techniques (such as Robust Principal Component Analysis (RPCA), factor analysis, and multiple regression).
		\item The classical MCD has rarely been applied because it is hard to compute, since it requires the evaluation of all $\binom{n}{h}$ subsets of size $h$. % binomial of n over p
		\item Its main use has started since the construction of the computationally efficient Fast-MCD algorithm \cite{rousseeuw1999fast}.
	\end{enumerate}
\end{frame}

\begin{frame}[c]{Fast-MCD}
	\begin{enumerate}
		\item Fast-MCD is an iterative, approximate and resampling algorithm for the MCD.
		\item It starts by random initial subset of size $p+1$ and performs concentration steps (C-steps) yielding consecutive h-subsets with decreasing covariance matrix determinant.
		\item Only two C-steps are applied to each initial subset, and the 10 results with lowest determinant are kept.
		\item C-steps are carried out until convergence and the best solution is kept.
		\item Fast-MCD also is affine equivariant and not deterministic.
	\end{enumerate}
\end{frame}

\begin{frame}[c]{Fast-MCD}

	\textbf{Theorem 1: C-Step.} 

	Consider a data set $\boldsymbol{X} \in \mathbb{R}^{n \times p}$ where $\boldsymbol{X} = \{\boldsymbol{x}_1,...,\boldsymbol{x}_n\}$ of $p$-variate observations. Let $\boldsymbol{h} \subset \{1,...,n\}$ with $|\boldsymbol{h}| = h$, where $\boldsymbol{h}$ is a subset of $h$ indexes from $n$ observations.

	Let the mean $\bar{\boldsymbol{x}} \in \mathbb{R}^{1 \times p}$ be

	\begin{equation}\label{eq:eq02}
		\bar{\boldsymbol{x}} = \displaystyle\frac{1}{h}\displaystyle\sum_{j\in \boldsymbol{h}} \boldsymbol{x}_j, 
	\end{equation}

	the covariance matrix $\boldsymbol{\hat{S}} \in \mathbb{R}^{p \times p}$ be

	\begin{equation}\label{eq:eq03}
		\boldsymbol{\hat{S}} = \displaystyle\frac{1}{h}\displaystyle\sum_{j\in \boldsymbol{h}} (\boldsymbol{x}_j - \bar{\boldsymbol{x}})(\boldsymbol{x}_j - \bar{\boldsymbol{x}})^\prime,
	\end{equation}
\end{frame}

\begin{frame}[c]{Fast-MCD}
	and the relative distance be 	

	\begin{equation}\label{eq:eq04}
		\boldsymbol{d}(i) = \sqrt{(\boldsymbol{x}_i - \bar{\boldsymbol{x}}) \boldsymbol{\hat{S}}^{-1}(\boldsymbol{x}_i - \bar{\boldsymbol{x}})^\prime}, 
	\end{equation}

	for $i = 1,...,n$.

	Let $\boldsymbol{h}_1, \bar{\boldsymbol{x}}_1$ and $\boldsymbol{\hat{S}}_1$ and be the selected observations, the location and scatter of the initial subset, and $\boldsymbol{d}_1(i)$ be the initial distances for $i = 1,...,n$. 

	Now take $\boldsymbol{h}_2$ containing the indexes of lowest distances, such that $\{\boldsymbol{d}_1(i); i \in \boldsymbol{h}_2\} = \{(\boldsymbol{d}_1)_{1:n},...,(\boldsymbol{d}_1)_{h:n}\}$, where $(\boldsymbol{d}_1)_{1:n} \leq (\boldsymbol{d}_1)_{2:n} \leq ... \leq (\boldsymbol{d}_1)_{n:n}$ are the ordered distances, and compute $\boldsymbol{\bar{x}}_2$ and $\boldsymbol{\hat{S}}_2$ for $\boldsymbol{h}_2$.
	
\end{frame}

\begin{frame}[c]{Fast-MCD}
	
	Then, $det(\boldsymbol{\hat{S}}_2) \leq det(\boldsymbol{\hat{S}}_1)$ with equality if and only if $\boldsymbol{\bar{x}}_2 = \boldsymbol{\bar{x}}_1$ and $\boldsymbol{\bar{S}}_2 = \boldsymbol{\hat{S}}_1$

	\begin{enumerate}
		\item The proof is given by Rousseeuw and Driessen \cite{rousseeuw1999fast}.
		\item \label{item:cstep}It is referred to the construction in Theorem 1 as a C-step, where C can be taken to stand for "covariance" or for "concentration".
	\end{enumerate}

\end{frame}

\begin{frame}[c]{Fast-MCD}
	\textbf{The C-step can be algorithmically described as follows:}
	\begin{algorithm}[H]\label{alg:alg01}
		\scriptsize
		\SetAlgoLined
		\KwResult{$\boldsymbol{\bar{x}}_2$, $\boldsymbol{\hat{S}}_2$}
		Given the initial $h-$subset $\boldsymbol{h}_1$ of indexes or the pair $(\boldsymbol{\bar{x}}_1, \boldsymbol{\hat{S}}_1)$\;
		\While{$det(\boldsymbol{\hat{S}}_2) < det(\boldsymbol{\hat{S}}_1)$}{
			\If{$\boldsymbol{\bar{x}}_2$ and $\boldsymbol{\hat{S}}_2$ exist}{
				Put $\boldsymbol{\bar{x}}_1 = \boldsymbol{\bar{x}}_2$ and $\boldsymbol{\hat{S}}_1 = \boldsymbol{\hat{S}}_2$\;
			}
			Compute the distances $\boldsymbol{d}_1( i )$ for $i = 1,..., n$\;
			Sort $\boldsymbol{d}_1( i )$, which yields a index $\boldsymbol{d}_1(\pi(1)) \leq ... \leq \boldsymbol{d}_1(\pi(n))$\;
			Put $\boldsymbol{h}_2 = \{ \pi(1), \pi(2), ..., \pi(h)\}$\;
			Compute $\boldsymbol{\bar{x}}_2$ and $\boldsymbol{\hat{S}}_2$ for $\boldsymbol{h}_2$\;
			Compute $det(\boldsymbol{\hat{S}}_2)$ and $det(\boldsymbol{\hat{S}}_1)$\;			
		}
		\caption{C-Step}
	\end{algorithm}
\end{frame}

\begin{frame}[c]{Fast-MCD}
	\textbf{Theorem 1} thus provides a partial idea for an algorithm: 
		\begin{itemize}
			\item \textit{Take many initial choices of $\boldsymbol{h}_1$ and apply C-steps to each until convergence (don't have determinant decreasing), and keep the result with lowest determinant}.
		\end{itemize}
\end{frame}

\begin{frame}[c]{Fast-MCD}
	\textbf{Initial subset can be algorithmically described as:}
	\begin{algorithm}[H]\label{alg:alg02}
		\scriptsize
		\SetAlgoLined
		\KwResult{$\boldsymbol{h}_1$}
		Given a data set $\boldsymbol{X} \in \mathbb{R}^{n \times p}$ with $p$-variate observations\;
		Draw a random $(p + 1)$-subset $\boldsymbol{L}$\;
		Compute $\boldsymbol{\bar{x}}_1$ and $\boldsymbol{\hat{S}}_1$ of $\boldsymbol{L}$ and $det(\boldsymbol{\hat{S}}_1)$\;
		\If{$det(\boldsymbol{\hat{S}}_1) \leq 0$}{
			Add another random observation into $\boldsymbol{L}$\;
			Compute $\boldsymbol{\bar{x}}_1$ and $\boldsymbol{\hat{S}}_1$ of $\boldsymbol{L}$ and $det(\boldsymbol{\hat{S}}_1)$\;
		}
		Compute $\boldsymbol{d}_1^2(i) = (\boldsymbol{x}_i - \boldsymbol{\bar{x}}_1)^\prime \boldsymbol{\hat{S}}_1^{-1}(\boldsymbol{X}_i - \boldsymbol{\bar{x}}_0)$ for $i = 1, ..., n$\;
		Sort them into $\boldsymbol{d}_1(\pi(1)) \leq ... \leq \boldsymbol{d}_0(\pi(n))$\;
		Put $\boldsymbol{h}_1 = \{\pi(1), ..., \pi(h)\}$\;
		\caption{Constructing the initial subset}
	\end{algorithm}
\end{frame}

\begin{frame}[c]{Fast-MCD}
	\begin{algorithm}[H]\label{alg:alg03}
		\scriptsize
		\SetAlgoLined
		\KwResult{$\boldsymbol{\bar{x}}_2$, $\boldsymbol{\hat{S}}_2$}
		Given $\boldsymbol{X} \in \mathbb{R}^{n \times p}$, $h < n$, $p \geq 2$ and $n \leq 600$\;
		\For{500 times}{
			Construct initial $h$-subset $\boldsymbol{h}_1$ (Algorithm \ref{alg:alg02})\;
			Carry out two C-steps (Algorithm \ref{alg:alg01}) resulting $\boldsymbol{\bar{x}}_2$ and $\boldsymbol{\hat{S}}_2$\;
		}		
		Put $\boldsymbol{h}_1$ as the 10 lowest $det(\boldsymbol{\hat{S}_2})$ computed above\;
		\While{$det(\boldsymbol{\hat{S}}_2) < det(\boldsymbol{\hat{S}}_1)$}{
			Carry out C-steps (Algorithm \ref{alg:alg01})\;
		}
		\caption{Fast-MCD when $n \leq 600$}
	\end{algorithm}
\end{frame}

\begin{frame}[c]{Fast-MCD}
	\begin{algorithm}[H]\label{alg:alg04}
		\scriptsize
		\SetAlgoLined
		\KwResult{$\boldsymbol{\bar{x}}_2$, $\boldsymbol{\hat{S}}_2$}
		Given $\boldsymbol{X} \in \mathbb{R}^{n \times p}$, $h < n$, $p \geq 2$ and $n > 600$\;
		Construct up to 5 disjoint random subsets of size $n_{sub} = 300$\;
		\For{each subset, repeat 100 times}{
			Construct initial $\boldsymbol{h}_1$ of size $h_{sub} = [n_{sub}(h/n)]$\;
			Carry out two C-steps of $n_{sub}$ and $h_{sub}$\;
			Keep the 10 best $(\boldsymbol{\bar{x}}_{sub}, \boldsymbol{\hat{S}}_{sub})$\;
		}		
		Pool the subsets, yielding the merged set of $n_{merged} = 1500$\;
		\For{50 best $(\boldsymbol{\bar{x}}_{sub}, \boldsymbol{\hat{S}}_{sub})$}{
			Carry out two C-steps of $n_{merged}$ and $h_{merged} = [n_{merged}(h/n)]$\;
			Keep the 10 best $(\boldsymbol{\bar{x}}_{merged}, \boldsymbol{\hat{S}}_{merged})$\;
		}
		\For{the full data set and $m_{full}$ best results}{
			Take several C-steps, using $n$ and $h$\;
			Keep the best final result $(\boldsymbol{\bar{x}}_{full}, \boldsymbol{\hat{S}}_{full})$\;
		}
		\caption{Fast-MCD when $n > 600$}
	\end{algorithm}
\end{frame}


%section-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
\section{Network Anomaly Detection Based on Robust Moments}

\begin{frame}{Network Anomaly Detection Based on Robust Moments}
	We propose to extend Fast-MCD for computing robust moments (skewness and kurtosis), in order to improve the anomaly detection in a skewed and imbalanced data set of network traffic with normal and botnet traffic.
		    
	Moments are a set of statistical parameters to measure a distribution. The arithmetic mean is the first general moment, the second is the variance, while skewness (asymmetry) is the third moment and kurtosis (excess) is the fourth moment.
	
	The general formula of the $r$-th moment can be expressed as:
	\begin{equation}\label{eq:eq05}
		m_r = \displaystyle\frac{1}{n}\displaystyle\sum_{i = 1}^{n}( x_i - \bar{x})^r. 
	\end{equation}
\end{frame}

\begin{frame}{Network Anomaly Detection Based on Robust Moments}
	Therefore, the Skewness is calculated as
	\begin{equation}\label{eq:eq06}
		\boldsymbol{\bar{s}} = \frac{m_3}{m_2^{\frac{3}{2}}},
	\end{equation}

	and Kurtosis as
	\begin{equation}\label{eq:eq07}
		\boldsymbol{\bar{k}} = \frac{m_4}{m_2^2} 
	\end{equation}
\end{frame}

\begin{frame}{Network Anomaly Detection Based on Robust Moments}
	For extending Fast-MCD, We initially modify the C-step Algorithm \ref{alg:alg01} to calculate Skewness and Kurtosis after to minimize $det(\boldsymbol{\hat{S}})$, accordingly to Algorithm \ref{alg:alg05}.
	
	\begin{algorithm}[H]\label{alg:alg05}
		\scriptsize
		\SetAlgoLined
		\KwResult{$\boldsymbol{\bar{x}}_2$, $\boldsymbol{\hat{S}}_2$, $\boldsymbol{\bar{s}}_2$ and $\boldsymbol{\bar{k}}_2$}
		Given the initial $h-$subset $\boldsymbol{h}_1$ of indexes or the pair $(\boldsymbol{\bar{x}}_1, \boldsymbol{\hat{S}}_1)$\;
		\While{$det(\boldsymbol{\hat{S}}_2) < det(\boldsymbol{\hat{S}}_1)$}{
			\If{$\boldsymbol{\bar{x}}_2$ and $\boldsymbol{\hat{S}}_2$ exist}{
				Put $\boldsymbol{\bar{x}}_1 = \boldsymbol{\bar{x}}_2$ and $\boldsymbol{\hat{S}}_1 = \boldsymbol{\hat{S}}_2$\;
			}
			Compute the distances $\boldsymbol{d}_1( i )$ for $i = 1,..., n$\;
			Sort $\boldsymbol{d}_1( i )$, which yields a index $\boldsymbol{d}_1(\pi(1)) \leq ... \leq \boldsymbol{d}_1(\pi(n))$\;
			Put $\boldsymbol{h}_2 = \{ \pi(1), \pi(2), ..., \pi(h)\}$\;
			Compute $\boldsymbol{\bar{x}}_2$ and $\boldsymbol{\hat{S}}_2$ for $\boldsymbol{h}_2$\;
			Compute $det(\boldsymbol{\hat{S}}_2)$ and $det(\boldsymbol{\hat{S}}_1)$\;			
		}
		Compute $\boldsymbol{\bar{s}}_2$ and $\boldsymbol{\bar{k}}_2$ for last $\boldsymbol{h}_2$\;
		\caption{C-Step with higher moments}
	\end{algorithm}
\end{frame}

\begin{frame}[c]{Network Anomaly Detection Based on Robust Moments}
    The Algorithm \ref{alg:alg06} describes the Fast-MCD with higher moments when $n \leq 600$, through the modification of the original Fast-MCD for computing Skewness and Kurtosis from the robust estimate.
    
	\begin{algorithm}[H]\label{alg:alg06}
		\scriptsize
		\SetAlgoLined
		\KwResult{$\boldsymbol{\bar{x}}_2$, $\boldsymbol{\hat{S}}_2$, $\boldsymbol{\bar{s}}_2$ and $\boldsymbol{\bar{k}}_2$}
		Given $\boldsymbol{X} \in \mathbb{R}^{n \times p}$, $h < n$, $p \geq 2$ and $n \leq 600$\;
		\For{500 times}{
			Construct initial $h$-subset $\boldsymbol{h}_1$ (Algorithm \ref{alg:alg02})\;
			Carry out two C-steps with higher moments (Algorithm \ref{alg:alg05}) resulting $\boldsymbol{\bar{x}}_2$, $\boldsymbol{\hat{S}}_2$, $\boldsymbol{\bar{s}}_2$ and $\boldsymbol{\bar{k}}_2$\;
		}		
		Put $\boldsymbol{h}_1$ as the 10 lowest $det(\boldsymbol{\hat{S}_2})$ computed above\;
		\While{$det(\boldsymbol{\hat{S}}_2) < det(\boldsymbol{\hat{S}}_1)$}{
			Carry out C-steps with higher moments (Algorithm \ref{alg:alg05})\;
		}
		\caption{Fast-MCD with higher moments when $n \leq 600$}
	\end{algorithm}
\end{frame}

\begin{frame}[c]{Network Anomaly Detection Based on Robust Moments}
	The Algorithm \ref{alg:alg07} presents the Fast-MCD with higher moments when $n > 600$, which modifies the original Fast-MCD for computing of Skewness and Kurtosis from the robust estimate data, through the adoption of the C-Step with higher moments. The Algorithm \ref{alg:alg07} is described below.
\end{frame}

\begin{frame}[c]{Network Anomaly Detection Based on Robust Moments}
	\begin{algorithm}[H]\label{alg:alg07}
		\scriptsize
		\SetAlgoLined
		\KwResult{$\boldsymbol{\bar{x}}_2$, $\boldsymbol{\hat{S}}_2$, $\boldsymbol{\bar{s}}_2$ and $\boldsymbol{\bar{k}}_2$}
		Given $\boldsymbol{X} \in \mathbb{R}^{n \times p}$, $h < n$, $p \geq 2$ and $n > 600$\;
		Construct up to 5 disjoint random subsets of size $n_{sub} = 300$\;
		\For{each subset, repeat 100 times}{
			Construct initial $\boldsymbol{h}_1$ of size $h_{sub} = [n_{sub}(h/n)]$\;
			Carry out two C-steps with higher moments of $n_{sub}$ and $h_{sub}$\;
			Keep the 10 best $(\boldsymbol{\bar{x}}_{sub}, \boldsymbol{\hat{S}}_{sub}, \boldsymbol{\bar{s}}_{sub}, \boldsymbol{\bar{k}}_{sub})$\;
		}		
		Pool the subsets, yielding the merged set of $n_{merged} = 1500$\;
		\For{50 best $(\boldsymbol{\bar{x}}_{sub}, \boldsymbol{\hat{S}}_{sub})$}{
			Carry out two C-steps with higher moments of $n_{merged}$ and $h_{merged} = [n_{merged}(h/n)]$\;
			Keep the 10 best $(\boldsymbol{\bar{x}}_{merged}, \boldsymbol{\hat{S}}_{merged}, \boldsymbol{\bar{s}}_{merged}, \boldsymbol{\bar{k}}_{merged})$\;
		}
		\For{the full data set and $m_{full}$ best results}{
			Take several C-steps, using $n$ and $h$\;
			Keep the best final result $(\boldsymbol{\bar{x}}_{full}, \boldsymbol{\hat{S}}_{full}, \boldsymbol{\bar{s}}_{full}, \boldsymbol{\bar{k}}_{full})$\;
		}
		\caption{Fast-MCD with higher moments when $n > 600$}
	\end{algorithm}
\end{frame}

\begin{frame}{Network Anomaly Detection Based on Robust Moments}
    We also propose to use Mahalanobis Distance for detecting anomalies through an approach based on robust Skewness and Kurtosis, instead of the location and scatter that are used by classical approaches.
    
    We propose to modify the Equation \ref{eq:eq01} to implement a Skewness-based Mahalanobis Distance, as follows:
    
    \begin{equation}\label{eq:eq08}
		\boldsymbol{d}(\boldsymbol{s}_{\boldsymbol{x}}, \bar{\boldsymbol{s}}, \boldsymbol{\hat{S}}) = \sqrt{(\boldsymbol{s}_{\boldsymbol{x}} - \bar{\boldsymbol{s}}) \boldsymbol{\hat{S}}^{-1}(\boldsymbol{s}_{\boldsymbol{x}} - \bar{\boldsymbol{s}})^\prime}, 
	\end{equation}
	
	where $\boldsymbol{s}_{\boldsymbol{x}} = \boldsymbol{x} - \boldsymbol{\bar{s}}(\boldsymbol{x})$ and $\boldsymbol{\bar{s}}(\boldsymbol{x})$ is the Skewness of $\boldsymbol{x}$.

\end{frame}

\begin{frame}{Network Anomaly Detection Based on Robust Moments}
    We also propose to modify the Equation \ref{eq:eq04} to implement a Kurtosis-based Mahalanobis Distance, as follows:
    
    \begin{equation}\label{eq:eq09}
		\boldsymbol{d}(\boldsymbol{k}_{\boldsymbol{x}}, \bar{\boldsymbol{k}}, \boldsymbol{\hat{S}}) = \sqrt{(\boldsymbol{k}_{\boldsymbol{x}} - \bar{\boldsymbol{k}}) \boldsymbol{\hat{S}}^{-1}(\boldsymbol{k}_{\boldsymbol{x}} - \bar{\boldsymbol{k}})^\prime}, 
	\end{equation}
	
	where $\boldsymbol{k}_{\boldsymbol{x}} = \boldsymbol{x} - \boldsymbol{\bar{k}}(\boldsymbol{x})$ and $\boldsymbol{\bar{k}}(\boldsymbol{x})$ is the Kurtosis of $\boldsymbol{x}$.
\end{frame}

\begin{frame}[c]{Network Anomaly Detection with Fast-MCD}
	Fast-MCD is commonly used as \textbf{supervised} or \textbf{unsupervised} algorithm for anomaly detection.
    \begin{itemize}		
		\item \textbf{Supervised} approach:
		\begin{enumerate}
		    \item Compute Fast-MCD for training data with known normal and anomalous data.
		    \item Select the best threshold $t$ for detecting known anomalies according to largest distances $\boldsymbol{d}(i)$.
		    \item Save the best $t, \boldsymbol{\bar{x}}$ and $\boldsymbol{\hat{S}}$.
		    \item Compute $d(\boldsymbol{x},\bar{\boldsymbol{x}}, \boldsymbol{\hat{S}})$ for new observations.
		    \item Classify observations with $\boldsymbol{d}(i)$ higher than $t$ as anomalous.
		\end{enumerate}
		\item \textbf{Unsupervised} approach:
		\begin{enumerate}
			\item Given a known contamination $c$, which is the percentage of anomalies in a data set.
			\item Compute Fast-MCD and $d(\boldsymbol{x},\bar{\boldsymbol{x}}, \boldsymbol{\hat{S}})$ for new observations.
			\item Classify the $c$ observations with largest distances as anomalous.
		\end{enumerate}
	\end{itemize}
\end{frame}

\begin{frame}[c]{Network Anomaly Detection Based on Robust Moments}
    Finally, We propose the following \textbf{Semi Supervised} approach for Fast-MCD based anomaly detection:
	\begin{enumerate}
		\item Compute Fast-MCD for training data containing only known normal observations.
		\item Save the fitted $\boldsymbol{\bar{x}}_{normal}$ and $\boldsymbol{\hat{S}}_{normal}$.
		\item Carry out cross validation on known contaminated data in order to identify the contamination $c$ that best classify anomalies and normal data.
		\item Compute $d(\boldsymbol{x},\bar{\boldsymbol{x}}_{normal}, \boldsymbol{\hat{S}}_{normal})$ for new observations.
		\item The $c$ observations with largest $\boldsymbol{d}(i)$ are classified as anomalous.
	\end{enumerate}
\end{frame}


%section-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
\section{Data Model}

\begin{frame}[c]{The CTU-13 Dataset}
	Evaluating network anomaly detection solutions remains a challenge due to the lack of up-to-date real-world data sets for training and testing \cite{osanaiye2016distributed}. 
	\begin{itemize}
		\item We propose to use CTU-13 \cite{garcia2014empirical} to evaluate our proposal, considering that the CTU-13 is a data set that aims to have a large capture of real botnet attacks and C\&C traffic mixed with normal and background traffic.
		\item The CTU-13 data set consists in thirteen captures (called scenarios) of different botnet samples. 
		\item On each scenario it was executed a specific malware, which used several protocols and performed different actions.		
	\end{itemize}
\end{frame}

\begin{frame}[c]{The CTU-13 Dataset}
	\begin{itemize}
		\item The botnet traffic is divided into attacks and Command and Control (C\&C) traffic.
		\begin{itemize}
			\item \textbf{Attacks:} Click Fraud, Port Scan, FastFlux, Compiled and Controled by Authors, SPAM and DDOS.
			\item \textbf{C\&C:} IRC, P2P and HTTP.
		\end{itemize}
		\item The data set contains the following features:
		\begin{itemize}
			\item Start Time, Duration, Protocol, Source Address, Source Port, Direction, Destination Address, Destination Port, State, Type of service from source to destination, Type of service from destination to source, Total of Packets, Total of bytes, Total bytes from source to destination, Label as normal or anomalous
		\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}[c]{The CTU-13 Dataset}
	\begin{table}[h!]
		\tiny
		\caption{CTU-13 Dataset Description}
		\label{tab:tab01}
		\begin{tabular}{| l | l | l | l | l | l | l | l | l | l | l | }
			\hline \rowcolor{Gray} \begin{tabular}[x]{@{}l@{}}Scenario\end{tabular}	& \begin{tabular}[x]{@{}l@{}}Malware\end{tabular}	 & \begin{tabular}[x]{@{}l@{}}Type\end{tabular}	& \begin{tabular}[x]{@{}l@{}}Total Flows\end{tabular} & \begin{tabular}[x]{@{}l@{}}Malicious\end{tabular} & \begin{tabular}[x]{@{}l@{}}C\&C\end{tabular} & \begin{tabular}[x]{@{}l@{}}Botnet\end{tabular}\\ \hline
				10 & neris & IRC, Spam, CF & 2,824,636 & 40,961 (1.45\%) & 341 (0.01\%) & 40,620 (1.44\%) \\ \hline
				11 & neris & IRC, Spam, CF & 1,808,122 & 20,941 (1.16\%) & 673 (0.04\%) & 20,268 (1.12\%) \\ \hline
				12 & rbot & IRC, PS, US & 4,710,638 & 26,822 (0.57\%) & 63 (0.00\%) & 26,759 (0.57\%) \\ \hline
				15 & rbot & IRC, DDoS, US & 1,121,076 & 2,580 (0.23\%) & 52 (0.00\%) & 2,528 (0.23\%) \\ \hline
				15-2 & virut & Spam, PS,HTTP & 129,832 & 901 (0.69\%) & 24 (0.02\%) & 877 (0.68\%) \\ \hline
				16 & menti & PS & 558,919 & 4,630 (0.83\%) & 199 (0.04\%) & 4,431 (0.79\%) \\ \hline
				16-2 & sogou & HTTP & 114,077 & 63 (0.06\%) & 26 (0.02\%) & 37 (0.03\%) \\ \hline
				16-3 & murlo & PS & 2,954,230 & 6,127 (0.21\%) & 1,074 (0.04\%) & 5,053 (0.17\%) \\ \hline
				17 & neris & IRC, Spam, CF,PS & 2,087,508 & 184,987 (8.86\%) & 2,973 (0.14\%) & 182,014 (8.72\%) \\ \hline
				18 & rbot & IRC, DDoS, US & 1,309,791 & 106,352 (8.12\%) & 33 (0.00\%) & 106,319 (8.12\%) \\ \hline
				18-2 & rbot & IRC, DDoS, US & 107,251 & 8,164 (7.61\%) & 2 (0.00\%) & 8,162 (7.61\%) \\ \hline
				19 & nsys.ay & P2P & 325,471 & 2,168 (0.67\%) & 25 (0.01\%) & 2,143 (0.66\%) \\ \hline
				15-3 & virut & Spam, PS, HTTP & 1,925,149 & 40,003 (2.08\%) & 536 (0.03\%) & 39,467 (2.05\%) \\ \hline
		\end{tabular}
	\end{table}

	Table \ref{tab:tab01} presents the malwares used for botnet attacks, the attack and C\&C types, the total number of flows, the number of malicious flows, which includes the C\&C and botnet flows. Table \ref{tab:tab01} also shows the ratio of C\&C and botnet flows.
\end{frame}

\begin{frame}[c]{The CTU-13 Dataset}
	\begin{itemize}
		\item Table \ref{tab:tab01} shows that CTU-13 is very imbalanced data set, when comparing the amount of malicious to normal traffic.
		\item Malicious observations are beteeen $0.06\%$ up to $8.86\%$ of all flows.
		\item The scenarios have between $107,251$ and $4,710,638$ flows.
		\item The CTU-13 data set is also very skewed. Figure \ref{fig:fig06} exemplifies the skewed distribution of normal and anomalous flows, showing the flow duration of scenario 12.
	\end{itemize}
\end{frame}

\begin{frame}[c]{The CTU-13 Dataset}
	\begin{figure}[h!]
	     \centering
	     \includegraphics[width=8cm]{figures/raw_distplot_capture20110812_Dur.png}
	     \caption{Distribution of flow duration for scenario 12.}
	     \label{fig:fig06}
	\end{figure}
\end{frame}

\begin{frame}[c]{The CTU-13 Dataset}
	\begin{itemize}
		\item Methods for imbalance learning has attracted growing attention from both academia and industry, due to the restrictions imposed by imbalanced data to machine learning algorithms.
		\item He and Garcia \cite{he2008learning} present a classification of imbalanced learning methods as sampling, cost-sensitive learning, kernel-based, and active learning.
		\item Data aggregation is a widely adoted approach for feature engineering \cite{chandrashekar2014survey,acarali2016survey}.
		\item Lakhina et al. \cite{lakhina2005mining} argue that the distributions of packet features (IP addresses and ports) observed in flow traces reveals both the presence and the structure of a wide range of anomalies.
	\end{itemize}
\end{frame}

\begin{frame}[c]{The CTU-13 Dataset}
	To deal with the data imbalance of CTU-13 data set, we adopt a sampling method based on data aggragation \cite{acarali2016survey} for feature generation, in order to obtain less imbalanced and more discrimnative data.	
	\begin{itemize}
		\item Based on related works \cite{lakhina2005mining,chandrashekar2014survey,acarali2016survey} and experience \cite{vieira2017model, galibus2017offline}, we selected some features for improving the anomaly detection.
		\item The features are generated by a data aggregation into a selected window size. We evalute our proposal for windows with 0.15, 0.25, 1 and 2 seconds, for each scenario of the CTU-13 data set.
	\end{itemize}
\end{frame}

\begin{frame}[c]{The CTU-13 Dataset}
	Following (2 slides) are the selected features:
	\begin{itemize}
		\item \textbf{normal-flow-count:} count of normal (known) flow.
		\item \textbf{background-flow-count :} count of background (unknown) flows.
		\item \textbf{avg-duration:} average duration time of flows.
		\item \textbf{n-conn:} count of flows.
		\item \textbf{n-icmp:} count of ICMP packets.
		\item \textbf{n-tcp:} count of TCP packets.
		\item \textbf{n-udp:} count of UDP packets.
		\item \textbf{n-dports<1024:} count of destinations ports lower than 1024.
		\item \textbf{n-sports<1024:} count of source ports lower than 1024.
		\item \textbf{n-sports>1024:} count of source ports higher than 1024.
	\end{itemize}
\end{frame}

\begin{frame}[c]{The CTU-13 Dataset}
	Following are the remaining selected features:
	\begin{itemize}
		\item \textbf{n-s-a-p-address:} count of class A source address.
		\item \textbf{n-d-a-p-address:} count of class A destination address.
		\item \textbf{n-s-b-p-address:} count of class B source address.
		\item \textbf{n-d-b-p-address:} count of class B destination address.
		\item \textbf{n-s-c-p-address:} count of class C source address.
		\item \textbf{n-d-c-p-address:} count of class C destination address.
		\item \textbf{n-s-na-p-address:} count of class D* source address.
		\item \textbf{n-d-na-p-address:} count of class D* destination address.
	\end{itemize}
\end{frame}

\begin{frame}[c]{The CTU-13 Dataset}
	\begin{itemize}
		\item Figures \ref{fig:fig07} and \ref{fig:fig08} show the distribution of average flow duration, for scenario 12, aggregated by windows of 0.15 and 2 seconds.
		\item In comparison to Figure \ref{fig:fig06}, that shows the distribution of flow duration for the same scenario, we can see that the feature based on the average is more discriminative.
	\end{itemize}
\end{frame}

\begin{frame}[c]{The CTU-13 Dataset}
	\begin{figure}[h!]
	     \centering
	     \includegraphics[width=8cm]{figures/agg_distplot_0_15s_12_avg_duration.png}
	     \caption{Distribution of average flow duration for scenario 12, aggregated by windows of 0.15 seconds.}
	     \label{fig:fig07}
	\end{figure}
\end{frame}

\begin{frame}[c]{The CTU-13 Dataset}
	\begin{figure}[h!]
	     \centering
	     \includegraphics[width=8cm]{figures/agg_distplot_2s_12_avg_duration.png}
	     \caption{Distribution of average flow duration for scenario 12, aggregated by windows of 2 seconds.}
	     \label{fig:fig08}
	\end{figure}
\end{frame}


%section-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
\section{Experiment}

\begin{frame}[c]{Experiment}
	\textbf{Experiment Plan:}
	\begin{itemize}
		\item Given a contaminated data set, split it into normal (60\%), cross validation (20\% normal + 50\% anomalous) and test (20\% normal + 50\% anomalous).
		\item Select the range of contamination that shall be evaluated and perform cross validation for each contamination. 
		\item For each contamination $c$:
		\begin{enumerate}
			\item Estimate $\boldsymbol{\bar{x}}_{normal}$ and $\boldsymbol{\hat{S}}_{normal}$ from normal data.
			\item Predict anomalies for cross validation data using the $c$, $\boldsymbol{\bar{x}}_{normal}$ and $\boldsymbol{\hat{S}}_{normal}$.
			\item Select the best F-measure by cross validation and get the best $c$.
		\end{enumerate}
		\item Test the F-measure of anomaly detection for new observations through $d(\boldsymbol{x},\bar{\boldsymbol{x}}_0, \boldsymbol{\hat{S}}_0)$ and $c$.
	\end{itemize}
\end{frame}

\begin{frame}[c]{Experiment}
	\textbf{Experiment Plan:}
	\begin{itemize}
	    \item Evaluation Measure:
		\begin{itemize}
			\item In anomalous detection problems, where anomalies are rare events, if we classify all observations as normal and apply a accuracy evaluation, we would have high accuracy but poor true-positive detection.
			\item Due to the importance given by F-measure to true-positive detection \cite{moustafa2019holistic}, the F-measure is the preferable measure for imbalanced data sets.
    	\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}[c]{Experiment}
	\textbf{Experiment Plan:}
	\begin{itemize}
		\item The F-measure is the harmonic mean of precision and recall and is defined by 
		\begin{equation}\label{eq:eq10}
			F-measure = 2 * \frac{PR * RC}{PR + RC},				
		\end{equation}
		where $PR$ is the precision, defined by 
		\begin{equation}\label{eq:eq11}
			PR = \frac{True Positive}{True Positive + False Positive}
		\end{equation}
		and $RC$ is the recall, defined by 
		\begin{equation}\label{eq:eq12}
			RC = \frac{True Positive}{True Positive + False Negative}.
		\end{equation}
	\end{itemize}	
\end{frame}

\begin{frame}[c]{Experiment}
	\textbf{Data splitting for training, cross validation and testing}
	\begin{itemize}
		\item Gargia et al. \cite{garcia2014empirical} propose a division of CTU-13's scearios into train and test, for evaluation of network attack detection algorithms. 
		\item However, we decide to evaluate our proposed approach for each scenario individually, considering that in the division proposed by Gargia et al. some bots are present only for training and not for testing. 
		\item Although our proposal does not rely on anomalous data for training, we desire to evaluate our proposal to detect all types of bots captured by CTU-13 data set.
	\end{itemize}
\end{frame}

\begin{frame}[c]{Experiment}
	We propose to compare our approach to clustering and statisctical based approaches (K-means and Gaussian Mixture Model (GMM), respectively).
	\begin{itemize}
		\item As clustering-based method, we selected K-means, which is the clustering algorithm most adopted for general purposes and for network anomaly detection \cite{gaddam2007k,moustafa2019holistic}.
		\item It is observed that network traffic do not belong to a Gaussian distribution \cite{benson2010network,moustafa2019holistic}, algorithms based on non-Gaussian distributions, such as GMM, has been adopted for network anomaly detection \cite{moustafa2019holistic}.
	\end{itemize}
\end{frame}


%section-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
\section{Results}

\begin{frame}[c]{Results}
	\begin{table}[h!]
	  \centering
	  \scriptsize
	  \caption{Results grouped by algorithm and scenario}
	  \label{tab:tab02}
	  \begin{tabular}{ c|c c|c c|c c|c c|c c }
		\toprule
		\multirow{2}{*}{\textbf{Scenario}}   &\multicolumn{2}{c}{\textbf{S-MCD}} &\multicolumn{2}{c}{\textbf{K-MCD}} &\multicolumn{2}{c}{\textbf{MCD}} &\multicolumn{2}{c}{\textbf{GMM}} &\multicolumn{2}{c}{\textbf{K-Means}}\\ 
				\hhline{~----------}
				&\textbf{0.15} &\textbf{0.25} &\textbf{0.15} &\textbf{0.25} &\textbf{0.15} &\textbf{0.25} &\textbf{0.15} &\textbf{0.25} &\textbf{0.15} &\textbf{0.25}\\
		\midrule
			10 &\color{red} 0.71 & 0.03 & 0.66 & 0.02 & 0.34 & 0.52 & 0.34 & 0.46 & 0.20 & 0.25 \\ \hline
			11 & 0.03 & 0.54 & 0.00 & 0.54 & 0.71 &\color{red} 0.73 & 0.35 & 0.48 & 0.13 & 0.15 \\ \hline
			12 & 0.19 & 0.02 & 0.20 & 0.05 & 0.06 &\color{red} 0.31 & 0.01 & 0.01 & 0.09 & 0.11 \\ \hline
			15 & 0.00 & 0.00 & 0.00 & 0.00 & 0.24 &\color{red} 0.32 & 0.14 & 0.22 & 0.09 & 0.11 \\ \hline
			15-2 & 0.04 & 0.85 & 0.67 &\color{red} 0.86 & 0.36 & 0.28 & 0.32 & 0.14 & 0.18 & 0.26 \\ \hline
			15-3 & 0.00 & 0.01 & 0.00 & 0.01 &\color{red} 0.55 & 0.48 & 0.00 & 0.01 & 0.22 & 0.29 \\ \hline
			16 & 0.45 & 0.36 & 0.42 & 0.33 & 0.27 & 0.41 &\color{red} 0.51 & 0.49 & 0.07 & 0.16 \\ \hline
			16-2 & 0.00 & 0.16 & 0.00 &\color{red} 0.20 & 0.04 & 0.04 & 0.02 & 0.08 & 0.04 & 0.03 \\ \hline
			16-3 & 0.00 & 0.05 & 0.00 & 0.07 &\color{red} 0.15 & 0.10 & 0.01 & 0.01 & 0.03 & 0.04 \\ \hline
			17 & 0.03 & 0.03 & 0.07 & 0.03 &\color{red} 0.59 & 0.53 & 0.16 & 0.47 & 0.35 & 0.39 \\ \hline
			18 & 0.63 & 0.10 & 0.63 & 0.08 & 0.70 &\color{red} 0.72 & 0.21 & 0.17 & 0.13 & 0.20 \\ \hline
			18-2 & 0.83 & 0.69 & 0.85 & 0.43 & 0.84 &\color{red} 0.91 & 0.50 & 0.55 & 0.62 & 0.70 \\ \hline
			19 &\color{red} 0.71 & 0.00 & 0.68 & 0.00 & 0.11 & 0.42 & 0.17 & 0.20 & 0.20 & 0.17 \\ \hline
			\rowcolor{Gray} \textbf{avg} & \textbf{0.28} & \textbf{0.22} & \textbf{0.32} & \textbf{0.20} & \textbf{0.38} &\color{red} \textbf{0.44} & \textbf{0.21} & \textbf{0.25} & \textbf{0.18} & \textbf{0.22} \\ 
	    \bottomrule
	  \end{tabular}
	\end{table}
\end{frame}

\begin{frame}[c]{Results}
	\begin{table}[h!]
	  \centering
	  \scriptsize
	  \caption{Results grouped by algorithm and scenario}
	  \label{tab:tab03}
	  \begin{tabular}{ c|c c|c c|c c|c c }
		\toprule
		\multirow{2}{*}{\textbf{Scenario}}   &\multicolumn{2}{c}{\textbf{RPCA}} &\multicolumn{2}{c}{\textbf{S-RPCA}} &\multicolumn{2}{c}{\textbf{S-RPCA2}} &\multicolumn{2}{c}{\textbf{MCD}}\\ 
				\hhline{~--------}
				&\textbf{0.15} &\textbf{0.25} &\textbf{0.15} &\textbf{0.25} &\textbf{0.15} &\textbf{0.25} &\textbf{0.15} &\textbf{0.25}\\
		\midrule
			10 & 0.93 & 0.93 &\color{red} 0.98 &\color{red} 0.98 &\color{red} 0.99 &\color{red} 1.00 & 0.34 & 0.52 \\ \hline
			11 & 0.96 & 0.94 &\color{red} 0.99 &\color{red} 0.98 &\color{red} 0.99 &\color{red} 1.00 & 0.71 & 0.73 \\ \hline
			12 & 0.32 & 0.19 & 0.00 & 0.00 &\color{red} 0.81 & 0.01 & 0.06 & 0.31 \\ \hline
			15 & 0.75 & 0.73 & 0.86 &\color{red} 0.96 &\color{red} 1.00 &\color{red} 0.96 & 0.24 & 0.32 \\ \hline
			15-2 & 0.40 & 0.57 &\color{red} 0.95 &\color{red} 0.94 & 0.25 & 0.93 & 0.36 & 0.28 \\ \hline
			15-3 & 0.68 & 0.68 &\color{red} 0.96 & 0.70 &\color{red} 0.96 &\color{red} 0.96 & 0.55 & 0.48 \\ \hline
			16 &\color{red} 0.47 &\color{red} 0.47 & 0.01 & 0.01 & 0.01 & 0.00 & 0.27 & 0.41 \\ \hline
			16-2 & 0.07 & 0.10 &\color{red} 0.80 & 0.68 & 0.00 & 0.00 & 0.04 & 0.04 \\ \hline
			16-3 & 0.29 & 0.29 & 0.01 & 0.46 &\color{red} 0.79 & 0.76 & 0.15 & 0.10 \\ \hline
			17 &\color{red} 0.82 & 0.79 &\color{red} 0.81 & 0.78 &\color{red} 0.82 & 0.79 & 0.59 & 0.53 \\ \hline
			18 & 0.87 & 0.89 & 0.97 &\color{red} 0.98 &\color{red} 0.98 &\color{red} 0.98 & 0.70 & 0.72 \\ \hline
			18-2 &\color{red} 0.94 &\color{red} 0.94 &\color{red} 0.91 & 0.86 & 0.09 & 0.10 & 0.84 & 0.91 \\ \hline
			19 & 0.27 & 0.31 &\color{red} 0.95 &\color{red} 0.95 & 0.02 & 0.02 & 0.11 & 0.42 \\ \hline
			\rowcolor{Gray} \textbf{Avg} & \textbf{0.60} & \textbf{0.60} &\color{red} \textbf{0.71} &\color{red} \textbf{0.71} & \textbf{0.59} & \textbf{0.58} & \textbf{0.38} & \textbf{0.44} \\
	    \bottomrule
	  \end{tabular}
	\end{table}
\end{frame}

\begin{frame}[c]{Results}
	Results grouped by algorithm for full CTU-13.
	\begin{enumerate}
		\item ...
	\end{enumerate}
\end{frame}

\begin{frame}[c]{Discussion}
	Discuss results..
	\begin{enumerate}
		\item there are no lots of researches using CTU-13, although it is been growing
		\item poor results for GMM, K-Means and MGM are ok and in accordance to \cite{wang2017botnet}, which also evaluate \cite{gu2007bothunter}
	\end{enumerate}
\end{frame}

%section-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
\section{Next Steps}
\begin{frame}[c]{Next Steps}
	\begin{enumerate}
		\item Say that the data is lognormal \cite{benson2010network} and link it to skewness https://www.weibull.com/hotwire/issue47/relbasics47.htm
		\item Test distribution fit again e present results
	    \item Conclude this presentation.
		\item Evaluate eigensim for CTU-13 and others algorithms for Danilo's data set.
		\item Compare to vanilla RPCA (Eduardo is working on it).
		\item Write a paper for JNCA (April).
		\item Write the thesis (May and June).
	\end{enumerate}
\end{frame}


%section-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-
\begin{frame}[plain, allowframebreaks]{Bibliography}
	\setbeamertemplate{bibliography item}[text]
	\printbibliography
\end{frame}

\end{document}

%PORTUGUESE
\resumo{Resumo}{

Resumo em Português, espaçamento de 1.5 requerido pela norma (automático).

}
%ENGLISH
\vspace{2cm}
\resumo{Abstract}{

Traditionally, cyber defense methods can be effective against well known types of attacks, yet may fail against innovative malicious techniques. In order to be able to detect and avoid novel attacks and their variations, it is necessary to develop or improve techniques to achieve efficiency in resource consumption, processing capacity and response time. Moreover, it is crucial to obtain high detection accuracy and capacity to detect variations of malicious patterns. Recently, signal processing schemes have been applied to detect malicious traffic in computer networks, showing advances in network traffic analysis.

Information security may consist of both technical and procedural aspects. The former includes equipment and security systems, while the latter corresponds to security rules and recommendations of good practices. Intrusion detection and intrusion prevention systems are two types of security systems that are being used, respectively, to detect (passively) and prevent (proactively) threats to computer systems and computer networks. Such systems can work in the following fashions: signature-based, anomaly-based or hybrid .

Anomaly detection can be defined as the identification of rare and suspicious events by comparing it to the normal data. Anomalies are also referred to as outliers, novelties, noise or deviations, and can be related to network attacks, frauds or defects. Additionally, anomaly detection techniques can be categorized by classification, statistical algorithms, information theory and cluster based algorithms. Anomalies can be hard to identify and separate from normal data due to the rare occurrences of anomalies in comparison to normal events. Therefore anomaly detection algorithms have to be highly discriminating, robust to corruption and able to deal with the imbalanced data problem.

Several efforts and researches aim to avoid network attacks based on known attackers, fingerprints or behavioral analysis. However, distributed attacks organized by botnet has increased and demanded the development of counter measures. This has been done to detect and avoid unknown attacks or even to deal with adversarial changes of behavior, location and other patterns. This has also been done in order to avoid detection by behavioral or pattern based detection systems. To face this problem, it is possible to adopt unsupervised or semi-supervised approaches for network anomaly detection, where it is not necessary known anomalies for training models.

Recent developments in science and technology have enabled the growth and availability of raw data to occur at an explosive rate. This has created an immense opportunity for knowledge discovery and data engineering research to play an essential role in a wide range of applications from daily civilian life to national security. Despite the actual high availability of information, the relevant information of some observations is generally of under much reduced dimensionality compared to available data sets. The extraction of relevant information by identifying the generating causes within classes of signals is useful for classification problems and for security analysis. 

The high availability of raw data increases the challenges related to big data analytics and to imbalanced learning problems, which corresponds to data sets exhibiting significant imbalances of classes or rare events of some classes. The fundamental issue with the imbalanced learning is the ability of imbalanced data to significantly compromise the performance of most standard learning algorithms. Data analysis of imbalanced data is challenging for classification and prediction problems related to anomaly detection, novelty detection, fraud detection and attack detection.

Some widely adopted algorithms for anomaly detection assume a gaussian distributed data, however this assumption may not be observed in real world problems, such as the case of network traffic analysis, where network traffic features are usually more characterized by skewed and heavy-tailed distributions. It was observed that the skeweness and heavy-tailed distributions can impact algorithms that rely on gaussian distributed data, as well as it can reveal characteristics that can be exploited in order to obtain accurate classifiers for network anomaly detection.

Therefore, as a first important contribution, we propose the Eigensimilarity, which is an approach based on signal processing concepts applied to detection of malicious traffic in computer networks. Eigensimilarity models the network traffic using a signal processing formulation as a composition of three components: signal, artifact and noise, taking into account the incoming and outgoing traffic in certain types of network ports (TCP or UDP). The Eigensimilarity is based on eigenvalue analysis, model order selection (MOS) and similarity analysis between eigenvectors, where MOS and eigenvalue analysis are applied to detect time frames under attack. We evaluate the accuracy and performance of the proposed framework applied to a experimental scenario and to the DARPA 1998 data set, which is a well known network traffic data set. The performed experiments show that synflood, fraggle and port scan attacks can be detected accurately and with great detail in an automatic and blind fashion, i.e. in an unsupervised approach, applying signal processing concepts for traffic modeling and through approaches based on MOS and eigen similarity analysis. The main contributions of the proposed framework are the capability to blindly detect time frames under network attack via MOS and eigen analysis, and the detailed identification of the network attack via eigen similarity analysis.


Some widely adopted algorithms for anomaly detection assume a Gaussian or symmetric distributed data, however this assumption may not be observed in some real world problems, such as the case of network traffic analysis, where network traffic features are usually more characterized by skewed and heavy-tailed distributions \cite{lakhina2005mining,benson2010network, leon2017probability}.

Findings of Benson \emph{et al.}  \cite{benson2010network} indicate that certain positive skewed and heavy-tailed distributions can model data center switch traffic, and highlights a difference between the data center environment and the wide area network, where the long-tailed Pareto distribution typically shows the best fit \cite{benson2010network}. Leon-Garcia \cite{leon2017probability} also argues that Pareto distribution has been found to capture the behavior of many quantities of interest in the study of Internet behavior. Moreover, Benson \emph{et al.}  \cite{benson2010network} observes that the Lognormal distribution is the best fit to model arrival processes in a data center.

These findings show that the skewness and heavy-tailed distributions may be important for network traffic analysis, and can motivate researches to evaluate the impact of skewed data into algorithms that rely on Gaussian distributed data. These findings can also highlight opportunities to  exploit the skewness and heavy-tailed distributions to obtain accurate classifiers for network anomaly detection.

An outlyingness-approach based on a robust estimator of skewness, combined with robust estimators of location and scale, can be able to flag the outlying measurements. When the same methodology would be used with non-robust estimators of location, scale and skewness, the outlyingness-values would be affected by the outliers such that the outlying group could be masked \cite{hubert2009robustskewed}.

Network anomaly detection problems are usually characterized by imbalanced data \cite{Phua2004minority,he2008learning,benson2010network}. However, learning algorithms for imbalanced data has been a challenging research topic, considering that the fundamental issue with the imbalanced learning problem is the ability of imbalanced data to significantly compromise the performance of most standard learning algorithms \cite{he2008learning}. Therefore, learning methods for imbalanced and skewed data have been attracted attention of researchers \cite{Phua2004minority,hubert2009robustskewed}.

We believe that the skewness of anomalous and normal traffic can highlight features for improving anomaly detection in imbalanced data. We also believe that the distance between robust estimates of normal traffic and contaminated observations can highlight discrepancies and be used for network attack detection. 

Therefore, we propose the m-RPCA, which is an approach based on distances between moments computed from a robust subspace learned by Robust Principal Component Analysis (RPCA) and contaminated observations, in order to detect anomalies from skewed data and network traffic. The proposed approach relies on a robust subspace computed from supposed normal observations, for estimating the moments to be used for distance analysis. The anomaly detection from contaminated observations evaluate the Mahalanobis distance between the robust moments and new contaminated observations, in a semi-supervised fashion, without the computational cost of new robust subspace learning for new observations. The m-RPCA can also be computed as an unsupervised algorithm, with subspace learning from the same contaminated data that is the target of the anomaly detection analysis.

We evaluate the accuracy of the m-RPCA for anomaly detection on simulated data sets, with skewed and heavy-tailed distributions, and for the CTU-13 data set \cite{garcia2014empirical}, which is a large data set of normal, background and botnet traffic that has been adopted to deal with the lack of up-to-date real-world data sets for anomaly detection systems \cite{osanaiye2016distributed}. The Experimental evaluation compares our proposal to widely adopted algorithms for anomaly detection based on clustering and statistical approaches. We also compare the results to ROBPCA \cite{hubert2005robpca}, which is a method that also relies on robust estimates with adjusted outlyingness based on robust skewness.

The main contribution of this work is the proposal of a novel semi-supervised and unsupervised method for anomaly detection in skewed and imbalanced data, with results of experimental evaluation on simulated and real data sets.

This Chapter is organized as follows. In Section \ref{sec:4_relatedworks}, it is conducted a literature review of network anomaly detection, botnet detection, and imbalanced learning. Section \ref{sec:4_datamodel} presents the data model and the evaluated data set. Section \ref{sec:4_proposal} describes the proposed approach for network attack detection. Section \ref{sec:4_experiments} discusses the experimental validation, Section \ref{sec:4_simulated_result} presents the results of the simulated scenarios and the Section \ref{sec:4_ctu13_result} describes the results for anomaly detection on CTU-13 data set. Finally, Section \ref{sec:4_conclusion} draws the conclusions and the suggestions for future work.

The protection scheme used in a mobile device should be both computationally secure as well as resource-constrained due to battery power limitations \cite{khan2015cloud}. Therefore, encrypting files and generating keys on a mobile device is not considered a good solution. On the other hand, the protection schemes with good computational qualities lack the security analysis in many cases \cite{khan2014bss}. The common practice is the shadow user activities monitoring \cite{yovel2014}. However, the mobile device usage stays unprotected in all the proposed scenarios while in offline mode. When the mobile client goes offline with the sensitive corporate data on board all powerful cloud-based tools can not help and the mobile client has to secure itself with its own limited resources. Moreover, due to the resources constraint, there is a crucial difference in strategy of online and offline mode protection.

Additional security issues and requirements have to be considered when mobile clients are actively used in corporate cloud environment \cite{yovel2014}. Today more and more organizations and enterprises are functioning in the Bring-Your-Own-Device (BYOD) paradigm. The uncontrolled usage of the mobile devices represents a serious risk to the development of secure SME cloud platforms being the bottleneck of the corporate information security system (ISS). While the enterprise cloud infrastructure based on the web interface can be protected by powerful third-party services, such as CASB and CAC, the corporate mobile client is usually light-weighted and generally less protected. 

This work proposes an architecture and approach for user behavior analysis based on Eigensim \cite{vieira2017model}, in order to detect possible threats in offline corporate mobile applications. The key expiration period is safely incorporated into the proposed system solution in order to enhance security and the behavioral analysis can indicate malicious behaviors, their variations, as well as novel attacks, which present low or high variance in comparison to legitimate user behaviors.

The work is structured as follows. Section \ref{sec:3_related_works} analyzes the most common security problems in the mobile cloud environment and the solutions for offline protection in the BYOD world. Section \ref{sec:3_the_client_security_architecture} outlines the mobile security architecture of the proposed solution. Section \ref{sec:3_offline_mode} presents the detailed scheme of the proposed solution to the problem of offline mobile client security. Section \ref{sec:3_eigensim} explains the use of Eigensim. Section \ref{sec:3_results} discusses the common threat scenarios, the data modeling, the performance analysis and discussion of the practical implementation results. Section \ref{sec:3_conclusion} concludes the chapter.

}

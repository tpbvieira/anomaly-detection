\chapter{Model Order Selection and Eigen Similarity based Framework for Detection and Identification of Network Attacks}
\label{ch:evaluation}

\begin{quotation}[]{Lao Tzu}
All difficult things have their origin in that which is easy, and great things in that which is small. 
\end{quotation}

The use of MapReduce for distributed data processing has been growing and achieving benefits with its application for different workloads. MapReduce can be used for distributed traffic analysis, although network traffic traces present characteristics which are not similar to the data type commonly processed through MapReduce, which in general is divisible and text-like data, while network traces are binary and may present restrictions about splitting, when processed through distributed approaches. 

Due to the lack of evaluation of MapReduce for traffic analysis and the peculiarity of this kind of data, this chapter deeply evaluates the performance of MapReduce in packet level analysis and DPI of distributed application traffic, evaluating its scalability, speed-up and the behaviour followed by MapReduce phases. The experiments provide evidences for the predominant phases in this kind of MapReduce job, and show the impact of input size, block size and number of nodes, on completion time and scalability.

This chapter is organized as follows. We first describe the motivation for a MapReduce performance evaluation for network traffic analysis in Section \ref{sc:eval_motivation}. Then we present the evaluation plan and methodology adopted in Section \ref{sc:eval_evaluation}, and the results are presented in Section \ref{sc:eval_results}. Section \ref{sc:eval_discussion} discusses the results and Section \ref{sc:eval_summary} summarizes the chapter.

\section{Motivation}
\label{sc:eval_motivation}

It is possible to measure, evaluate and diagnose distributed applications through the evaluation of information from communication protocols, flows, throughput, and load distribution \citep{Mi2012, Nagaraj2012, Sambasivan2011, Aguilera2003, Yu2011}. This information can be collected through network traffic analysis, but to retrieve application information from network traces, it is necessary to recognize the application protocol and deeply inspect the traffic to retrieve details about its behaviour, session and states.

MapReduce can be used for offline evaluation of distributed applications, analysing application traffic inside a data center, through packet level analysis \citep{Lee2011}, evaluating each packet individually, and through DPI \citep{Vieira2012_icpads,Vieira2012_icsea}, adopting a different approach for data splitting, where a whole block is processed without division into individual packets, due to the necessity to reassemble two or more packages to retrieve information of the application layer, in order to evaluate application messages and protocols. 

The kind of workload submitted for processing by MapReduce impacts on the behaviour and performance of MapReduce \citep{Tan2012, Groot2012}, requiring specific configuration to obtain an optimal performance. Information about the occupation of MapReduce phases, the processing characteristics (if the job is I/O or CPU bound), and the mean duration of Map and Reduce tasks, can be used to optimize parameter configurations, and to improve resource allocation and task scheduling. 

The main evaluations of MapReduce are in text processing \citep{Zaharia2008, Yanpei2011, Jiang2010, Guanying2009}, where the input data are split into blocks and into records, to be processed by parallel and independent Map functions. For distributed processing of network traffic traces, which are usually binary, the data splitting into packets is a concern and, in some cases, may require data without splitting, especially when packet reassembly is required to extract application information from the application layer. 

Although work has been done to understand, analyse and improve workload management decisions in MapReduce \citep{Lu2012, Groot2012}, there is no evaluation to characterize the MapReduce behaviour or to identify its optimal configuration to achieve the best performance for packet level analysis and DPI.

Due to the lack of evaluation of MapReduce for traffic analysis and the peculiarity of this kind of data, it is necessary to understand the behaviour of MapReduce to process network traces and to understand what optimizations can be done to better explore the potential provided by MapReduce for packet level analysis and DPI. 

This chapter evaluates MapReduce performance for network packet level analysis and DPI using Hadoop, characterizing the behaviour followed by MapReduce phases, scalability and speed-up, over variation of input, block and cluster sizes. The main contributions of this chapter are:

\begin{enumerate}
	\item Characterization of MapReduce phases behaviour for packet level analysis and DPI;
	\item Description of scalability behaviour and its relation with important MapReduce factors;
	\item Identification of the performance provided by the block sizes adopted for different cluster size;
	\item Description of speed-up obtained for DPI.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% na secao de metodologia, coloque uma figura para ilustrar todo o processo de coleta e analise. além disso, coloque numa tabela (ou mais de uma) a descricao das metricas de interesse, alem dos fatores e niveis selecionados em outra tabela. explique o motivo desses fatores influenciarem as metricas de interesse.
% Falar sobre o planejamento da avaliacao, destacando como sera feita an avaliacao e por que sera feito desta maneira. eh importante destacar que sera utilizada uma versao de p3 para comparar com a solucao desenvolvida.
% Descrever que avaliacoes de speedup e comportamento das fases foram aplicados para ambos os tipos analisados, packet analysis e dpi. Um comparativo especifico foi feito para identificar o desempenho de non-spliting contra p3
\section{Evaluation}
\label{sc:eval_evaluation}

The goal of this evaluation is to characterize the behaviour of MapReduce phases, its scalability characteristics over node addition and the speed-up achieved with MapReduce for packet level analysis and DPI. Thus, we performed a performance measurement and evaluation of MapReduce jobs that execute packet level analysis and DPI algorithms. 

To evaluate MapReduce for DPI, the Algorithm \ref{alg:JXTAperfmapper} implemented by \textit{JXTAPerfMapper} and \textit{JXTAPerfReducer} was used, and applied to new factors and levels. To evaluate MapReduce for packet level analysis, a port counter algorithm developed by \cite{Lee2011} was used, which divides a block into packets and processes each packet individually to count the number of occurrence of TCP and UDP port numbers. The same algorithm was evaluated using the splitting approach that processes a whole block per Map function, without the division of a block into records or packets. A comparison was done between these two approaches for packet level analysis.

\subsection{Evaluation Methodology}
\label{ssc:eval_evaluation_methodology}

For this evaluation, we adopted a methodology based on the systematic approach to performance evaluation defined by \cite{Jain1991}, which consists of the definition of the goal, metrics, factors and levels for a performance study. 

The goal of this evaluation is to characterize the behaviour of MapReduce phases, its scalability through node addition and the speed-up achieved with MapReduce for packet level analysis and DPI, to understand the impact of each factor on MapReduce performance for this kind of input data, in order to be able to configure the MapReduce and obtain an optimal performance over the evaluated factors.

\begin{table}[htb]
	\centering
	\caption{Metrics for evaluating MapReduce for DPI and packet level analysis}
	\label{tab:Metrics2}
    \begin{tabular}{|l|p{11cm}|} \hline
        \textbf{Metrics}	&\textbf{Description}\\ 																		\hline
        Completion Time   	&Completion time of MapReduce jobs\\ 															\hline
        Phases Time			&Time consumed by each MapReduce phase in total completion time of MapReduce jobs\\ 			\hline
        Phases Occupation	&Relative time consumed by each MapReduce phase in total completion time of MapReduce jobs\\ 	\hline
        Scalability			&Processing capacity increasing obtained with node addition in a MapReduce cluster\\			\hline
        Speed-up 			&Improvement in completion time against the same algorithm implemented without distributed processing\\ 									\hline
    \end{tabular}
\end{table}

Table \ref{tab:Metrics2} describes the metrics evaluated, that is the completion time of MapReduce jobs, the relative and absolute time of each MapReduce phases from the total job time, the processing capacity scalability, and the speed-up against non-distributed processing. 

The experiments adopt the factors and level described in Table \ref{tab:FactorsLevels2}. The selected factors were chosen due to its importance for MapReduce performance evaluations and its adoption by relevant previous researches \citep{Jiang2010, Yanpei2011, Shafer2010, Guanying2009}.

\begin{table}[htb]
	\centering
	\caption{Factors and Levels}
	\label{tab:FactorsLevels2}	
	\begin{tabular}{|l|l|} 								\hline
	    \textbf{Factors}		&\textbf{Levels}		\\ \hline
   		Number of Worker Nodes	&2 up to 29				\\ \hline
	    Block Size				&32MB, 64MB and 128MB	\\ \hline
   		Input Size				&90Gb and 30Gb			\\ \hline
	\end{tabular}
\end{table}

Hadoop logs are a valuable information source about its environment and execution jobs; important MapReduce indicators and information about jobs, tasks, attempts, failures and topology are logged by Hadoop during its execution. The collected data to perform this performance evaluation was extracted from Hadoop logs.

To extract information from Hadoop logs and to evaluate the selected metricis, we developed the \textit{Hadoop-Analyzer} \citep{Hadoopanalyzer}, an open source and publicly available tool to extract and evaluate MapReduce indicators, such as job completion time and MapReduce phases distribution, from logs generated by Hadoop from its job executions. With \textit{Hadoop-Analyzer} is possible to generate graphs with the extracted indicators and thereby evaluate the desired metrics. 

\textit{Hadoop-Analyzer} relies on \cite{Rumen} to extract raw data from Hadoop logs and generate structured information, which is processed and shown in graphs generated through \textit{R} \citep{Eddelbuettel2012} and \textit{Gnuplot} \citep{Janert2010}, such as the results presented in Section \ref{sc:eval_results}

\subsection{Experiment Setup}
\label{ssc:eval_experiment_setup}

Network traffic traces of distributed applications were captured to be used as input for MapReduce jobs of our experiments; these traces were divided into files with size defined by the block size adopted in each experiment, and then the files were stored into HDFS, following the process described in the previous chapter. The packets were captured using Tcpdump and were split into files with sizes of 32MB, 64MB and 128MB. 

For packet level analysis and DPI evaluation, two sizes of datasets were captured from network traffic transferred between some nodes of distributed systems. One dataset was 30Gb of network traffic, with data divided in 30 files of 128MB, 60 files of 64MB and 120 files of 32MB. The other dataset was 90Gb of network traffic, split in 90 files of 128MB, 180 files of 64MB and 360 files of 32MB. 

For the experiments of DPI through MapReduce, we used network traffic captured from the same JXTA-based application described in Section \ref{ssc:prof_setup}, but with different sizes of traces and files. To evaluate MapReduce for packet level analysis, we processed network traffic captured from data transferred between 5 clients and one server of a data storage service provided through the Internet, known as Dropbox\footnote{http://www.dropbox.com/}.

To evaluate MapReduce for packet level analysis and DPI, one driver was developed for each case of network traffic analysis, with one version using MapReduce and another without it. 

\textit{CountUpDriver} implements packet level analysis for a port counter of network traces, which records how many times a port appears in TCP or UDP packets; its implementation is based on processing a whole block as input for Map functions, without splitting and with block size defined by the block size of HDFS. Furthermore a port counter implemented with \textit{P3} was evaluated; this implementation is a version of the tool presented by \cite{Lee2011}, which adopts an approach that divides a block into packets and processes each packet individually, without dependent information between packets.

\textit{JxtaSocketPerfDriver} implements DPI to extract, from a JXTA \citep{Duigou2003} network traffic, the round-trip time of JXTA messages, the number of connection requisitions per time and the number of JXTA Socket messages from JXTA clients and a JXTA Socket server. \textit{JxtaSocketPerfDriver} uses whole files as input for each Map function, with size defined by the HDFS block size, in order to reassemble JXTA messages with its content divided into many TCP packets. 

One TCP packet can transport one or more JXTA messages per time, which makes it necessary to evaluate the full content of TCP segments to recognize all possible JXTA messages, instead of to evaluate only a message header or signature. The round-trip time of JXTA messages is calculated from the time between a client peer sending a JXTA message and receiving the JXTA message arrival confirmation. To evaluate the round-trip time it is necessary to keep the information of requests and which responses correspond to each request, and thus, it is necessary to analyse several packages to retrieve and evaluate information of the application behaviour and its states.

To analyse the speed-up provided by MapReduce against a single machine execution, two drivers were developed that use the same dataset and implement the same algorithms implemented by \textit{CountUpDriver} and \textit{JxtaSocketPerfDriver}, but without distributed processing. These drivers are respectively \textit{CountUpMono} and \textit{JxtaSocketPerfMono}. 

The source code of all implemented drivers and other implementations to support the use of MapReduce for network traffic analysis, are open source and publicly available at \cite{Hadoopdpi}.

The experiments were performed on a 30-node Hadoop-1.0.3 cluster composed of nodes with four 3.2GHz cores, 8 GB RAM and 260GB of available hard disk space, running Linux kernel 3.2.0-29. Hadoop was used as our MapReduce implementation, and configured to permit a maximum of 4 Map and 1 Reduce task per node; also, we defined the value \textit{-Xmx1500m} as child option of JVM and 400 as $io.Sort.mb$ value. 

For drivers \textit{CountUpDriver} and \textit{JxtaSocketPerfDriver}, the number of Reducers was defined as a function of the number of slots of Reducers per node, defined by $numReducers = (0.95)(numNodes)(maxReducersPerNode)$ \citep{Kavulya2010}. The driver implemented with \textit{P3} \citep{Lee2011} adopts a fixed number of Reducers, defined as 10 by the available version of \textit{P3}. Each experiment was executed 20 times to obtain reliable values \citep{Yanpei2011}, within confidence interval of 95\% and a maximum error ratio of 5\%.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results}
\label{sc:eval_results}

Two dataset sizes of network traffic were used during the experiments, with 30Gb and 90Gb. Each dataset was processed by MapReduce jobs that implement packet level analysis and DPI, in Hadoop clusters with variation in number of worker nodes between 2 and 29, and block size of 32MB, 64MB and 128MB. 

Each dataset was processed by algorithms implemented through MapReduce and without distributed processing, to evaluate the speed-up achieved. The Table \ref{tab:monoExecution} shows the execution times obtained by the non-distributed processing, implemented and executed through \textit{JxtaSocketPerfMono} and \textit{CountUpMono}, using a single machine, with the resource configuration described in Subsection \ref{ssc:eval_experiment_setup}.

\begin{table}[!htbp]
  \centering
  \caption{Non-Distributed Execution Time in seconds}
  \label{tab:monoExecution}
  \begin{tabular}{*5c}
	\toprule
	\textbf{Block} &  \multicolumn{2}{c}{\textbf{JxtaSocketPerfMono}} & \multicolumn{2}{c}{\textbf{CountUpMono}} \\
	\midrule
    {}		& \textbf{90Gb}	& \textbf{30Gb}	& \textbf{90Gb}	& \textbf{30Gb}\\
   	\midrule
    32MB	& 1.745,35		& 584,92		& 872,40		& 86,71\\
    64MB	& 1.755,40		& 587,02		& 571,33 		& 91,76\\
    128MB	& 1.765,50		& 606,50		& 745,25		& 94,82\\
    \bottomrule
  \end{tabular}
\end{table}

Figure \ref{fig:JxtaMaxSpeedupBarLines} shows the completion time and speed-up of the DPI Algorithm \ref{alg:JXTAperfmapper} to extract indicators from a JXTA-based distributed application. The \textit{Completion Time} represents the job time of \textit{JxtaSocketPerfDriver} and the \textit{Speed-up} represents gains in execution time of \textit{JxtaSocketPerfDriver} against \textit{JxtaSocketPerfMono} to process 90Gb of network traffic. 

\begin{figure}[htb]
	\centering
	\includegraphics[width=0.6\textwidth]{images/JxtaSocketPerfDriverMaxJobSpeedupBarLines.eps}
	\caption{DPI Completion Time and Speed-up of MapReduce for 90Gb of a JXTA-application network traffic}
	\label{fig:JxtaMaxSpeedupBarLines}
\end{figure}

According to Figure \ref{fig:JxtaMaxSpeedupBarLines}, \textit{JxtaSocketPerfDriver} performs better than \textit{JxtaSocketPerfMono} over all factors variation. Initially we observed the best speed-up of 3.70 times with 2 nodes and block of 128MB, lastly we observed a maximum speed-up of 16.19 times with 29 nodes and block of 64MB. The speed-up achieved with block size of 32MB was the worst case initially, but its speed-up increased with node addition and became better than blocks with 128MB and near of the speed-up achieved with block of 64MB, for a cluster with 29 nodes. 

The completion time scalability behaviour of 32MB block size showed reduction in completion time for all node additions, although cases with block size of 64MB and 128MB present no significant reduction in completion time in clusters with more than 25 nodes. According to Figure \ref{fig:JxtaMaxSpeedupBarLines}, the completion time does not reduce linearly with node addition, and the improvement in completion time was less significant when the dataset was processed by more than 14 nodes, specially for cases that adopted blocks of 64MB and 128MB.

Figure \ref{fig:JxtaMaxProcessing} shows the processing capacity of MapReduce applied to DPI of 90Gb of a JXTA-based application traffic, over variation of cluster size and block size. The processing capacity was evaluated by the throughput of network traffic processed, and by the relative throughput, defined by the processing capacity achieved per number of allocated nodes. 

\begin{figure}[htb]
	\centering
	\includegraphics[width=0.6\textwidth]{images/JxtaSocketPerfDriverMaxJobProcessingLines.eps}
	\caption{DPI Processing Capacity for 90Gb}
	\label{fig:JxtaMaxProcessing}
\end{figure}

The processing capacity achieved for DPI of 90Gb using block size of 64MB was 159.89 Mbps with 2 worker nodes, increasing up to 869.43 Mbps with 29 worker nodes. For the same case, the relative processing capacity achieved was 79.94 Mbps/node with 2 nodes and 29.98 Mbps/node with 29 nodes, showing a decrease of relative processing capacity with the growth of the MapReduce cluster size. 

Although the processing capacity increased, the relative processing capacity, defined by the processing capacity per allocated node, decreased with all node addition. This behaviour indicates that MapReduce presents a reduction of efficiency with the increase of cluster size \citep{Gunther2006}, which highlights the importance of evaluation about the cost of node allocation and its benefits for completion time and processing capacity.

Figures \ref{fig:JxtaMaxSpeedupBarLines} and \ref{fig:JxtaMaxProcessing} also show the difference of performance achieved with different blocks sizes, and its relation to the cluster size. It was observed that blocks of 128MB achieved a higher throughput in cluster sizes up to 14 nodes, and that blocks of 64MB performed better in clusters bigger than 14 worker nodes.

Figures \ref{fig:JxtaMaxPhases} and \ref{fig:JxtaMaxPhasesPercent} show the behaviour of MapReduce phases to DPI of 90Gb.

\begin{figure}[!htb]
	\centering
	\subfigure[Phases Time for DPI]{
		\includegraphics[width=0.6\textwidth]{images/JxtaSocketPerfDriverMaxPhasesEvaluation.eps}
		\label{fig:JxtaMaxPhases}
	}
	\subfigure[Phases Distribution for DPI]{
		\includegraphics[width=0.6\textwidth]{images/JxtaSocketPerfDriverMaxPhasesPercentEvaluation.eps}
		\label{fig:JxtaMaxPhasesPercent}
	}
	\caption[MapReduce Phases Behaviour for DPI of 90Gb]{MapReduce Phases Behaviour for DPI of 90Gb}
	\label{fig:JxtaMaxPhasesBehaviour}
\end{figure}

MapReduce execution can be divided into Map, Shuffle, Sort and Reduce phases, although Shuffle tasks can be executed before the conclusion of all Map tasks, thereby Map and Shuffle tasks can overlap. According to the Hadoop default configuration, the overlapping between Map and Shuffle tasks starts after 5\% of Map tasks are concluded; and then Shuffle tasks are started and run until the Map phase ends. 

In Figures \ref{fig:JxtaMaxPhases} and \ref{fig:JxtaMaxPhasesPercent} we showed the overlapping between Map and Shuffle tasks as a specific MapReduce phase, represented as "Map and Shuffle" phase. The time consumed by Setup and Cleanup tasks was considered too, for a better visualization of the execution time division in Hadoop jobs.

Figure \ref{fig:JxtaMaxPhases} shows the cumulative time of each MapReduce phase in total job time. For DPI, Map time, which is the Map and the "Map and Shuffle" phases, consumes the major part of a job execution time and it is the phase that exhibits more variation with the number of nodes variation, but no significant time reduction is achieved with more than 21 nodes and block size of 64MB or 128MB. 

The Shuffle time, which happens after all Map tasks are completed, presented low variation with node addition. Sort and Reduce phases required relatively low execution times and did not appear in some bars of the graph. Setup and Cleanup tasks consumed an almost constant time, independently of cluster size or block size variation.

Figure \ref{fig:JxtaMaxPhasesPercent} shows the percentage of each MapReduce phase in total job completion time. We also considered an additional phase, called \textit{others}, which represents the time consumed by cluster management tasks, like scheduling and tasks assignment. The behaviour followed by phases occupation is similar over all block sizes evaluated, with the exception of the case where Map time does not decrease with node addition, in clusters using block size of 128MB and with more than 21 nodes. 

With cluster size variation, a relative reduction in Map time was observed and a relative increase in the time of Shuffle, Setup and Cleanup phases. During the evaluation of Figure \ref{fig:JxtaMaxPhases}, it was observed that Setup and Cleanup phases consume an almost absolute constant time, independently of cluster size and block size, and thereby with node addition and completion time decreasing, the time consumed by Setup and Cleanup tasks became more significant in relation to the total execution time, due to the taotal job completion time decreasing and the time of Setup and Cleanup tasks remaining almost the same; therefore, the Setup and Cleanup percentage time increased and became more significant over the total job completion time reduction, by node addition into the MapReduce cluster.

According to Figures \ref{fig:JxtaMaxPhases} and \ref{fig:JxtaMaxPhasesPercent}, Map phase is predominant in MapReduce jobs for DPI, and the reduction of the total job completion time over node addition is related to the decreasing of Map phase time. Thus, improvements in Map phase execution for DPI workloads can produce more significant gains, in order to reduce the total job completion time for DPI.

Figure \ref{fig:CountUpDriverMaxJobTimeComp} shows the comparison between completion time of \textit{CountUpDriver} and \textit{P3} to packet level analysis of 90Gb of network traffic, over variation of cluster size and block size. 

\begin{figure}[htb]
	\centering
	\includegraphics[width=0.7\textwidth]{images/CountUpDriverMaxJobTimeBarComp.eps}
	\caption{Completion time comparison of MapReduce for packet level analysis, evaluating the approach with and without splitting into packets}
	\label{fig:CountUpDriverMaxJobTimeComp}
\end{figure}

\textit{P3} achieves better completion time than \textit{CountUpDriver} over all factors, showing that a divisible files approach performs better for packet level analysis and that block size is a significant factor for both approaches, due to significant impact on completion time caused by adoption of blocks with different sizes. 

With variation in the number of nodes, it was observed that using a block size of 128MB was achieved better completion time up to 10 nodes, but that no more improvement in completion time was achieved with node addition on clusters with more than 10 nodes. Blocks of 32MB and 64MB only present significant completion time difference in a cluster up to 14 nodes; for a cluster bigger than 14 nodes a similar completion time was achieved for both block sizes adopted, but these are still better completion times than the completion time achieved with blocks of 128MB.

Figures \ref{fig:P3MaxSpeedupBarLines} and \ref{fig:CountUpDriverMaxSpeedupBarLines} show respectively the completion time and speed-up of \textit{P3} and \textit{CountUpDriver} against \textit{CountUpMono}, for packet level analysis, with variation on the number of nodes and block size. For both cases, the use of a block size of 128MB provides the best completion time in smaller clusters, up to 10 nodes, but it provides a worse completion time in a cluster with more than 21 nodes. For both evaluations, the speed-up adopting block of 128MB scales up to 10 nodes, but for a bigger cluster a speed-up gain was not achieved with node addition. 

\begin{figure}[!htb]
	\centering
	\subfigure[P3 evaluation]{
		\includegraphics[width=0.6\textwidth]{images/P3MaxJobSpeedupBarLines.eps}
		\label{fig:P3MaxSpeedupBarLines}
	}
	\subfigure[CountUpDriver evaluation]{
		\includegraphics[width=0.6\textwidth]{images/CountUpDriverMaxJobSpeedupBarLines.eps}
		\label{fig:CountUpDriverMaxSpeedupBarLines}
	}
	\caption[CountUp completion time and speed-up of 90Gb]{CountUp completion time and speed-up of 90Gb}
	\label{fig:CountUpMaxSpeedup}
\end{figure}

Using blocks of 32MB was achieved improvement in completion time for all node addition, which causes improvement of speed-up for all cluster size, although the use of this block size did not present better completion time than others block sizes in any case. 

The adoption of 32MB blocks provided better speed-up than other block sizes in a cluster with more than 14 nodes, due to the time consumed by \textit{CountUpMono} for processing of 90Gb divided into 32MB files which was bigger than the time consumed by cases with other block sizes, as shown in Table \ref{tab:monoExecution}.

Figures \ref{fig:P3MaxThroughput} and \ref{fig:CountUpDriverThroughput} show the processing capacity of \textit{P3} and \textit{CountUpDriver} to perform packet level analysis of 90Gb of network traffic, over variation of cluster size and block size. 

\begin{figure}[!htb]
	\centering
	\subfigure[P3 processing capacity]{
		\includegraphics[width=0.6\textwidth]{images/P3MaxJobProcessingLines.eps}
		\label{fig:P3MaxThroughput}
	}
	\subfigure[CountUpDriver processing capacity]{
		\includegraphics[width=0.6\textwidth]{images/CountUpDriverMaxJobProcessingLines.eps}
		\label{fig:CountUpDriverThroughput}
	}
	\caption[CountUp processing capacity for 90Gb]{CountUp processing capacity for 90Gb}
	\label{fig:CountUpThroughput}
\end{figure}

Using block size of 64MB, \textit{P3} achieved throughput of 413.16 Mbps with 2 nodes and the maximum of 1606.13 Mbps with 28 nodes, while its relative throughput for the same configuration was 206.58 Mbps and 55.38 Mbps. The processing capacity for packet level analysis, evaluated for \textit{P3} and \textit{CountUpDriver}, follows the same behaviour showed in Figure \ref{fig:JxtaMaxProcessing}. Additionally, it is possible to observe a convergent decreasing of relative processing capacity for all block sizes evaluated, starting in a cluster size of 14 nodes, where the relative throughput achieved by all block sizes is quite similar.

Figure \ref{fig:CountUpDriverThroughput} shows a relative processing capacity increasing with the addition of 2 nodes into a cluster with 4 nodes. For packet level analysis of 90Gb, MapReduce achieved a better processing capacity efficiency per node using 6 nodes, which provides 24 Mappers and 5 Reducers per Map and Reduce waves. With the adopted variation in number of Reducers, according to cluster size, using 5 Reducers was achieved better processing efficiency and significant reduction on Reduce time, as shown in Figure \ref{fig:CountUpDriverMaxPhases}.

Figures \ref{fig:P3MaxPhases} and \ref{fig:CountUpDriverMaxPhases} show the cumulative time per phase during a job execution. 

\begin{figure}[!htb]
	\centering
	\subfigure[MapReduce Phases Times of P3]{
		\includegraphics[width=0.6\textwidth]{images/P3MaxPhasesEvaluation.eps}
		\label{fig:P3MaxPhases}
	}
	\subfigure[MapReduce Phases Times for CountUpDriver]{
		\includegraphics[width=0.6\textwidth]{images/CountUpDriverMaxPhasesEvaluation.eps}
		\label{fig:CountUpDriverMaxPhases}
	}
	\caption[MapReduce Phases time of CountUp for 90Gb]{MapReduce Phases time of CountUp for 90Gb}
	\label{fig:CountUpThroughput}
\end{figure}

The behaviour of MapReduce phases for packet level analysis  is similar to the behaviour observed for DPI; with Map time being predominant, Map and Shuffle time do not decreasing with node addition in a cluster bigger than a specific size, and Sort and Reduce phases consuming low execution time. The exception is that Shuffle phase consumes more time in packet level analysis jobs than in DPI, specially in smaller clusters. 

For packet level analysis, the amount of intermediate data generated by Map functions is bigger than the amount generated through the use of MapReduce for DPI; packet level analysis generates an intermediate data for each packet evaluated, but for DPI it is necessary to evaluate more than one packet to generate intermediate data. Shuffle phase is responsible for sorting and transferring the Map outputs to the Reducers as inputs; then the amount of intermediate data generated by Map tasks and the network transfer cost, will impact on the Shuffle phase time.

Figures \ref{fig:P3MaxPhasesPercent} and \ref{fig:CountUpDriverMaxPhasesPercent} show the percentage of each phase on job completion time of \textit{P3} and \textit{CountUpDriver}, respectively.

\begin{figure}[!htb]
	\centering
	\subfigure[Phases Distribution for P3]{
		\includegraphics[width=0.6\textwidth]{images/P3MaxPhasesPercentEvaluation.eps}
		\label{fig:P3MaxPhasesPercent}
	}
	\subfigure[Phases Distribution for CountUpDriver]{
		\includegraphics[width=0.6\textwidth]{images/CountUpDriverMaxPhasesPercentEvaluation.eps}
		\label{fig:CountUpDriverMaxPhasesPercent}
	}
	\caption[MapReduce Phases Distribution for CountUp of 90Gb]{MapReduce Phases Distribution for CountUp of 90Gb}
	\label{fig:CountUpThroughput}
\end{figure}

As the behaviour observed in Figure \ref{fig:JxtaMaxPhasesPercent} and followed by these cases, Map and Shuffle phases consume more relative time than all others phases, over all factors. But, for packet level analysis, Map phase occupation decreases significantly, with node addition, only when block sizes are 32MB or 64MB, and following the completion time behaviour observed in Figures \ref{fig:P3MaxSpeedupBarLines} and \ref{fig:CountUpDriverMaxSpeedupBarLines}.

For the dataset of 30Gb of network traffic the same experiments were conducted, and the results about MapReduce phases evaluation presented a behaviour quite similar to the results already presented by 90Gb experiments, for DPI and packet level analysis. 

Relevant differences were identified for speed-up, completion time and scalability evaluation, as shown by Figures \ref{fig:JxtaMinSpeedupBarLines} and \ref{fig:JxtaMinThroughput}, which exhibit the completion time and processing capacity scalability of MapReduce for DPI of 30Gb of network traffic, with variation in cluster size and block size. 

\begin{figure}[!htb]
	\centering
	\subfigure[DPI Completion Time and Speed-up of MapReduce for 30Gb of a JXTA-application network traffic]{
		\includegraphics[width=0.6\textwidth]{images/JxtaSocketPerfDriverMinJobSpeedupBarLines.eps}
		\label{fig:JxtaMinSpeedupBarLines}
	}
	\subfigure[DPI Processing Capacity of 30Gb]{
		\includegraphics[width=0.6\textwidth]{images/JxtaSocketPerfDriverMinJobProcessingLines.eps}
		\label{fig:JxtaMinThroughput}
	}
	\caption[MapReduce Phases Distribution for CountUp of 90Gb]{DPI Completion Time and Processing capacity for 90Gb}
	\label{fig:JxtaMinSpeedupThroughput}
\end{figure}

The completion time of DPI of 30Gb scales significantly up to 10 nodes; then the experiment presents no more gains with node addition using block size of 128MB, and presents a low increase of completion time in cases using blocks of 32MB and 64MB. This behaviour is the same observed for job completion time for 90Gb, as shown in Figures \ref{fig:P3MaxSpeedupBarLines} and \ref{fig:CountUpDriverMaxSpeedupBarLines}, but with significant scaling just up to 10 nodes for the dataset of 30Gb, while it was achieved scaling up to 25 nodes for 90Gb. 

Figure \ref{fig:JxtaMinSpeedupBarLines} shows that a completion time of 199.12 seconds was obtained with 2 nodes using block of 128MB, scaling up to 87.33 seconds with 10 nodes and the same block size, while it was achieved respectively 474.44 and 147.12 seconds by the same configuration for DPI of 90Gb, as shown in Figure \ref{fig:JxtaMaxSpeedupBarLines}. 

Although the case for processing of 90Gb (Figure \ref{fig:JxtaMaxSpeedupBarLines}) had processed a dataset 3 times bigger than the case of 30Gb (Figure \ref{fig:JxtaMinSpeedupBarLines}), the completion time achieved in all cases for processing of 90Gb was smaller than 3 times the completion time for processing of 30Gb. For the cases with 2 and 10 nodes using block of 128MB, it was consumed respectively 2.38 and 1.68 times more time to process a dataset 90Gb, which is a dataset 3 times bigger than the dataset of 30Gb. 

Figure \ref{fig:JxtaMinThroughput} shows the processing capacity for DPI of 30Gb. The maximum speed-up achieved for DPI of 30Gb was 7.90 times, using block of 32MB and 29 worker nodes, while it was achieved the maximum speed-up of 16.19 times for DPI of 90Gb with 28 nodes.

From these results, it is possible to conclude that the MapReduce efficiency fits better for bigger data and in some cases can be more efficient to accumulate input data to process a bigger amount of data. Therefore it is important to analyse the dataset size to be processed, and to quantify the ideal number of allocated nodes for each job, in order to avoid wasting resources.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% explicar por que MapReduce alcanca um speedup de 3.7 com dois nos. Isto ocorre por que o MapReduce ja eh programado para explorar concorrencia diponivel nos nos, alocando uma task por core, diferente da implementacao monolitica avaliada
%destacar os beneficios da avaliação das aplicacoes, verificação da distruicao de carga entre servidores, avaliar as metricas, taxa de transferência, tempo de resposta, identificar padrões nos tempos que ocorrem as requisições e respostas
\section{Discussion}
\label{sc:eval_discussion}

In this section, we discuss the measured results and evaluate their meaning, restrictions and opportunities. We also discuss possible threats to the validity of our experiment.

\subsection{Results Discussion}
\label{ssc:eval_results_discussion}

According to the processing capacity presented in our experimental results, of the evaluation of MapReduce for packet level analysis and DPI, it is possible to see that the MapReduce adoption for packet level analysis and DPI provided high processing capacity and speed-up of completion time, when compared with a solution without distributed processing, making it possible to evaluate a large amount of network traffic and extract information from distributed applications of an evaluated data center.

The block size adopted and the number of nodes allocated to data processing are important factors for obtaining an efficient job completion time and processing capacity scalability. Some benchmarks show that MapReduce performance can be improved by an optimal block size choice \citep{Jiang2010}, showing better performance with the adoption of bigger block sizes. We evaluated the impact of the block size for packet level analysis and DPI workloads; blocks with 128MB provided a better completion time for smaller clusters, but blocks with 64MB performed better in bigger clusters. Thus, in order to obtain an optimal completion time adopting bigger block size it is necessary also to evaluate the node allocation for the MapReduce job, due to the variation in block size and cluster size can cause significant impact into completion time. 

The different processing capacity achieved for processing datasets of 30Gb and 90Gb highlights the efficiency of MapReduce for dealing with bigger data processing, and that can be more efficient to accumulate input data, to process a larger amount of data. Therefore, it is important to analyse the dataset size to be processed, and to quantify the ideal number of allocated nodes for each job, in order to avoid wasting resources.

The evaluation of the dataset size and the optimal number of nodes is important to understand how to schedule MapReduce jobs and resource allocation through specific Hadoop schedulers, such as \textit{Capacity Scheduler} and \textit{Fair Scheduler} \citep{Zaharia2010}, in order to avoid resource wasting with allocation of nodes that will not produce significant gains \citep{Verma20122}. Thus, the variation of processing capacity achieved in out experiments highlights the importance of evaluation of the cost of node allocation and its benefits, and the need to evaluate the ideal size of pools in the Hadoop cluster, to obtain efficiency between the cluster size allocated to process an input size, and the resource sharing of a Hadoop cluster.

The MapReduce processing capacity does not scale proportionally to node addition; in some cases there is not significant processing capacity which increases with node addition, as shown in Figure \ref{fig:JxtaMaxSpeedupBarLines}, where jobs using block size of 64MB and 128MB in clusters with more than 14 nodes for DPI of 90Gb, present no significant completion time gain with node addition. 

The number of execution waves is a factor that must be evaluated \citep{Kavulya2010} when MapReduce scalability is analysed, due to the execution time decrease wis related to the number of execution waves necessary to process all input data. The number of execution waves is defined by the available slots for execution of Map and Reduce tasks; for example, if a MapReduce job is divided into 10 tasks in a cluster with 5 available slots, then it will be necessary to have 2 execution waves for all tasks be executed.

Figure \ref{fig:JxtaMinSpeedupBarLines} shows a case of DPI of 30Gb, using block size of 128MB, in which there was not a reduction of completion time with cluster size bigger than 10 nodes, because there was not a reduction in the number of execution waves. But in our experiments, cases with a reduction of execution waves also presented no significant reduction of completion time, as cases using block size of 128MB in clusters with 21 nodes or more, for DPI and packet level analysis, showed in Figure \ref{fig:JxtaMaxSpeedupBarLines}. Thus, node addition or tasks distribution must be evaluated for resources usage optimization and to avoid additional or unnecessary costs with machines and power consumption.

The comparison of completion time between \textit{CountUpDriver} and \textit{P3} shows that \textit{P3}, which splits the data into packets, performs better than \textit{CountUpDriver}, which processes a whole block without splitting. Processing a whole block as input,  the local node parallelism is limited to the number of slots per node, while in the divisible approach each split can be processed by an independent thread, increasing the possible parallelism. Because some cases require data without splitting, such as DPI and video processing cases \citep{Pereira2010}, improvements for this issue must be evaluated, considering better schedulers, data location and task assignment.

The behavioural evaluation of MapReduce phases showed that the Map phase is predominant in total execution time for packet level analysis and DPI, with Shuffle being the second most expressive phase. Shuffle can overlap Map phase, and this condition must be considered in MapReduce evaluations, specially in our case, due to the overlap of Map and Shuffle which represents more than 50\% of total time execution. 

The long time of "Map and Shuffle" phase represents a long time of Shuffle tasks being executed in parallel with Map tasks, and a long time of slots allocated for Shuffle tasks that only will be concluded after all Map tasks are finished, although these Shuffle tasks can be longer than the time required to read and process the generated intermediate data. If there are slots allocated for Shuffle tasks that are only waiting for Map phase conclusion, these slots could be used for other task executions, which could accelerate the job completion time.

With the increase of cluster size and reduction of job completion time, it was observed that the Map phase showed a proportional decrease, while Shuffle phase increased with the growth of the number of nodes. With more nodes, the intermediate data generated by Map tasks is placed in more nodes, which are responsible for shuffling the data and sending them to specific Reducers, increasing the amount of remote I/O from Mappers to Reducers and the number of data sources for each Reducer. Shuffle phase may represent a bottleneck \citep{Shubin2009} for scalability and could be optimized, due to I/O restrictions \citep{Lee2012, Akram2012} and data locality issues for Reduce phase \citep{Hammoud2011}.

Information extracted from the analysed results about the performance obtained with specific cluster, block and input sizes, is important for configuring MapReduce resource allocation and specialized schedulers, such as the Fair Scheduler \citep{Zaharia2008}, which defines pool sizes and resource share for MapReduce jobs. Thus, with the information of the performance achieved with specific resources, it is possible to configure MapReduce parameters in order to obtain efficiency between the resource allocation, and the expected completion time or resource sharing \citep{Zaharia2008, Zaharia2010}.

\subsection{Possible threats to validity}
\label{ssc:eval_threats}

In this chapter we evaluated for packet level analysis, a port counter implemented with \textit{P3}. It was used a version of this implementation published in \cite{Lee2011} website\footnote{https://sites.google.com/a/networks.cnu.ac.kr/yhlee/p3}, obtained on 2012 February, when a complete binary version was available, which was used in our experiments, but this binary version is currently not available. 

Part of the \textit{P3} source code was published later, but not all necessary code to compile all binary libraries necessary to evaluate the \textit{P3} implementation of a port counter. Thereby it is important to highlight that the obtained results, through our evaluation, was for the \textit{P3} version obtained on 2012 February from \cite{Lee2011} website.

It is also important to highlight that the DPI can present restrictions to evaluate encrypted messages, and that the obtained results are specifics for the input datasets, factors, levels and experiment setup used in our evaluation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Chapter Summary}
\label{sc:eval_summary}

In this chapter, we evaluated the performance of MapReduce for packet level analysis and DPI of applications traffic. We evaluated how data input, block and cluster sizes, impacts on MapReduce phases, job completion time, processing capacity scalability and on the speed-up achieved in comparison with the same algorithm executed by a non-distributed implementation.

The results show that MapReduce presents high processing capacity for dealing with massive application traffic analysis. The behaviour of MapReduce phases over variation of block size and cluster size was evaluated; we verified that packet level analysis and DPI are Map-intensive jobs, and that Map phase consumes more than 70\% of execution time, with Shuffle phase being the second predominant phase. 

We showed that input size, block size and cluster size are important factors to be considered to achieve better job completion time and to explore MapReduce scalability and efficient resource allocation, due to the variation in completion time provided by the block size adopted and, in some cases, due to the processing capacity which does not increase with node addition into the cluster. 

We also showed that using a whole block as input for Map functions, achieved a poorer performance than using divisible data, thereby more evaluation is necessary to understand how it can be handled and improved.
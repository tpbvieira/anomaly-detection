% Introdução: cenario, motivacao, problema, solucoes atuais, escopo do trabalho, lista de contribuicoes, organizacao do resto do artigo. 
% Chapter Structure
% 	Motivation
% 	Problems Statement
%	Contributions
% 	Dissertation Organization
\chapter{Introduction}
\label{ch:1_introduction}

\begin{quotation}[]{Lao Tzu}
The softest things in the world overcome the hardest things in the world.
\end{quotation}


\section{Motivation}
\label{sc:motivation}

Traditionally, cyber defense methods can be effective against ordinary and conventional types of attacks, yet may fail against innovative malicious techniques \cite{lakhina2005mining}. In order to be able to detect and avoid novel attacks and their variations, it is necessary to develop or improve techniques to achieve efficiency on resource consumption, processing capacity and response time. Moreover, it is crucial to obtain high detection accuracy and capacity to detect variations of malicious patterns. Recently, signal processing schemes have being applied to detect malicious traffic in computer networks \cite{Lu2009,Huang2009,Zonglin2009,david2011blind,da2012improved,tenorio2013greatest, vieira2017model}, showing advances in network traffic analysis.

Information security may consist of both technical and procedural aspects. The former includes equipment and security systems, while the latter corresponds to security rules and recommendations of good practices. Intrusion detection and intrusion prevention systems are security systems used, respectively, to detect (passively) and prevent (proactively) threats to computer systems and computer networks. Such systems can work in the following fashions: signature-based, anomaly-based or hybrid \cite{Huang2009,mudzingwa2012study}.

Anomaly detection can be defined as the identification of rare and suspicions events by differing the normal or majority of the data. Anomalies are also referred to as outliers, novelties, noise or deviations, and can be related to network attacks, frauds or defects \cite{bhuyan2014network,ahmed2016survey}. Additionally, anomaly detection techniques can be categorized in classification, statistical, information theory and clustering based, according to \cite{bhuyan2014network,ahmed2016survey,osanaiye2016distributed}. Anomalies can be hard to identify and separate from normal data due to the rare occurrences of anomalies in comparison to normal events, therefore anomaly detection algorithms have to be high discriminative, robust to contamination and able to deal with the imbalanced data problem \cite{he2008learning}.

Several efforts and researches aim to avoid network attacks based on known attackers, fingerprints or behavioral analysis. However, distributed attacks organized by botnet has increasing and demanding the development of counter measures in order to detect and avoid unknown attacks or even to deal with adversarial changes on behavior, location and other patterns, in order to avoid detection by behavioral or pattern based detection systems \cite{gu2008botminer, garcia2014empirical,khattak2015botflex,acarali2016survey,wang2017botnet,Wang2018ddosbotnetssurvey}. To face this problem, it is possible to adopt unsupervised or semi-supervised approaches for network anomaly detection, where it is not necessary known anomalies for training models \cite{moustafa2019holistic}.

Recent developments in science and technology have enabled the growth and availability of raw data to occur at an explosive rate. This has created an immense opportunity for knowledge discovery and data engineering research to play an essential role in a wide range of applications from daily civilian life to national security. Despite the actual high availability of information, the relevant information of some observations is generally of under much reduced dimensionality compared to available data sets. The extraction of relevant information by identifying the generating causes within classes of signals is useful for classification problems and for security analysis. 

The high availability of raw data increases the challenges related to big data analytics and to imbalanced learning problems, which corresponds to data sets exhibiting significant imbalances of classes or rare events of some classes. The fundamental issue with the imbalanced learning is the ability of imbalanced data to significantly compromise the performance of most standard learning algorithms. Data analysis of imbalanced data is challenging for classification and prediction problems related to anomaly detection, novelty detection, fraud detection and attack detection.

Some widely adopted algorithms assume a gaussian distributed data, however this assumption may not be observed in real world problems, such as the case of network traffic analysis, where network traffic features are usually more characterized by skewed and heavy-tailed distributions \cite{lakhina2005mining,benson2010network,leon2017probability}. It was observed that the skeweness and heavy-tailed distributions can impact algorithms that rely on gaussian distributed data, as well as it can reveal characteristics that can be exploited in order to obtain accurate classifiers for network anomaly detection.

\section{Problem Statement}
\label{sc:problems}

Considering the above described landscape, this thesis outlines the development and evaluation of approaches based on subspace learning for network attack detection, through methods to make the data discriminative and able to identify structures, hidden patterns and the most relevant information for anomaly detection. In the Subsection \ref{sc:Hypothesis} we present our hypothesis formulation and the Subsection \ref{sc:proposals} describes the proposals to answer the questions and validate the hypotheses.

\subsection{Hypothesis Formulation}
\label{sc:Hypothesis}

For the experimental evaluation of our proposals, we adopt a methodology based on aspects of GQM (Goal-Question-Metric) template \citep{Basili1994} and define two questions to achieve our goal, which are:

\begin{itemize}
	\item $Q_1$: Can the analysis of patterns from a learned subspace identify and detect anomalies in network traffic?
	\item $Q_2$: Can the robust subspace learning improve the anomaly detection in imbalanced and skewed data?
\end{itemize}

Our testing hypotheses are defined in Table \ref{tab:hypothesis}, that describe the null hypothesis and alternative hypothesis for each previously defined question. 

\begin{table}[htb]
	\centering
	\caption{Hypotheses to evaluate the defined metrics}
	\label{tab:hypothesis}
    \begin{tabular}{|p{6cm}|p{6cm}|c|} \hline
        \textbf{Alternative Hypothesis}	&\textbf{Null Hypothesis}	&\textbf{Question} 	\\ \hline
        $H_{1sub}$: It is possible to use subspace learning to identify and detect anomalies in network traffic.
        &$H_{0sub}$. It is not possible to use subspace learning to identify and detect anomalies in network traffic.	&$Q_1$\\ \hline
        $H_{1rob.sub}$: The robust subspace learning improves the anomaly detection from imbalanced and skewed data.	&$H_{0rob.sub}$. The robust subspace learning does not improves the anomaly detection from imbalanced and skewed data.	&$Q_2$\\ \hline
    \end{tabular}
\end{table}

The hypotheses $H_{1num.indct}$ and $H_{0num.indct}$ were defined to evaluate if a subspace learned by singular value decomposition are sensitive to outliers and can be used to detect network attacks. We defined the hypotheses $H_{1scale.prop}$ and $H_{0scale.prop}$ to evaluate the feasibility and performance of a robust subspace learned by Robust PCA to network anomaly detection. 

The metrics most adopted to evaluate experimental results of network attack detection are the true positive (TP), true negative (TN), false positive (FP) and false negative (FN). We adopt these metrics and also adopt the misclassification rate and the F1-score, which is the preferable measure for imbalanced data sets \cite{powers2011evaluation,moustafa2019holistic}.

\subsection{Proposals}
\label{sc:problem_statement}

In the context of anomaly-based schemes, this thesis proposes the Eigensim, which is a approach based on subspace learning techniques for detection of malicious traffic in computer networks. 

Inspired by \cite{david2011blind,da2012improved}, Eigensim models the network traffic using a signal processing formulation as a composition of three components: signal, artifact and noise, taking into account the incoming and outgoing traffic in certain types of network ports (TCP or UDP). The proposed technique is based on eigenvalue analysis, model order selection (MOS) and similarity analysis. In contrast to \cite{david2011blind,da2012improved,tenorio2013greatest}, MOS and eigenvalue analysis are applied to detect detailed time frames under attack. We evaluate the accuracy and performance of the proposed framework applied to a experimental scenario and to the DARPA 1998 data set \citep{osanaiye2016distributed}, which is a well known network traffic data set. Furthermore, this proposed approach is validated into a prove of concept regarding behavioral anomaly detection to detect possible threats to a offline corporate mobile app. 

Network anomaly detection problems are usually characterized by skewed and imbalanced data \cite{Phua2004minority,he2008learning,benson2010network}. Learning algorithms for imbalanced data has been a challenging research topic, considering that the fundamental issue with the imbalanced learning problem is the ability of imbalanced data to significantly compromise the performance of most standard learning algorithms \cite{he2008learning}. 

Some widely adopted algorithms assume a gaussian distributed data, however this assumption may not be observed in real world problems, such as the case of network traffic analysis, where network traffic features are usually more characterized by skewed and heavy-tailed distributions \cite{lakhina2005mining,benson2010network, leon2017probability}. Therefore, the skeweness and heavy-tailed distributions can impact algorithms that rely on gaussian distributed data, as well as it can be exploited to reveal characteristics that can improve classifiers for network anomaly detection. 

We believe that the skewness of anomalous and normal data can highlight features for improving anomaly detection in imbalanced data. We also believe that the distance between robust estimates of normal and contaminated data can highlight discrepancies and be used anomaly detection and for network attack detection. Therefore, we propose the m-RPCA, which is an approach based on  distances of moments computed from a robust subspace learned by RPCA, for anomaly detection on imbalanced and skewed data.

\section{Contributions}
\label{sc:contributions}

We analyze problems related to detection of information security issues and propose new approaches to improve malicious behavior detection through signal processing techniques based on subspace learning. The results of the work presented in this thesis provide the following contributions:

\begin{enumerate}
	\item We propose an approach based on eigen similarity analysis for extracting detailed information about accurate time and network ports under network attack, and evaluate the accuracy and performance of the proposed framework applied to an experimental scenario and to the DARPA 1998 data set;
	\item We discuss the computational complexity of the proposed framework and evaluate the required processing time for tested scenarios;
	\item We propose an architecture and techniques for offline behavioral analysis of a corporate mobile client security architecture, and discuss the processing time of the proposed framework for mobile devices;
	\item We propose an approach based on distances of moments computed from a robust subspace learned by RPCA, for anomaly detection on imbalanced and skewed data, and evaluate the anomaly detection rate on synthetic and real data sets;
\end{enumerate}

\section{Thesis Organization}
\label{sc:organization}

This thesis is organized as follows. In Chapter \ref{ch:2_mos_eig_sim}, we propose the Eigensim, which is an approach based on signal processing techniques for detection of malicious traffic in computer networks, based on eigenvalue analysis, model order selection (MOS) and similarity analysis. Chapter \ref{ch:3_mobile} presents a prove of concept regarding the evaluation of an approach and architecture based on user behavior analysis through the Eigensim  \cite{tenorio2013greatest}, in order to detect threats in a mobile application. Chapter \ref{ch:4_m_rpca} proposes the m-RPCA, which is an approach based on distances of moments computed from a robust subspace learned by RPCA, for anomaly detection on imbalanced and skewed data. Chapter \ref{ch:5_conclusionfuturework} draws the conclusions and the suggestions for future work. Furthermore, the Appendix \ref{apx:a_mos} presents mathematical concepts of examples of state-of-the-art MOS schemes and the Appendix \ref{apx:b_csf_fs} presents a critical factors analysis based on Principal Component Analysis (PCA) for visual discriminant analysis, and presents an approach based on Recursive Feature Elimination (RFE) combined with Support Vector Machine (SVM) \cite{hearst1998support}, applied to the survey that evaluates the IT governance of Brazilian public organizations, in order to identify the Critical Success Factors (CSF) for IT governance of the public sector according to TCU.
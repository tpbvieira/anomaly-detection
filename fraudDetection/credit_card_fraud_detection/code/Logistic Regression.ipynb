{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a239913a-b27f-15e7-8911-53b62bf41c4e"
   },
   "source": [
    "# Credit card fraud detection\n",
    "\n",
    "#### This notebook will test different methods on skewed data. The idea is to compare if preprocessing techniques work better when there is an overwhelming majority class that can disrupt the efficiency of our predictive model.\n",
    "\n",
    "#### You will also be able to see how to apply cross validation for hyperparameter tuning on different classification models. My intention is to create models using:\n",
    "1. Logistic Regression\n",
    "2. SVMs\n",
    "3. Decision trees\n",
    "4. I also want to have a try at anomaly detection techniques, but I still have to investigate a bit on that, so any advise will be appreciated!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "029ecde6-086d-7a8e-de44-363a7a23dbd8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import KFold, cross_val_score\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, classification_report\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b4de5f93-d467-ad7d-4597-03d5f3e89f86"
   },
   "source": [
    "# Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "7e5ca1e3-3597-19d2-b4be-dffd335df630"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...         V21       V22       V23       V24  \\\n",
       "0  0.098698  0.363787  ...   -0.018307  0.277838 -0.110474  0.066928   \n",
       "1  0.085102 -0.255425  ...   -0.225775 -0.638672  0.101288 -0.339846   \n",
       "2  0.247676 -1.514654  ...    0.247998  0.771679  0.909412 -0.689281   \n",
       "3  0.377436 -1.387024  ...   -0.108300  0.005274 -0.190321 -1.175575   \n",
       "4 -0.270533  0.817739  ...   -0.009431  0.798278 -0.137458  0.141267   \n",
       "\n",
       "        V25       V26       V27       V28  Amount  Class  \n",
       "0  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.167170  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"../input/creditcard.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6268bbd8-6de5-2389-5693-ecd9a14872d4"
   },
   "source": [
    "# Checking the target classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "3f6e6674-12e9-6983-5788-5755f80c7ec2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7fadc5cbefd0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fae0038ec88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "count_classes = pd.value_counts(data['Class'], sort = True).sort_index()\n",
    "count_classes.plot(kind = 'bar')\n",
    "plt.title(\"Fraud class histogram\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "801dd843-a90b-bb5f-97e7-2da84cc6cd6a"
   },
   "source": [
    "### Clearly the data is totally unbalanced!! \n",
    "\n",
    "#### This is a clear example where using a typical accuracy score to evaluate our classification algorithm. For example, if we just used a majority class to assign values to all records, we will still be having a high accuracy, BUT WE WOULD BE CLASSIFYING ALL \"1\" INCORRECTLY!!\n",
    "\n",
    "#### There are several ways to approach this classification problem taking into consideration this unbalance. \n",
    "\n",
    "- Collect more data? Nice strategy but not applicable in this case\n",
    "- Changing the performance metric:\n",
    "    - Use the confusio nmatrix to calculate Precision, Recall\n",
    "    - F1score (weighted average of precision recall)\n",
    "    - Use Kappa - which is a classification accuracy normalized by the imbalance of the classes in the data\n",
    "    - ROC curves - calculates sensitivity/specificity ratio.\n",
    "- Resampling the dataset\n",
    "    - Essentially this is a method that will process the data to have an approximate 50-50 ratio.\n",
    "    - One way to achieve this is by OVER-sampling, which is adding copies of the under-represented class (better when you have little data)\n",
    "    - Another is UNDER-sampling, which deletes instances from the over-represented class (better when he have lot's of data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "555427ea-a862-c64f-9d88-ddbb4f0366a3"
   },
   "source": [
    "# Approach\n",
    "\n",
    "1. We are not going to perform feature engineering in first instance. The dataset has been downgraded in order to contain 30 features (28 anonamised + time + amount). \n",
    "2. We will then compare what happens when using resampling and when not using it. We will test this approach using a simple logistic regression classifier.\n",
    "3. We will evaluate the models by using some of the performance metrics mentioned above.\n",
    "4. We will repeat the best resampling/not resampling method, by tuning the parameters in the logistic regression classifier.\n",
    "5. We will finally perform classifications model using other classification algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fb559c9d-f4fe-5c63-7bdb-14cd3529f660"
   },
   "source": [
    "# Setting our input and target variables + resampling.\n",
    "\n",
    "#### 1. Normalising the amount column. The amount column is not in line with the anonimised features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "3fd30a6f-c0ad-5ece-943c-651cdf14d0d6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel_launcher.py:1: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "      <th>normAmount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>0</td>\n",
       "      <td>0.244964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.342475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>0</td>\n",
       "      <td>1.160686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>0</td>\n",
       "      <td>0.140534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.073403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9       V10     ...           V21       V22       V23  \\\n",
       "0  0.098698  0.363787  0.090794     ...     -0.018307  0.277838 -0.110474   \n",
       "1  0.085102 -0.255425 -0.166974     ...     -0.225775 -0.638672  0.101288   \n",
       "2  0.247676 -1.514654  0.207643     ...      0.247998  0.771679  0.909412   \n",
       "3  0.377436 -1.387024 -0.054952     ...     -0.108300  0.005274 -0.190321   \n",
       "4 -0.270533  0.817739  0.753074     ...     -0.009431  0.798278 -0.137458   \n",
       "\n",
       "        V24       V25       V26       V27       V28  Class  normAmount  \n",
       "0  0.066928  0.128539 -0.189115  0.133558 -0.021053      0    0.244964  \n",
       "1 -0.339846  0.167170  0.125895 -0.008983  0.014724      0   -0.342475  \n",
       "2 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752      0    1.160686  \n",
       "3 -1.175575  0.647376 -0.221929  0.062723  0.061458      0    0.140534  \n",
       "4  0.141267 -0.206010  0.502292  0.219422  0.215153      0   -0.073403  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['normAmount'] = StandardScaler().fit_transform(data['Amount'].reshape(-1, 1))\n",
    "data = data.drop(['Time','Amount'],axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1e02844a-34ff-34ff-51f8-ef074ee2f1d5"
   },
   "source": [
    "#### 2. Assigning X and Y. No resampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cfffc4c5-b621-250f-3b6b-6118cef52b9d"
   },
   "source": [
    "#### 3. Resampling.\n",
    "\n",
    "- As we mentioned earlier, there are several ways to resample skewed data. Apart from under and over sampling, there is a very popular approach called SMOTE (Synthetic Minority Over-Sampling Technique), which is a combination of oversampling and undersampling, but the oversampling approach is not by replicating minority class but constructing new minority class data instance via an algorithm.\n",
    "\n",
    "- In this notebook, we will use traditional UNDER-sampling. I will probably try to implement SMOTE in future versions of the code, but for now I will use traditional undersamplig.\n",
    "\n",
    "- The way we will under sample the dataset will be by creating a 50/50 ratio. This will be done by randomly selecting \"x\" amount of sample from the majority class, being \"x\" the total number of records with the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "c1d874fa-5ea5-edbb-726c-ae98c84e6120"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "X = data.ix[:, data.columns != 'Class']\n",
    "y = data.ix[:, data.columns == 'Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "2af7c203-44ed-66b6-6141-ac0d0637fcc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of normal transactions:  0.5\n",
      "Percentage of fraud transactions:  0.5\n",
      "Total number of transactions in resampled data:  984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel_launcher.py:18: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n"
     ]
    }
   ],
   "source": [
    "# Number of data points in the minority class\n",
    "number_records_fraud = len(data[data.Class == 1])\n",
    "fraud_indices = np.array(data[data.Class == 1].index)\n",
    "\n",
    "# Picking the indices of the normal classes\n",
    "normal_indices = data[data.Class == 0].index\n",
    "\n",
    "# Out of the indices we picked, randomly select \"x\" number (number_records_fraud)\n",
    "random_normal_indices = np.random.choice(normal_indices, number_records_fraud, replace = False)\n",
    "random_normal_indices = np.array(random_normal_indices)\n",
    "\n",
    "# Appending the 2 indices\n",
    "under_sample_indices = np.concatenate([fraud_indices,random_normal_indices])\n",
    "\n",
    "# Under sample dataset\n",
    "under_sample_data = data.iloc[under_sample_indices,:]\n",
    "\n",
    "X_undersample = under_sample_data.ix[:, under_sample_data.columns != 'Class']\n",
    "y_undersample = under_sample_data.ix[:, under_sample_data.columns == 'Class']\n",
    "\n",
    "# Showing ratio\n",
    "print(\"Percentage of normal transactions: \", len(under_sample_data[under_sample_data.Class == 0])/len(under_sample_data))\n",
    "print(\"Percentage of fraud transactions: \", len(under_sample_data[under_sample_data.Class == 1])/len(under_sample_data))\n",
    "print(\"Total number of transactions in resampled data: \", len(under_sample_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6b74ba73-82a8-3585-b790-44fe486ff19d"
   },
   "source": [
    "# Splitting data into train and test set. Cross validation will be used when calculating accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "4a725b16-c14a-2be8-8240-617b7b2ed8cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number transactions train dataset:  199364\n",
      "Number transactions test dataset:  85443\n",
      "Total number of transactions:  284807\n",
      "\n",
      "Number transactions train dataset:  688\n",
      "Number transactions test dataset:  296\n",
      "Total number of transactions:  984\n"
     ]
    }
   ],
   "source": [
    "# Whole dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state = 0)\n",
    "print(\"Number transactions train dataset: \", len(X_train))\n",
    "print(\"Number transactions test dataset: \", len(X_test))\n",
    "print(\"Total number of transactions: \", len(X_train)+len(X_test))\n",
    "\n",
    "# Undersampled dataset\n",
    "X_train_undersample, X_test_undersample, y_train_undersample, y_test_undersample = train_test_split(X_undersample\n",
    "                                                                                                   ,y_undersample\n",
    "                                                                                                   ,test_size = 0.3\n",
    "                                                                                                   ,random_state = 0)\n",
    "print(\"\")\n",
    "print(\"Number transactions train dataset: \", len(X_train_undersample))\n",
    "print(\"Number transactions test dataset: \", len(X_test_undersample))\n",
    "print(\"Total number of transactions: \", len(X_train_undersample)+len(X_test_undersample))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6927cc69-57e6-4f34-4680-0b0016d414a0"
   },
   "source": [
    "# Logistic regression classifier - Undersampled data\n",
    "\n",
    "#### We are very interested in the recall score, because that is the metric that will help us try to capture the most fraudulent transactions. If you think how Accuracy, Precision and Recall work for a confusion matrix, recall would be the most interesting:\n",
    "\n",
    "- Accuracy = (TP+TN)/total\n",
    "- Precision = TP/(TP+FP)\n",
    "- Recall = TP/(TP+FN)\n",
    "\n",
    "#### As we know, due to the imbalacing of the data, many observations could be predicted as False Negatives, being, that we predict a normal transaction, but it is in fact a fraudulent one. Recall captures this.\n",
    "- Obviously, trying to increase recall, tends to come with a decrease of precision. However, in our case, if we predict that a transaction is fraudulent and turns out not to be, is not a massive problem compared to the opposite. \n",
    "- We could even apply a cost function when having FN and FP with different weights for each type of error, but let's leave that aside for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "88765ef8-cb56-860a-d249-9691d90afcb2"
   },
   "source": [
    "#### Very ad-hoc function to print K_fold_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "069bc837-cfd1-006e-c589-7085d5d29a8e"
   },
   "outputs": [],
   "source": [
    "def print_classification_report(y_test, y_predic):\n",
    "    print('Classification report:')\n",
    "    print(classification_report(y_test, y_predic))\n",
    "\n",
    "    print('\\nBinary F1 Score, Recall and Precision:')\n",
    "    f = f1_score(y_test, y_predic, average = \"binary\")\n",
    "    Recall = recall_score(y_test, y_predic, average = \"binary\")\n",
    "    Precision = precision_score(y_test, y_predic, average = \"binary\")\n",
    "    print('\\tF1 Score %f' %f)\n",
    "    print('\\tRecall Score %f' %Recall)\n",
    "    print('\\tPrecision Score %f' %Precision)\n",
    "\n",
    "    print('\\nMicro F1 Score, Recall and Precision:')\n",
    "    f = f1_score(y_test, y_predic, average = \"micro\")\n",
    "    Recall = recall_score(y_test, y_predic, average = \"micro\")\n",
    "    Precision = precision_score(y_test, y_predic, average = \"micro\")\n",
    "    print('\\tF1 Score %f' %f)\n",
    "    print('\\tRecall Score %f' %Recall)\n",
    "    print('\\tPrecision Score %f' %Precision)\n",
    "\n",
    "def printing_Kfold_scores(x_train_data,y_train_data):\n",
    "    fold = KFold(len(y_train_data),5,shuffle=False) \n",
    "\n",
    "    # Different C parameters\n",
    "    c_param_range = [0.01,0.1,1,10,100]\n",
    "\n",
    "    results_table = pd.DataFrame(index = range(len(c_param_range),2), columns = ['C_parameter','Mean F1 score'])\n",
    "    results_table['C_parameter'] = c_param_range\n",
    "\n",
    "    # the k-fold will give 2 lists: train_indices = indices[0], test_indices = indices[1]\n",
    "    j = 0\n",
    "    for c_param in c_param_range:\n",
    "        print('-------------------------------------------')\n",
    "        print('C parameter: ', c_param)\n",
    "        print('-------------------------------------------')\n",
    "        print('')\n",
    "\n",
    "        f1_accs = []\n",
    "        for iteration, indices in enumerate(fold,start=1):\n",
    "\n",
    "            # Call the logistic regression model with a certain C parameter\n",
    "            lr = LogisticRegression(C = c_param, penalty = 'l1')\n",
    "\n",
    "            # Use the training data to fit the model. In this case, we use the portion of the fold to train the model\n",
    "            # with indices[0]. We then predict on the portion assigned as the 'test cross validation' with indices[1]\n",
    "            lr.fit(x_train_data.iloc[indices[0],:],y_train_data.iloc[indices[0],:].values.ravel())\n",
    "\n",
    "            # Predict values using the test indices in the training data\n",
    "            y_pred_undersample = lr.predict(x_train_data.iloc[indices[1],:].values)\n",
    "\n",
    "            # Calculate the F1 score and append it to a list for F1 scores representing the current c_parameter\n",
    "            f1_acc = f1_score(y_train_data.iloc[indices[1],:].values,y_pred_undersample)\n",
    "            f1_accs.append(f1_acc)\n",
    "            print('Iteration ', iteration,': F1 score = ', f1_acc)\n",
    "\n",
    "        # The mean value of those F1 scores is the metric we want to save and get hold of.\n",
    "        results_table.ix[j,'Mean F1 score'] = np.mean(f1_accs)\n",
    "        j += 1\n",
    "        print('')\n",
    "        print('Mean F1 score ', np.mean(f1_accs))\n",
    "        print('')\n",
    "\n",
    "    best_c = results_table.loc[results_table['Mean F1 score'].idxmax()]['C_parameter']\n",
    "    \n",
    "    # Finally, we can check which C parameter is the best amongst the chosen.\n",
    "    print('*********************************************************************************')\n",
    "    print('Best model to choose from cross validation is with C parameter = ', best_c)\n",
    "    print('*********************************************************************************')\n",
    "    \n",
    "    return best_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "983c1c75-8092-9a8e-40ca-754fde9e2301"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel_launcher.py:57: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "C parameter:  0.01\n",
      "-------------------------------------------\n",
      "\n",
      "Iteration  1 : F1 score =  0.8235294117647058\n",
      "Iteration  2 : F1 score =  0.8414634146341463\n",
      "Iteration  3 : F1 score =  0.7763157894736843\n",
      "Iteration  4 : F1 score =  0.8135593220338985\n",
      "Iteration  5 : F1 score =  0.8148148148148148\n",
      "\n",
      "Mean F1 score  0.8139365505442498\n",
      "\n",
      "-------------------------------------------\n",
      "C parameter:  0.1\n",
      "-------------------------------------------\n",
      "\n",
      "Iteration  1 : F1 score =  0.9037037037037038\n",
      "Iteration  2 : F1 score =  0.8936170212765958\n",
      "Iteration  3 : F1 score =  0.9482758620689654\n",
      "Iteration  4 : F1 score =  0.9583333333333333\n",
      "Iteration  5 : F1 score =  0.9291338582677166\n",
      "\n",
      "Mean F1 score  0.926612755730063\n",
      "\n",
      "-------------------------------------------\n",
      "C parameter:  1\n",
      "-------------------------------------------\n",
      "\n",
      "Iteration  1 : F1 score =  0.9185185185185185\n",
      "Iteration  2 : F1 score =  0.8767123287671232\n",
      "Iteration  3 : F1 score =  0.95\n",
      "Iteration  4 : F1 score =  0.9655172413793103\n",
      "Iteration  5 : F1 score =  0.9457364341085271\n",
      "\n",
      "Mean F1 score  0.9312969045546959\n",
      "\n",
      "-------------------------------------------\n",
      "C parameter:  10\n",
      "-------------------------------------------\n",
      "\n",
      "Iteration  1 : F1 score =  0.9264705882352942\n",
      "Iteration  2 : F1 score =  0.8767123287671232\n",
      "Iteration  3 : F1 score =  0.95\n",
      "Iteration  4 : F1 score =  0.9589041095890412\n",
      "Iteration  5 : F1 score =  0.9384615384615383\n",
      "\n",
      "Mean F1 score  0.9301097130105994\n",
      "\n",
      "-------------------------------------------\n",
      "C parameter:  100\n",
      "-------------------------------------------\n",
      "\n",
      "Iteration  1 : F1 score =  0.9264705882352942\n",
      "Iteration  2 : F1 score =  0.8724832214765101\n",
      "Iteration  3 : F1 score =  0.95\n",
      "Iteration  4 : F1 score =  0.9589041095890412\n",
      "Iteration  5 : F1 score =  0.9384615384615383\n",
      "\n",
      "Mean F1 score  0.9292638915524769\n",
      "\n",
      "*********************************************************************************\n",
      "Best model to choose from cross validation is with C parameter =  1.0\n",
      "*********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "best_c = printing_Kfold_scores(X_train_undersample,y_train_undersample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bdb1ff38-9d27-ae15-58df-9be4e4f76550"
   },
   "source": [
    "#### Create a function to plot a fancy confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "f5b049b3-4f9a-f5bb-db3d-6c48e9b1e1a5"
   },
   "outputs": [],
   "source": [
    "# def plot_confusion_matrix(cm, classes,\n",
    "#                           normalize=False,\n",
    "#                           title='Confusion matrix',\n",
    "#                           cmap=plt.cm.Blues):\n",
    "#     \"\"\"\n",
    "#     This function prints and plots the confusion matrix.\n",
    "#     Normalization can be applied by setting `normalize=True`.\n",
    "#     \"\"\"\n",
    "#     plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "#     plt.title(title)\n",
    "#     plt.colorbar()\n",
    "#     tick_marks = np.arange(len(classes))\n",
    "#     plt.xticks(tick_marks, classes, rotation=0)\n",
    "#     plt.yticks(tick_marks, classes)\n",
    "\n",
    "#     if normalize:\n",
    "#         cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "#         #print(\"Normalized confusion matrix\")\n",
    "#     else:\n",
    "#         1#print('Confusion matrix, without normalization')\n",
    "\n",
    "#     #print(cm)\n",
    "\n",
    "#     thresh = cm.max() / 2.\n",
    "#     for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "#         plt.text(j, i, cm[i, j],\n",
    "#                  horizontalalignment=\"center\",\n",
    "#                  color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.ylabel('True label')\n",
    "#     plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e40b554a-5b88-2828-f655-d54560ad7480"
   },
   "source": [
    "### Predictions on test set and plotting confusion matrix\n",
    "\n",
    "#### We have been talking about using the recall metric as our proxy of how effective our predictive model is. Even though recall is still the recall we want to calculate, just bear mind in mind that the undersampled data hasn't got a skewness towards a certain class, which doesn't make recall metric as critical. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "5c8e4c0e-8cfd-7422-04a8-1b47b8531267"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.98      0.96       149\n",
      "          1       0.98      0.93      0.95       147\n",
      "\n",
      "avg / total       0.96      0.96      0.96       296\n",
      "\n",
      "\n",
      "Binary F1 Score, Recall and Precision:\n",
      "\tF1 Score 0.954704\n",
      "\tRecall Score 0.931973\n",
      "\tPrecision Score 0.978571\n",
      "\n",
      "Micro F1 Score, Recall and Precision:\n",
      "\tF1 Score 0.956081\n",
      "\tRecall Score 0.956081\n",
      "\tPrecision Score 0.956081\n"
     ]
    }
   ],
   "source": [
    "# Use this C_parameter to build the final model with the whole training dataset and predict the classes in the test\n",
    "# dataset\n",
    "lr = LogisticRegression(C = best_c, penalty = 'l1')\n",
    "lr.fit(X_train_undersample,y_train_undersample.values.ravel())\n",
    "y_pred_undersample = lr.predict(X_test_undersample.values)\n",
    "\n",
    "# Print Classification Report for Undersampled Test Data\n",
    "print_classification_report(y_test_undersample,y_pred_undersample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3aee694e-c838-434a-0cb6-dafaa3a0b224"
   },
   "source": [
    "#### So, the model is offering an 93.2% recall accuracy on the generalised unseen data (test set). Not a bad percentage to be the first try. However, recall this is a 93.2% recall accuracy measure on the undersampled test set.\n",
    "\n",
    "### Being happy with this result, let's apply the model we fitted and test it on the whole data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "2fac80a6-cc45-49e8-3fd6-2322e2461955"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.97      0.98     85296\n",
      "          1       0.05      0.94      0.09       147\n",
      "\n",
      "avg / total       1.00      0.97      0.98     85443\n",
      "\n",
      "\n",
      "Binary F1 Score, Recall and Precision:\n",
      "\tF1 Score 0.093973\n",
      "\tRecall Score 0.938776\n",
      "\tPrecision Score 0.049462\n",
      "\n",
      "Micro F1 Score, Recall and Precision:\n",
      "\tF1 Score 0.968856\n",
      "\tRecall Score 0.968856\n",
      "\tPrecision Score 0.968856\n"
     ]
    }
   ],
   "source": [
    "# Use this C_parameter to build the final model with the whole training dataset and predict the classes in the test\n",
    "# dataset\n",
    "lr = LogisticRegression(C = best_c, penalty = 'l1')\n",
    "lr.fit(X_train_undersample, y_train_undersample.values.ravel())\n",
    "y_pred = lr.predict(X_test.values)\n",
    "\n",
    "# Print Classification Report for Whole Test Data\n",
    "print_classification_report(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1f1e40a4-bca8-87ad-f368-d0b50e7d6158"
   },
   "source": [
    "#### An additional comment that would be interesting to do is to initialise multiple undersampled datasets and repeat the process in loop. Remember that, to create an undersample data, we randomly got records from the majority class. Even though this is a valid technique, is doesn't represent the real population, so it would be interesting to repeat the process with different undersample configurations and check if the previous chosen parameters are still the most effective. In the end, the idea is to use a wider random representation of the whole dataset and rely on the averaged best parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2e0e6bfc-37ac-2d2e-61af-7118619fdf27"
   },
   "source": [
    "# Logistic regression classifier - Skewed data\n",
    "\n",
    "#### Having tested our previous approach, I find really interesting to test the same process on the skewed data. Our intuition is that skewness will introduce issues difficult to capture, and therefore, provide a less effective algorithm.\n",
    "- To be fair, taking into account the fact that the train and test datasets are substantially bigger than the undersampled ones, I believe a K-fold cross validation is necessary. I guess that by splitting the data with 60% in training set, 20% cross validation and 20% test should be enough... but let's take the same approach as before (no harm on this, it's just that K-fold is computationally more expensive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "2aaf245f-43cd-d543-b857-562fb696fc4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "C parameter:  0.01\n",
      "-------------------------------------------\n",
      "\n",
      "Iteration  1 : F1 score =  0.611111111111111\n",
      "Iteration  2 : F1 score =  0.7272727272727272\n",
      "Iteration  3 : F1 score =  0.7454545454545455\n",
      "Iteration  4 : F1 score =  0.7115384615384616\n",
      "Iteration  5 : F1 score =  0.5950413223140496\n",
      "\n",
      "Mean F1 score  0.678083633538179\n",
      "\n",
      "-------------------------------------------\n",
      "C parameter:  0.1\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/ipykernel_launcher.py:57: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  1 : F1 score =  0.672566371681416\n",
      "Iteration  2 : F1 score =  0.7377049180327868\n",
      "Iteration  3 : F1 score =  0.7454545454545455\n",
      "Iteration  4 : F1 score =  0.7238095238095238\n",
      "Iteration  5 : F1 score =  0.6614173228346456\n",
      "\n",
      "Mean F1 score  0.7081905363625836\n",
      "\n",
      "-------------------------------------------\n",
      "C parameter:  1\n",
      "-------------------------------------------\n",
      "\n",
      "Iteration  1 : F1 score =  0.6607142857142858\n",
      "Iteration  2 : F1 score =  0.7317073170731706\n",
      "Iteration  3 : F1 score =  0.7678571428571429\n",
      "Iteration  4 : F1 score =  0.7476635514018692\n",
      "Iteration  5 : F1 score =  0.6923076923076923\n",
      "\n",
      "Mean F1 score  0.7200499978708322\n",
      "\n",
      "-------------------------------------------\n",
      "C parameter:  10\n",
      "-------------------------------------------\n",
      "\n",
      "Iteration  1 : F1 score =  0.6607142857142858\n",
      "Iteration  2 : F1 score =  0.7317073170731706\n",
      "Iteration  3 : F1 score =  0.7787610619469026\n",
      "Iteration  4 : F1 score =  0.7476635514018692\n",
      "Iteration  5 : F1 score =  0.7022900763358777\n",
      "\n",
      "Mean F1 score  0.7242272584944212\n",
      "\n",
      "-------------------------------------------\n",
      "C parameter:  100\n",
      "-------------------------------------------\n",
      "\n",
      "Iteration  1 : F1 score =  0.6607142857142858\n",
      "Iteration  2 : F1 score =  0.7317073170731706\n",
      "Iteration  3 : F1 score =  0.7787610619469026\n",
      "Iteration  4 : F1 score =  0.7476635514018692\n",
      "Iteration  5 : F1 score =  0.7022900763358777\n",
      "\n",
      "Mean F1 score  0.7242272584944212\n",
      "\n",
      "*********************************************************************************\n",
      "Best model to choose from cross validation is with C parameter =  10.0\n",
      "*********************************************************************************\n"
     ]
    }
   ],
   "source": [
    "best_c = printing_Kfold_scores(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "634c1907-a5c5-888c-c2e9-da73f81ee445"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00     85296\n",
      "          1       0.88      0.62      0.73       147\n",
      "\n",
      "avg / total       1.00      1.00      1.00     85443\n",
      "\n",
      "\n",
      "Binary F1 Score, Recall and Precision:\n",
      "\tF1 Score 0.728000\n",
      "\tRecall Score 0.619048\n",
      "\tPrecision Score 0.883495\n",
      "\n",
      "Micro F1 Score, Recall and Precision:\n",
      "\tF1 Score 0.999204\n",
      "\tRecall Score 0.999204\n",
      "\tPrecision Score 0.999204\n"
     ]
    }
   ],
   "source": [
    "# Use this C_parameter to build the final model with the whole training dataset and predict the classes in the test\n",
    "# dataset\n",
    "lr = LogisticRegression(C = best_c, penalty = 'l1')\n",
    "lr.fit(X_train, y_train.values.ravel())\n",
    "y_pred = lr.predict(X_test.values)\n",
    "\n",
    "# Print Classification Report for Whole Test Data\n",
    "print_classification_report(y_test,y_pred)"
   ]
  }
 ],
 "metadata": {
  "_change_revision": 0,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

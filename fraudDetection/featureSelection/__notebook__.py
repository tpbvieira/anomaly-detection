# coding: utf-8

# #INTRODUCTION 
# 
# **Caveat** : Running this notebook will take a while if you do decide to fork so take note. A handful of minutes.
# 
# This notebook will aim to provide an explanation and application of different feature ranking methods, namely that of 
# Recursive Feature Elimination (RFE), Stability Selection, linear models as well as Random Forest. But first off, it is
# always imperative to give credit where credit is due. The stuff in this notebook is indebted to and borrows heavily
# from the excellent 4-part blog article by Ando Saabas on feature selection. So please do check out his article from
# this
# link: http://blog.datadive.net/selecting-good-features-part-iv-stability-selection-rfe-and-everything-side-by-side/
# 
# The contents of this notebook are as follows: 
# 
#  1. **Data Cleaning and Visualisation** : This section will revolve around exploring the data and visualising some
# 	summary statistics.
#  2. **Stability Selection via Randomised Lasso Method** : Introduce a relatively new feature selection method called
# 	"Stability Selection" and using the Randomised Lasso in its implementation
#  3. **Recursive Feature Elimination** : Implementing the Recursive Feature Elimination method of feature ranking via
# 	the use of basic Linear Regression
#  4. **Linear Model Feature Coefficients** : Implementing 3 of Sklearn's linear models (Linear Regression, Lasso and
# 	Ridge) and using the inbuilt estimated coefficients for our feature selection
#  5. **Random Forest Feature Selection** : Using the Random Forest's convenient attribute "feature_importances" to
# 	calculate and ultimately rank the feature importance.
# 
# Finally, with all the points 1 to 5 above, we will combine the results to create our:
# 
# **Feature Ranking Matrix** : Matrix of all the features along with the respective model scores which we can use in our
#  ranking.
#  

# In[1]:


import pandas as pd
import numpy as np
import seaborn as sns
from matplotlib import pyplot as plt
from sklearn.feature_selection import RFE
from sklearn.linear_model import (LinearRegression, Ridge, Lasso, RandomizedLasso)
from sklearn.preprocessing import MinMaxScaler
from sklearn.ensemble import RandomForestRegressor


# # 1. DATA CLEANSING AND ANALYSIS
# 
# Let's first read in the house data as a dataframe "house" and inspect the first 5 rows

# In[2]:


house = pd.read_csv("/media/thiago/ubuntu/datasets/prediction/House_Sales_in_King_County/kc_house_data.csv")
house.head()


# Now its time for some general data inspection. Let's first examine to see if there are any nulls in the dataframe as
# well as look at the type of the data (i.e whether it is a string or numeric)

# In[3]:


# Looking for nulls
print(house.isnull().any())
# Inspecting type
print(house.dtypes)


# The data is pretty clean. There are no pesky nulls which we need to treat and most of the features are in numeric
# format. Let's go ahead and drop the "id" and "date" columns as these 2 features will not be used in this analysis.

# In[4]:


# Dropping the id and date columns
house = house.drop(['id', 'date'], axis=1)


# **Pairplot Visualisation**
# 
# Let's create some Seaborn pairplots for the features ('sqft_lot','sqft_above','price','sqft_living','bedrooms') to
# get a feel for how the various features are distributed vis-a-vis the price as well as the number of bedrooms

# In[5]:


# sns.pairplot(house[['sqft_lot','sqft_above','price','sqft_living','bedrooms']], hue='bedrooms', palette='afmhot',
# size=1.4)


# In[6]:
g = sns.pairplot(house[['sqft_lot', 'sqft_above', 'price', 'sqft_living', 'bedrooms']], hue='bedrooms',
				 palette='afmhot', size=1.4)
g.set(xticklabels=[])
plt.show()

# From the pairplots, we seem to get the classical linear distribution of the data points, for example with price
# against sqft_living. This bodes well as in the latter analysis, we will implement some linear models which we will
# use in our Feature ranking. Let's look at the correlation heatmap:

# In[7]:


str_list = [] # empty list to contain columns with strings (words)
for colname, colvalue in house.iteritems():
	if type(colvalue[1]) == str:
		str_list.append(colname)
# Get to the numeric columns by inversion
num_list = house.columns.difference(str_list) 
# Create Dataframe containing only numerical features
house_num = house[num_list]
f, ax = plt.subplots(figsize=(11, 11))
plt.title('Pearson Correlation of features')
# Draw the heatmap using seaborn
# sns.heatmap(house_num.astype(float).corr(),linewidths=0.25,vmax=1.0, square=True, cmap="PuBuGn", linecolor='k',
# annot=True)
sns.heatmap(house_num.astype(float).corr(), linewidths=0.25,vmax=1.0, square=True, cmap="cubehelix", linecolor='k', annot=True)


# # 2. Stability Selection via Randomized Lasso
# 
# In a nutshell, this method serves to apply the feature selection on different parts of the data and features
# repeatedly until the results can be aggregated. Therefore stronger features ( defined as being selected as important)
# will have greater scores in this method as compared to weaker features. Refer to this paper by Nicolai Meinshausen and
#  Peter Buhlmann for a much greater detail on the method : http://stat.ethz.ch/~nicolai/stability.pdf
# 
# In this notebook, the Stability Selection method is conveniently inbuilt into sklearn's randomized lasso model and
# therefore this will be implemented as follows:

# In[8]:


# First extract the target variable which is our House prices
Y = house.price.values
# Drop price from the house dataframe and create a matrix out of the house data
house = house.drop(['price'], axis=1)
X = house.as_matrix()
# Store the column/feature names into a list "colnames"
colnames = house.columns


# Next, we create a function which will be able to conveniently store our feature rankings obtained from the various
# methods described here into a Python dictionary. In case you are thinking I created this function, no this is not the
# case. All credit goes to Ando Saabas and I am only trying to apply what he has discussed in this context.

# In[9]:


# Define dictionary to store our rankings
ranks = {}
# Create our function which stores the feature rankings to the ranks dictionary
def ranking(ranks, names, order=1):
	minmax = MinMaxScaler()
	ranks = minmax.fit_transform(order*np.array([ranks]).T).T[0]
	ranks = map(lambda x: round(x,2), ranks)
	return dict(zip(names, ranks))


# In[10]:


# Finally let's run our Selection Stability method with Randomized Lasso
rlasso = RandomizedLasso(alpha=0.04)
rlasso.fit(X, Y)
ranks["rlasso/Stability"] = ranking(np.abs(rlasso.scores_), colnames)
print('finished')


# # 3. Recursive Feature Elimination ( RFE )
# 
# Now onto the next method in our feature ranking endeavour. Recursive Feature Elimination or RFE uses a model ( eg.
# linear Regression or SVM) to select either the best or worst-performing feature, and then excludes the feature. After
# this, the whole process is iterated until all features in the dataset are used up ( or up to a user-defined limit).
# Sklearn conveniently possesses a RFE function via the sklearn.feature_selection call and we will use this along with
# a simple linear regression model for our ranking search as follows:

# In[11]:


# Construct our Linear Regression model
lr = LinearRegression(normalize=True)
lr.fit(X,Y)
#stop the search when only the last feature is left
rfe = RFE(lr, n_features_to_select=1, verbose =3 )
rfe.fit(X,Y)
ranks["RFE"] = ranking(list(map(float, rfe.ranking_)), colnames, order=-1)


# # 4. Linear Model Feature Ranking
# 
# Now let's apply 3 different linear models (Linear, Lasso and Ridge Regression) and how the features are selected and
# prioritised via these models. To achieve this, I shall use the sklearn implementation of these models and in
# particular the attribute .coef to return the estimated coefficients for each feature in the linear model.

# In[12]:


# Using Linear Regression
lr = LinearRegression(normalize=True)
lr.fit(X,Y)
ranks["LinReg"] = ranking(np.abs(lr.coef_), colnames)

# Using Ridge 
ridge = Ridge(alpha = 7)
ridge.fit(X,Y)
ranks['Ridge'] = ranking(np.abs(ridge.coef_), colnames)

# Using Lasso
lasso = Lasso(alpha=.05)
lasso.fit(X, Y)
ranks["Lasso"] = ranking(np.abs(lasso.coef_), colnames)


# # 5. Random Forest feature ranking
# 
# Sklearn's Random Forest model also comes with it's own inbuilt feature ranking attribute and one can conveniently
# just call it via "feature_importances_". That is what we will be using as follows:

# In[13]:


rf = RandomForestRegressor(n_jobs=-1, n_estimators=50, verbose=3)
rf.fit(X,Y)
ranks["RF"] = ranking(rf.feature_importances_, colnames)


# # 6. Creating the Feature Ranking Matrix
# 
# We combine the scores from the various methods above and output it in a matrix form for convenient viewing as such:

# In[14]:


# Create empty dictionary to store the mean value calculated from all the scores
r = {}
for name in colnames:
	r[name] = round(np.mean([ranks[method][name] 
							 for method in ranks.keys()]), 2)
 
methods = sorted(ranks.keys())
ranks["Mean"] = r
methods.append("Mean")
 
print("\t%s" % "\t".join(methods))
for name in colnames:
	print("%s\t%s" % (name, "\t".join(map(str, 
						 [ranks[method][name] for method in methods]))))


# Now, with the matrix above, the numbers and layout does not seem very easy or pleasant to the eye. Therefore, let's
# just collate the mean ranking score attributed to each of the feature and plot that via Seaborn's factorplot.

# In[15]:


# Put the mean scores into a Pandas dataframe
meanplot = pd.DataFrame(list(r.items()), columns= ['Feature','Mean Ranking'])

# Sort the dataframe
meanplot = meanplot.sort_values('Mean Ranking', ascending=False)


# In[16]:


# Let's plot the ranking of the features
sns.factorplot(x="Mean Ranking", y="Feature", data = meanplot, kind="bar", size=4, aspect=1.9, palette='coolwarm')


# Well as you can see from our feature ranking endeavours, the top 3 features are 'lat', 'waterfront' and 'grade'.
# The bottom 3 are 'sqft_lot15', 'sqft_lot' and 'sqft_basement'.
# This sort of feature ranking can be really useful, especially if one has many many features in the dataset and would
# like to trim or cut off features that contribute negligibly.
